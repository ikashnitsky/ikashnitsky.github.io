[
  {
    "objectID": "2017/align-six-maps/index.html",
    "href": "2017/align-six-maps/index.html",
    "title": "Arranging subplots with ggplot2",
    "section": "",
    "text": "For my recently published paper, I produced not-so-standard figures that show the two step decomposition used in the analysis. Have a look:\nFigure 3 from my paper (PDF)\nActually, ggplot2 is a very powerful and flexible tool that allows to draw figures with quite a complex layout. Today I want to show the code that aligns six square plots (actually, maps) just as in the figure above. And it’s all about the handy function ggplot2::annotation_custom(). Since I used the layout more than once, I wrapped the code that produced it into a function that takes a list of 6 square plots as an input and yields the arranged figure with arrows as an output. Here is the commented code of the function.\n\nalign_six_plots <- function(list.plots, \n                                    family = \"\",\n                                    labels=LETTERS[1:6], \n                                    labels.size=8){\n\n        require(tidyverse)\n        require(gridExtra)\n\n        gg <- ggplot()+\n                coord_equal(xlim = c(0, 21), ylim = c(0, 30), expand = c(0,0))+\n\n                annotation_custom(ggplotGrob(list.plots[[1]]),\n                                  xmin = 0.5, xmax = 8.5, ymin = 21, ymax = 29)+\n\n                annotation_custom(ggplotGrob(list.plots[[2]]),\n                                  xmin = 12.5, xmax = 20.5, ymin = 19.5, ymax = 27.5)+\n                annotation_custom(ggplotGrob(list.plots[[3]]),\n                                  xmin = 12.5,xmax = 20.5,ymin = 10.5,ymax = 18.5)+\n\n                annotation_custom(ggplotGrob(list.plots[[4]]),\n                                  xmin = 0.5, xmax = 8.5, ymin = 9,ymax = 17)+\n                annotation_custom(ggplotGrob(list.plots[[5]]),\n                                  xmin = 0.5, xmax = 8.5, ymin = 0, ymax = 8)+\n                annotation_custom(ggplotGrob(list.plots[[6]]),\n                                  xmin = 12.5,xmax = 20.5, ymin = 0, ymax = 8)+\n\n                labs(x = NULL, y = NULL)+\n                theme_void()\n\n        # DF with the coordinates of the 5 arrows\n        df.arrows <- data.frame(id=1:5,\n                                x=c(8.5,8.5,12.5,12.5,12.5),\n                                y=c(21,21,10.5,10.5,10.5),\n                                xend=c(12.5,12.5,8.5,8.5,12.5),\n                                yend=c(20.5,17.5,10,7,7))\n\n        # add arrows\n        gg <- gg +\n                geom_curve(data = df.arrows %>% filter(id==1),\n                           aes(x=x,y=y,xend=xend,yend=yend),\n                           curvature = 0.1,\n                           arrow = arrow(type=\"closed\",length = unit(0.25,\"cm\"))) +\n                geom_curve(data = df.arrows %>% filter(id==2),\n                           aes(x=x,y=y,xend=xend,yend=yend),\n                           curvature = -0.1,\n                           arrow = arrow(type=\"closed\",length = unit(0.25,\"cm\"))) +\n                geom_curve(data = df.arrows %>% filter(id==3),\n                           aes(x=x,y=y,xend=xend,yend=yend),\n                           curvature = -0.15,\n                           arrow = arrow(type=\"closed\",length = unit(0.25,\"cm\"))) +\n                geom_curve(data = df.arrows %>% filter(id==4),\n                           aes(x=x,y=y,xend=xend,yend=yend),\n                           curvature = 0,\n                           arrow = arrow(type=\"closed\",length = unit(0.25,\"cm\"))) +\n                geom_curve(data = df.arrows %>% filter(id==5),\n                           aes(x=x,y=y,xend=xend,yend=yend),\n                           curvature = 0.3,\n                           arrow = arrow(type=\"closed\",length = unit(0.25,\"cm\")))\n\n        # add labes\n        gg <- gg + annotate('text',label = labels,\n                            x=c(.5,12.5,12.5,.5,.5,12.5)+.5,\n                            y=c(29,27.5,18.5,17,8,8)+.1,\n                            size=labels.size,hjust=0, vjust=0, family = family)\n\n        return(gg)\n}\n\nLet’s check, if the function works. For that I create just a blank plot, clone it six times, store the six plots in a list, and finally feed it to the function.\n\nlibrary(tidyverse)\nlibrary(ggthemes)\n\n# create a simple blank square plot\np <- ggplot()+\n  expand_limits(x = c(0,1), y = c(0,1))+\n  theme_map()+\n  theme(panel.border = element_rect(color = \"black\", size = 0.5, fill = NA),\n        aspect.ratio = 1)\n\n# clone this plot six times and store as a list of six\nplots <- mget(rep(\"p\", 6))\n\n# use the function on the list\nsix <- align_six_plots(plots)\n\n# save the output\nggsave(\"six_square_plots_aligned.png\", six, width=12, height=18)\n\n\nJust what we wanted to get.\n\n\n\n\n\n\n\nTo reproduce all the actual results and figures from my paper, have a look at this github repo"
  },
  {
    "objectID": "2017/colorcoded-map/index.html",
    "href": "2017/colorcoded-map/index.html",
    "title": "Colorcoded map: regional population structures at a glance",
    "section": "",
    "text": "Data visualization is quite often a struggle to represent multiple relevant dimensions preserving the readability of the plot. In this post I will show my recent multidimensional dataviz prepared for Rostock Retreat Visualization, an event that gathered demographers for an amazing “three days long coffebreak”.\nEuropean population is rapidly ageing. But the process is not happening uniformly in all parts of Europe (see my recent paper for more info). Regions differ quite a lot: Eastern Europe still undergoes demographic dividend; Southern European regions form a cluster of lowest-low fertility; Western Europe experiences the greying of the baby boomers; urban regions attract young professionals and force out young parents; peripheral rural regions lose their youths forever… How can we grasp all the differences at a glance?\nHere I want to present a colorcoded map. For each NUTS-3 region the unique color is produced by mixing red, green, and blue color spectrums in the proportions that reflect,correspondingly, relative shares of elderly populating (aged 65+), population at working ages (15-64), and kids (0-14).\n\nEach of the three variables mapped here is scaled between 0 and 1: otherwise, the map would be just green with slightly variations in tones because the share of working age population is ranged between 65-75% for modern European regions. Thus, it is important to note that this map is not meant to be able to inform the reader of the exact population structure in a specific region. Rather, it provides a snapshot of all the regional population structures, facilitating comparisons between them. So, by design, the colors are only meaningful in comparison only for the given set of regions in a given year, in this case 2015. If we want cross-year comparisons, the variables are to be scaled across the whole timeseries, meaning that each separate map would, most likely, become less contrast.\nIn the map we can easily spot the major differences between subregions of Europe. Turkey is still having relatively high fertility, especially in the south-eastern Kurdish part, thus it has higher share of kids and it’s colored in blueish tones. The high-fertility Ireland is also evidently blue in the map. East-European regions are green due to the still lasting demographic dividend. Southern Europe is ageing fastest, thus the colors are reddish.\nWe can also see most of the major capital regions that are bright-green as opposed to the depleted periphery. In some countries there are huge regional differences: Northern and Southern Italy, Eastern and Western Germany.\nIt is striking how clearly can we see the borders between European countries: Poland and Germany, Czech Republic and Slovakia, Portugal and Spain, France and all the neighbors. The slowly evolving population structures bare imprints of unique populations’ histories, that largely correspond with state borders.\nThe obvious drawback of the map is that it is not colorblind friendly, and there is no way to make it so because color is the main player in this dataviz.\n\n\n\n\n\n\n\nTo reproduce the map from the scratch please see the gist"
  },
  {
    "objectID": "2017/data-acquisition-one/index.html",
    "href": "2017/data-acquisition-one/index.html",
    "title": "Data acquisition in R (1/4)",
    "section": "",
    "text": "The series consists of four posts:\n\n\n\n\n\nLoading prepared datasets\n\n\nAccessing popular statistical databases\n\n\nDemographic data sources\n\nGetting spatial data\n\n\n\n\nFor each of the data acquisition options I provide a small visualization use case.\nBuilt-in datasets\nFor illustration purposes, many R packages include data samples. Base R comes with a datasets package that offers a wide range of simple, sometimes very famous, datasets. Quite a detailed list of built-in datasets from various packages is maintained by Vincent Arel-Bundock.\nThe nice feature of the datasets form datasets package is that they are “always there”. The unique names of the datasets may be referred as the objects from Global Environment. Let’s have a look at a beautiful small dataset calls swiss - Swiss Fertility and Socioeconomic Indicators (1888) Data. I am going to check visually the difference in fertility based of rurality and domination of Catholic population.\n\nlibrary(tidyverse)\n\nswiss %>% \n        ggplot(aes(x = Agriculture, y = Fertility, \n                   color = Catholic > 50))+\n        geom_point()+\n        stat_ellipse()+\n        theme_minimal(base_family = \"mono\")\n\n\nGapminder\nSome packages are created specifically to disseminate datasets in a ready to use format. One of the nice examples is a package gapminder that contains a neat dataset widely used by Hans Rosling in his Gapminder project.\n\nlibrary(tidyverse)\nlibrary(gapminder)\n\ngapminder %>% \n        ggplot(aes(x = year, y = lifeExp, \n                   color = continent))+\n        geom_jitter(size = 1, alpha = .2, width = .75)+\n        stat_summary(geom = \"path\", fun.y = mean, size = 1)+\n        theme_minimal(base_family = \"mono\")\n\n\nGrab a dataset by URL\nIf a dataset is hosted online and has a direct link to the file, it can be easily imported into the R session just specifying the URL. For illustration, I will access Galton dataset from HistData package using a direct link from Vincent Arel-Bundock’s list.\n\nlibrary(tidyverse)\n\ngalton <- read_csv(\"https://raw.githubusercontent.com/vincentarelbundock/Rdatasets/master/csv/HistData/Galton.csv\")\n\ngalton %>% \n        ggplot(aes(x = father, y = height))+\n        geom_point(alpha = .2)+\n        stat_smooth(method = \"lm\")+\n        theme_minimal(base_family = \"mono\")\n\n\nDownload and unzip an archive\nQuite often datasets are stored in archived from. With R it is very simple to download and unzip the desired data archives. As an example, I will download Historical New York City Crime Data provided by the Government of the Sate of New York and hosted at data.gov portal. The logic of the process is: first, we create a directory for the unzipped data; second, we download the archive; finally, unzip the archive and read the data.\n\nlibrary(tidyverse)\nlibrary(readxl)\n\n# create a directory for the unzipped data\nifelse(!dir.exists(\"unzipped\"), dir.create(\"unzipped\"), \"Directory already exists\")\n\n# specify the URL of the archive\nurl_zip <- \"http://www.nyc.gov/html/nypd/downloads/zip/analysis_and_planning/citywide_historical_crime_data_archive.zip\"\n\n# storing the archive in a temporary file\nf <- tempfile()\ndownload.file(url_zip, destfile = f)\nunzip(f, exdir = \"unzipped/.\")\n\nIf the zipped file is rather big and we don’t want to download it again the next time we run the code, it might be useful to keep the archived data.\n\n# if we want to keep the .zip file\npath_unzip <- \"unzipped/data_archive.zip\"\nifelse(!file.exists(path_unzip), \n       download.file(url_zip, path_unzip, mode=\"wb\"), \n       'file alredy exists')\nunzip(path_unzip, exdir = \"unzipped/.\")\n\nFinally, let’s read and plot some of the downloaded data.\n\nmurder <- read_xls(\"unzipped/Web Data 2010-2011/Seven Major Felony Offenses 2000 - 2011.xls\",\n                   sheet = 1, range = \"A5:M13\") %>% \n        filter(OFFENSE %>% substr(1, 6) == \"MURDER\") %>% \n        gather(\"year\", \"value\", 2:13) %>% \n        mutate(year = year %>% as.numeric())\n\nmurder %>% \n        ggplot(aes(year, value))+\n        geom_point()+\n        stat_smooth(method = \"lm\")+\n        theme_minimal(base_family = \"mono\")+\n        labs(title = \"Murders in New York\")\n\n\nFigshare\nIn Academia it is becoming more and more popular to store the datasets accompanying papers in the specialized repositories. Figshare is one of the most popular free repositories. There is an R package rfigshare to access the datasets from this portal. As an example I will grab the dataset on ice-hockey playes height that I assembled manually for my blog post. Please note that at the first run the package will ask to enter your Figshare login details to access API - a web page will be opened in browser.\nThere is a search function fs_search, though my experience shows that it is easier to search for a dataset in a browser and then use the id of a file to download it. The function fs_download turns an id number into a direct URL to download the file.\n\nlibrary(tidyverse)\nlibrary(rfigshare)\n\nurl <- fs_download(article_id = \"3394735\")\n\nhockey <- read_csv(url)\n\nhockey %>% \n        ggplot(aes(x = year, y = height))+\n        geom_jitter(size = 2, color = \"#35978f\", alpha = .1, width = .25)+\n        stat_smooth(method = \"lm\", size = 1)+\n        ylab(\"height, cm\")+\n        xlab(\"year of competition\")+\n        scale_x_continuous(breaks = seq(2005, 2015, 5), labels = seq(2005, 2015, 5))+\n        theme_minimal(base_family = \"mono\")\n\n\n\n\n\n\n\n\n\nAll the code chunks can be found in this gist"
  },
  {
    "objectID": "2017/data-acquisition-three/index.html",
    "href": "2017/data-acquisition-three/index.html",
    "title": "Data acquisition in R (3/4)",
    "section": "",
    "text": "The series consists of four posts:\n\n\n\n\nLoading prepared datasets\nAccessing popular statistical databases\nDemographic data sources\nGetting spatial data\n\n\n\nFor each of the data acquisition options I provide a small visualization use case.\nHuman Mortality Database\nWhen it comes to testing the big questions of human population dynamics, there is no more reliable data source than Human Mortality Database. This database is run by demographers who use state-of-the-art methodology to overcome issues in the data. As the result, the estimates are as precise as possible. Their methods protocol is a masterpiece of demographic data processing. On the down side, the data of decent enough quality is available for only a bunch of countries. To explore the data I highly recommend [Human Mortality Database Explorer][exp] by Jonas Schöley.\nThanks to Tim Riffe’s HMDHFDplus package, one can now download HMD data with just a couple of lines of R code. Please note that an account at mortality.org is needed in order to download data. As one may guess from the package name, it also helps to grab data from equally brilliant Human Fertility Database.\nThe following example is taken from my earlier post (and updated a bit). I think it illustrates nicely the power of automated data acquisition in R. Here I am going to download one year population population structures for both males and females for each single country of HMD. If you want to reproduce the result, beware that the script will download a couple of dozens megabits of data. Then I will calculate and visualize as small multiples sex ratios in all countries along age dimension. Sex ratios reflect the two basic regularities of human demographics: 1) there are always more boys being born; 2) males experience higher mortality throughout their life-course. Besides some artificial and well known exceptions, sex ratio at birth does not vary dramatically and is more or less constant at the level of 105-106 boys per 100 girls. Hence, differences in the sex ratio profiles of countries mainly reflect gender gap in mortality.\n\n# load required packages\nlibrary(HMDHFDplus)\nlibrary(tidyverse)\nlibrary(purrr)\n\n\n# help function to list the available countries\ncountry <- getHMDcountries()\n\n# remove optional populations\nopt_pop <- c(\"FRACNP\", \"DEUTE\", \"DEUTW\", \"GBRCENW\", \"GBR_NP\")\ncountry <- country[!country %in% opt_pop]\n\n# temporary function to download HMD data for a simgle county (dot = input)\ntempf_get_hmd <- . %>% readHMDweb(\"Exposures_1x1\", ik_user_hmd, ik_pass_hmd) \n\n\n# download the data iteratively for all countries using purrr::map()\nexposures <- country %>% map(tempf_get_hmd)\n\n# data transformation to apply to each county dataframe\ntempf_trans_data <- . %>% \n        select(Year, Age, Female, Male) %>% \n        filter(Year %in% 2012) %>%\n        select(-Year) %>%\n        transmute(age = Age, ratio = Male / Female * 100)\n\n# perform transformation\ndf_hmd <- exposures %>% \n        map(tempf_trans_data) %>% \n        bind_rows(.id = \"country\")\n\n# summarize all ages older than 90 (too jerky)\ndf_hmd_90 <- df_hmd %>% \n        filter(age %in% 90:110) %>% \n        group_by(country) %>% \n        summarise(ratio = ratio %>% mean(na.rm = T)) %>%\n        ungroup() %>% \n        transmute(country, age = 90, ratio)\n\n# insert summarized 90+\ndf_hmd_fin <- bind_rows(df_hmd %>% filter(!age %in% 90:110), df_hmd_90)\n\n# finaly - plot\ndf_hmd_fin %>%  \n        ggplot(aes(age, ratio, color = country, group = country))+\n        geom_hline(yintercept = 100, color = \"grey50\", size = 1)+\n        geom_line(size = 1)+\n        scale_y_continuous(limits = c(0, 120), \n                           expand = c(0, 0), \n                           breaks = seq(0, 120, 20))+\n        scale_x_continuous(limits = c(0, 90), \n                           expand = c(0, 0), \n                           breaks = seq(0, 80, 20))+\n        facet_wrap(~country, ncol = 6)+\n        theme_minimal(base_family = \"mono\", base_size = 15)+\n        theme(legend.position = \"none\",\n              panel.border = element_rect(size = .5, fill = NA, \n                                          color = \"grey50\"))+\n        labs(x = \"Age\", \n             y = \"Sex ratio, males per 100 females\", \n             title = \"Sex ratio in all countries from Human Mortality Database\",\n             subtitle = \"HMD 2012, via HMDHFDplus by @timriffe1\",\n             caption = \"ikashnitsky.github.io\")\n\n\nUnited Nations World Population Prospects\nPopulation Department of the United Nations provides high quality population estimates for all countries of the world. They update estimates every 2-3 years and publish openly as an interactive report World Population Prospects. One may find in these reports key highlights and, of course, rich data. The data is later wrapped in R packages called wpp20xx. Currently, the available packages are for the estimate updates 2008, 2010, 2012, 2015, and 2017. I will give here an example of wpp2015 use adapted from my earlier post.\nUsing ridgeplot, the amazing type of dataviz promoted ggridges package by Claus Wilke, I am going to show the impressive reduction of global inequality in male mortality that took place since 1950.\n\nlibrary(wpp2015)\nlibrary(tidyverse)\nlibrary(ggridges)\nlibrary(viridis)\n\n# get the UN country names\ndata(UNlocations)\n\ncountries <- UNlocations %>% pull(name) %>% paste\n\n# data on male life expectancy at birth\ndata(e0M) \n\ne0M %>% \n        filter(country %in% countries) %>% \n        select(-last.observed) %>% \n        gather(period, value, 3:15) %>% \n        ggplot(aes(x = value, y = period %>% fct_rev()))+\n        geom_density_ridges(aes(fill = period))+\n        scale_fill_viridis(discrete = T, option = \"B\", direction = -1, \n                           begin = .1, end = .9)+\n        labs(x = \"Male life expectancy at birth\",\n             y = \"Period\",\n             title = \"Global convergence in male life expectancy at birth since 1950\",\n             subtitle = \"UNPD World Population Prospects 2015 Revision, via wpp2015\",\n             caption = \"ikashnitsky.github.io\")+\n        theme_minimal(base_family =  \"mono\")+\n        theme(legend.position = \"none\")\n\n\nEuropean Social Survey (ESS)\nEuropean Social Survey provides uniquely rich nationally representative cross-county comparable information on the values of Europeans. Every two years a cross-sectional sample is taken in each participating country. All the data is easily available upon registration. Datasets are distributed as SAS, SPSS, or STATA files. Thanks to Jorge Cimentada, these datasets are now easily available via ess package. I am going to visualize how respondents assessed their level of trust in police in all available countries at the latest round of the survey.\n\nlibrary(ess)\nlibrary(tidyverse) \n\n# help gunction to see the available countries\nshow_countries()\n\n# check the available rounds for a selected country\nshow_country_rounds(\"Netherlands\")\n\n# get the full dataset of the last (8) round\ndf_ess <- ess_rounds(8, your_email =  ik_email) \n\n# select a variable and calculate mean value\ndf_ess_select <- df_ess %>% \n        bind_rows() %>% \n        select(idno, cntry, trstplc) %>% \n        group_by(cntry) %>% \n        mutate(avg = trstplc %>% mean(na.rm = T)) %>% \n        ungroup() %>% \n        mutate(cntry = cntry %>% as_factor() %>% fct_reorder(avg))\n\ndf_ess_select %>% \n        ggplot(aes(trstplc, fill = avg))+\n        geom_histogram()+\n        scale_x_continuous(limits = c(0, 11), breaks = seq(2, 10, 2))+\n        scale_fill_gradient(\"Average\\ntrust\\nscore\", \n                            low = \"black\", high = \"aquamarine\")+\n        facet_wrap(~cntry, ncol = 6)+\n        theme_minimal(base_family = \"mono\")+\n        labs(x = \"Trust score [0 -- 10]\",\n             y = \"# of respondents\",\n             title = \"Trust in police\",\n             subtitle = \"ESS wave 8 2017, via ess by @cimentadaj\",\n             caption = \"ikashnitsky.github.io\")\n\n\nAmerican Community Survey and Census\nThere are several packages that provide access to the US Census and ACS data. Perhaps the most convenient one is the recent tidycensus package by Kyle Walker. One extremely useful feature of this approach is the ability to download geodata along with stats in the form of simple features. Simple features, a revolutionary approach to deal with spatial data in R implemented in sf package by Edzer Pebesma, allow to manage and visualize geodata tidy and efficiently. Note that in order to reproduce the following example one would have to install the development version of ggplot2.\nBelow I map median ages of census tracts population in Chicago based on the ACS estimates in 2015. To use tidycensus, an API key is required. API is instantly provided upon registration.\n\nlibrary(tidycensus)\nlibrary(tidyverse)\nlibrary(viridis)\nlibrary(janitor)\nlibrary(sf)\n# to use geom_sf we need the latest development version of ggplot2\ndevtools::install_github(\"tidyverse/ggplot2\", \"develop\")\nlibrary(ggplot2)\n\n\n# you need a personal API key, available free at\n# https://api.census.gov/data/key_signup.html\n# normally, this key is to be stored in .Renviron\n\n# see state and county codes and names\nfips_codes %>% View\n\n# the available variables\nload_variables(year = 2015, dataset = \"acs5\") %>% View\n\n# data on median age of population in Chicago\ndf_acs <- get_acs(\n        geography = \"tract\",\n        county = \"Cook County\",\n        state = \"IL\",\n        variables = \"B01002_001E\",\n        year = 2015,\n        key = ik_api_acs,\n        geometry = TRUE\n) %>% clean_names()\n\n\n# map the data\ndf_acs %>% \n        ggplot()+\n        geom_sf(aes(fill = estimate %>% \n                            cut(breaks = seq(20, 60, 10))), \n                color = NA)+\n        scale_fill_viridis_d(\"Median age\", begin = .4)+\n        coord_sf(datum = NA)+\n        theme_void(base_family =  \"mono\")+\n        theme(legend.position = c(.15, .15))+\n        labs(title = \"Median age of population in Chicago\\nby census tracts\\n\",\n             subtitle = \"ACS 2015, via tidycensus by @kyle_e_walker\",\n             caption = \"ikashnitsky.github.io\", \n             x = NULL, y = NULL)\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nAll the code chunks together can be found in this gist"
  },
  {
    "objectID": "2017/data-acquisition-two/index.html",
    "href": "2017/data-acquisition-two/index.html",
    "title": "Data acquisition in R (2/4)",
    "section": "",
    "text": "R is an incredible tool for reproducible research. In the present series of blog posts I want to show how one can easily acquire data within an R session, documenting every step in a fully reproducible way. There are numerous data acquisition options for R users. Of course, I do not attempt to show all the data possibilities and tend to focus mostly on demographic data. If your prime interest lies outside human population statistics, it’s worth checking the amazing Open Data Task View.\nThe series consists of four posts: - Loading prepared datasets - Accessing popular statistical databases - Demographic data sources - Getting spatial data\nFor each of the data acquisition options I provide a small visualization use case.\nEurostat\nThe package eurostat has a function search_eurostat to search for the relevant datasets. Though, sadly enough, this function does not provide the codes of all the datasets that has the expression of interest in the title. For example, the search on the expression life expectancy produces an output with just 2 results, which does not make any sense. Thus, the best strategy is to go to Eurostat website, find the needed dataset code, and fetch the desired dataset by its code. Note that there is a separate database for subregional level indicators.\nI am going to download life expectancy estimates for European countries; the dataset code is demo_mlexpec.\n\nlibrary(tidyverse) \nlibrary(lubridate)\nlibrary(eurostat) \n\n# download the selected dataset\ne0 <- get_eurostat(\"demo_mlexpec\")\n\nIt can take a while because the dataset is quite big (0.4m obs). If the automated procedure does not work, one can download the data manually via the Bulk Download Service of Eurostat.\nLet’s have a look at the remaining life expectancy at age 65, the most common conventional age at retirement, in some European countries, separately for males, females, and total population. Some data preparation steps are needed. First, we only need the life expectancy estimates for those aged 65. Next, we don’t need total population, only males and females separately. Finally, let’s select just a bunch of countries: Germany, France, Italy, Russia, Spain, the UK.\n\ne0 %>% \n        filter(! sex == \"T\",\n               age == \"Y65\", \n               geo %in% c(\"DE\", \"FR\", \"IT\", \"RU\", \"ES\", \"UK\")) %>% \n        ggplot(aes(x = time %>% year(), y = values, color = sex))+\n        geom_path()+\n        facet_wrap(~ geo, ncol = 3)+\n        labs(y = \"Life expectancy at age 65\", x = NULL)+\n        theme_minimal(base_family = \"mono\")\n\n\nWorld Bank\nThere are several packages that provide an API to World Bank data. Probably, the most elaborated one is a fairly recent wbstats. Its wbsearch function really does great job searching through the database for the relevant datasets. For example, wbsearch(\"fertility\") produces a dataframe of 339 entries with the codes and names of the relevant indicators.\n\nlibrary(tidyverse) \nlibrary(wbstats)\n\n# search for a dataset of interest\nwbsearch(\"fertility\") %>% head\n\n\n\n\n\n\n\n\n\nindicatorID\nindicator\n\n\n\n2479\nSP.DYN.WFRT.Q5\nTotal wanted fertility rate (births per woman): Q5 (highest)\n\n\n2480\nSP.DYN.WFRT.Q4\nTotal wanted fertility rate (births per woman): Q4\n\n\n2481\nSP.DYN.WFRT.Q3\nTotal wanted fertility rate (births per woman): Q3\n\n\n2482\nSP.DYN.WFRT.Q2\nTotal wanted fertility rate (births per woman): Q2\n\n\n2483\nSP.DYN.WFRT.Q1\nTotal wanted fertility rate (births per woman): Q1 (lowest)\n\n\n2484\nSP.DYN.WFRT\nWanted fertility rate (births per woman)\n\n\n\nLet’s have a look at the indicator Lifetime risk of maternal death (%) (code SH.MMR.RISK.ZS). World Bank provides a variety of country groupings. One of the curious groupings divides countries based on the advance over the Demographic Transition path. Below I plot our selected indicator for (1) the countries that have passed the Demographic Transition, (2) the countries that haven’t yet experienced demographic dividend, and (3) the whole World.\n\n# fetch the selected dataset\ndf_wb <- wb(indicator = \"SH.MMR.RISK.ZS\", startdate = 2000, enddate = 2015)\n\n# have look at the data for one year\ndf_wb %>% filter(date == 2015) %>% View\n\ndf_wb %>% \n        filter(iso2c %in% c(\"V4\", \"V1\", \"1W\")) %>% \n        ggplot(aes(x = date %>% as.numeric(), y = value, color = country))+\n        geom_path(size = 1)+\n        scale_color_brewer(NULL, palette = \"Dark2\")+\n        labs(x = NULL, y = NULL, title = \"Lifetime risk of maternal death (%)\")+\n        theme_minimal(base_family = \"mono\")+\n        theme(panel.grid.minor = element_blank(),\n              legend.position = c(.8, .9))\n\n\nOECD\nOrganization for Economic Cooperation and Development provides detailed economic and demographic data on the member countries. There is an R package OECD that streamlines the use of their data in R. The function search_dataset works nicely to browse the available datasets by keywords. Then get_dataset would fetch the chosen dataset. In the example below I grab the data on the duration of unemployment and then plot the data for the male population of EU16, EU28 and the US as heatmaps.\n\nlibrary(tidyverse) \nlibrary(viridis)\nlibrary(OECD)\n\n# search by keyword\nsearch_dataset(\"unemployment\") %>% View\n\n# download the selected dataset\ndf_oecd <- get_dataset(\"AVD_DUR\")\n\n# turn variable names to lowercase\nnames(df_oecd) <- names(df_oecd) %>% tolower()\n\ndf_oecd %>% \n        filter(country %in% c(\"EU16\", \"EU28\", \"USA\"), sex == \"MEN\", ! age == \"1524\") %>% \n        ggplot(aes(obstime, age, fill = obsvalue))+\n        geom_tile()+\n        scale_fill_viridis(\"Months\", option = \"B\")+\n        scale_x_discrete(breaks = seq(1970, 2015, 5) %>% paste)+\n        facet_wrap(~ country, ncol = 1)+\n        labs(x = NULL, y = \"Age groups\", \n             title = \"Average duration of unemployment in months, males\")+\n        theme_minimal(base_family = \"mono\")\n\n\nWID\nWorld Wealth and Income Database is a harmonized dataset on income and wealth inequality. The developers of the database provide an R package to get their data, which is only available from github so far.\n\nlibrary(tidyverse) \n\n#install.packages(\"devtools\")\ndevtools::install_github(\"WIDworld/wid-r-tool\")\nlibrary(wid)\n\nThe function to acquire data is download_wid(). To specify the arguments, one would have to consult help pages of the package and select desired datasets.\n\n?wid_series_type\n?wid_concepts\n\nThe following nice example is adapted from the package vignette. It shows the share of wealth that was owned by the richest 1% and 10% of population in France and Great Britain.\n\ndf_wid <- download_wid(\n        indicators = \"shweal\", # Shares of personal wealth\n        areas = c(\"FR\", \"GB\"), # In France an Italy\n        perc = c(\"p90p100\", \"p99p100\") # Top 1% and top 10%\n)\n\n\ndf_wid %>% \n        ggplot(aes(x = year, y = value, color = country)) +\n        geom_path()+\n        labs(title = \"Top 1% and top 10% personal wealth shares in France and Great Britain\",\n             y = \"top share\")+\n        facet_wrap(~ percentile)+\n        theme_minimal(base_family = \"mono\")\n\n\n\n\n\n\n\n\n\nAll the code chunks together can be found in this gist"
  },
  {
    "objectID": "2017/dd-journals-frequency/index.html",
    "href": "2017/dd-journals-frequency/index.html",
    "title": "30 issues of Demographic Digest - the most frequent journals",
    "section": "",
    "text": "This week, the 30-th issue of my Demographic Digest was published.\nDemographic Digest is my project that started in November 2015. Twice a month I select fresh demographic papers and write brief summaries of them in Russian to be published in Demoscope Weekly, the most popular Russian journal/website in social sciences. If you read Russian, you may want to browse the archive or visit the website of the project (which is still to be filled).\nThe project is in the transitional phase now. Since 2016 Demographic Digest welcomes contributions from from external authors. In February 2017 I launched the first iteration of a project for the students of National Research University Higher School of Economics.\nTo draw a line after the first phase of the project, I analysed what journals supplied Demographic Digest most frequently. Also, my desire was to try visualizing data with treemaps, which I mentioned in the bonus part1 of the latest digest issue.1 I finish each issue of Demographic Digest with a bonus, in which I cover fun papers, discuss some academia related issues, or just provide link to cool visualizations and projects.\nFor that, I exported the bibliographic data of all the papers covered in Demographic Digest. I use Zotero as a reference manager; the paper records are exported as a single .bib file, which I then saved as a plain text (.txt) file. Then I read this data in R, cleaned it, and finally visualized.\n\n# load required packages\nlibrary(tidyverse)\nlibrary(stringr)\nlibrary(readxl)\nlibrary(extrafont)\nmyfont <- \"Roboto Condensed\"\n\ndf <- data.frame(lines = readLines(\"https://ikashnitsky.github.io/share/1702-dd-stats/dd-bib.txt\")) %>% \n        mutate(lines = lines %>% as.character()) %>% \n        \n        # grab only the lines that contain journals' titles\n        filter(lines %>% str_detect(\"journaltitle\")) %>% \n        \n        # remove everything that is not the bare journal's title\n        transmute(journals = lines %>% \n                       str_replace_all(pattern = \"\\tjournaltitle = |\\\\Q{\\\\E|\\\\Q}\\\\E,|\\\\Q}\\\\E\", \n                                       replacement = \"\")) %>% \n        # calculate frequencies\n        group_by(journals) %>% \n        summarise(n = n())\n\nFor one journal title, Ageing and Society, I failed to replace the “&” using regular expressions. This one is to be fixed manually. I also corrected the title of Lancet journal removing the article “The”. Finally, I corrected the frequencies for Population Studies and Population and Development Review subtracting 6, because for both journals I provided lists of most cited papers as a bonus. Following the same logic, I cleaned the data from the papers that appeared in the bonus part.\n\n# correct \"Ageing and Society\"\ndf[1,1] <- \"Ageing and Society\"\n\n# correct the title of Lancet\ndf <- df %>% mutate(journals = journals %>% str_replace(\"The Lancet\", \"Lancet\"))\n\n# correct \"Population and Development Review\" and \"Population Studies\" for 6 each\n# Reason - top cited papers bonus\ndf[df$journals %in% c(\"Population and Development Review\", \"Population Studies\"), 2] <- \n        df[df$journals %in% c(\"Population and Development Review\", \"Population Studies\"), 2] - 6\n\nTo provide some additional metrics of the journals, I downloaded bibliometric data from the SCImago Journal & Country Rank projecthttp://www.scimagojr.com/aboutus.php. Demographic journals usually have rather low SJR, compared to medical journals; that’s why I downloaded the data only for journals in Social Sciences (the.xlsx file). Then I read the data in R and join to my data frame.\n\n# read SJR data for journals in Social Sciences\nsjr <- readxl::read_excel(\"https://ikashnitsky.github.io/share/1702-dd-stats/scimagojr.xlsx\", 1) %>% \n        mutate(id = Title %>% tolower())\n\n# join the data frames; note that I create an \"id\" variable in lower case\ndf_sjr <- left_join(df %>% mutate(id = journals %>% tolower), sjr, \"id\") \n\nFinally, it’s time to visualize the data. I use the amazing treemap package2.2 I also tried portfolio and treemapify, but liked the output from treemap most.\n\n# Treemap visualization\nlibrary(treemap)\n\ntreemap(dtf = df_sjr, \n        index = \"journals\", \n        vSize = \"n\", \n        vColor = \"SJR\", \n        type = \"value\",\n        n = 5,\n        palette = \"BrBG\", \n        border.col = \"grey10\", \n        title = \"Journals' frequency in Demographic Digest\",\n        title.legend = \"SJR (only social sciences)\",\n        fontfamily.title = myfont,\n        fontfamily.labels = myfont,\n        fontfamily.legend = myfont,\n        drop.unused.levels = T)\n\nHere is how the output looks\n\nNote that the lion’s share of Population Studies is mainly explained by the first issue of Demographic Digest, in which I covered all the papers from the brilliant special issue Population — The long view."
  },
  {
    "objectID": "2017/denmark-nuts-reconstruction/index.html",
    "href": "2017/denmark-nuts-reconstruction/index.html",
    "title": "R, GIS, and fuzzyjoin to reconstruct demographic data for NUTS regions of Denmark",
    "section": "",
    "text": "NUTS stands for the Nomenclature of Territorial Units For Statistics. The history of NUTS dates back to the beginning of 1970s, when European countries developed unified standards for systems of administrative geography. It was not until the beginning of this century when such a system finally became widely used. There are three main hierarchical levels of NUTS, and the most commonly used for regional analysis is NUTS-2.\n\nFigure 1. Illustration of the principle of NUTS hierarchical system\nOne of the objectives of NUTS was to provide more or less comparable administrative divisions for all countries of Europe. Nevertheless, in 2013, population figures for single NUTS-2 regions ranged from 28.5 thousands in Aland island (Finland) to almost 12 million in Ile-de-France (Paris and surroundings, France).\nThe broken time series\nQuite arbitrary in its essence, territorial division tends to evolve. Changes in administrative boundaries can cause problems for regional analysis as they break the time series and therefore make it harder to analyze trends. Despite this inconvenience, the boundaries of regions actually change quite often based on the needs and interests of local or national governmenta. Eurostat tracks all modifications providing detailed explanations of all the changes that happen between versions of NUTS (figure 2).\nFigure 2. Changes in NUTS between versions 2006 and 2010\nDespite this, Eurostat does not recalculate historic demographic data to match the most recent NUTS version. This means that, for the most recent version of NUTS, there is missing data for all years before the latest administrative change. So researchers have to reconstruct historical data manually to obtain a long time series. Of course, crude assumptions often have to be accepted in order to approximate the population figures for the current regions that did not exist in the past.\nTo make thing even more complex, Eurostat provides the data only for the latest version of NUTS (at least, I did not work out how to download previous versions). In my PhD project I carry out regional analysis for the NUTS-2 regions of European Union. To have the longest possible time series, when I did the data preparation in 2015, I chose the 2010 version of NUTS, on which the regional demographic projection EUROPOP2013 is based. For reproducibility, I uploaded the precise versions of the Eurostat data at NUTS-2 level on population age structures and deaths, as downloaded in 2015, to figshare.\nDenmark\nSome countries had to perform major changes in their systems of territorial division to fit the NUTS standards. The most significant reform happened in Denmark in 2007, where the former 271 municipalities were transformed into the new 98 municipalities. At the same time, NUTS was introduced, so that 98 municipalities were allocated to 11 NUTS-3 regions, which aggregate to 5 NUTS-2 regions. Typically, for a small country, there is only one NUTS-1 region in Denmark, which is the whole country.\nAs far as I know, there was no official attempt of Eurostat to reconstruct the time series for Denmark before 2007. The typical map of Eurostat for the pre-2007 period shows Denmark as “no data available” country (figure 3).\nFigure 3. Life expectancy at birth in European NUTS-2 regions, 2006; a screenshot from the Eurostat’s interactive data exploratory tool\nSuch a data loss is somewhat surprising for a country such as Denmark. It might be quite difficult to match the old and new municipal systems; but it should be relatively easy to re-aggregate the old municipalities into the new (higher level) NUTS regions. That is precisely what I did during my data preparation1 and what I now want to share in this post.1 I have spent quite some time searching if someone else did the job before me and failed to find.\nThe task is basically to identify which of the old 271 municipalities are located within the modern 11 NUTS-3 regions and to aggregate the municipal data. Then, NUTS-3 data is easily aggregated for the NUTS-2 level. Such a task could have meant working late into the night, but luckily we live in the GIS era. I used GIS to match the old municipalities with the NUTS-3 regions. Here I want to show (with code) how the task can be performed using the amazing and opensource R. Below I show the process of matching old municipalities to the NUTS regions and the process that I used to aggregate population data.\nData\nThe data on the population age structures for the old 271 municipalities of Denmark was downloaded from the official website of Statistics Denmark. The system only allows you to grab up to 10K cells for unregistered users and up to 100K for registered users. So the process of downloading the data involves some tedious manual manipulations. For the purpose of my phd project, I downloaded the data for the period 2001-2006; but, if needed, the data is available since 1979. The data, downloaded in 2015 and ‘tidied up’ can be found here.\nI have spent a lot of time trying to find geodata with the boundaries of the old municipalities. Now, coming back to the topic more than 1.5 year later, I failed to identify the original source of the shapefile, though I am pretty sure that it came from here 2. The copy of the shapefile that I used can be found here.2 There is a note on the website saying that, due to a planned change in the structure of the website, there might be some problems with data accuisition. I failed to download the geodata on 2017-02-23.\nFinally, we need a shapefile of NUTS-3 regions. It can be easily downloaded from Eurostat geodata repository. The shapefile that I used is “NUTS_2010_20M_SH.zip”. The selection of the 11 Danish regions can be found here.\nThe projection used for both shapefiles is ESPG-3044, the one often used to map Denmark.\nNow, the code to prepare the R session and load the data.\n\n# set locale and encoding parameters to read Danish\nif(Sys.info()['sysname']==\"Linux\"){\n        Sys.setlocale(\"LC_CTYPE\", \"da_DK.utf8\")\n        danish_ecnoding <- \"WINDOWS-1252\"\n}else if(Sys.info()['sysname']==\"Windows\"){\n        Sys.setlocale(\"LC_CTYPE\", \"danish\")\n        danish_ecnoding <- \"Danish_Denmark.1252\"\n}\n\n# load required packages (install first if needed)\nlibrary(tidyverse) # version: 1.0.0\nlibrary(ggthemes) # version: 3.3.0\nlibrary(rgdal) # version: 1.2-4\nlibrary(rgeos) # version: 0.3-21\nlibrary(RColorBrewer) # version: 1.1-2\nmypal <- brewer.pal(11, \"BrBG\")\nlibrary(fuzzyjoin) # version: 0.1.2\nlibrary(viridis) # version: 0.3.4\n\n# load Denmark pop structures for the old municipalities\ndf <- read_csv(\"https://ikashnitsky.github.io/share/1703-nuts2-denmark/BEF1A.csv.gz\")\n\n# create a directory for geodata\nifelse(!dir.exists(\"geodata\"), dir.create(\"geodata\"), \"Directory already exists\")\n\n# download, unzip and read Danish NUTS-3 geodata (31KB)\nurl_nuts <- \"https://ikashnitsky.github.io/share/1703-nuts2-denmark/denmark-nuts3-espg3044.tgz\"\npath_nuts <- \"geodata/denmark-nuts3-espg3044.tgz\"\nifelse(!file.exists(path_nuts), download.file(url_nuts, path_nuts, mode=\"wb\"), 'file alredy exists')\n# If there are problems downloading the data automatically, please download it manually from\n# https://ikashnitsky.github.io/share/1703-nuts2-denmark/denmark-nuts3-espg3044.tgz\nuntar(tarfile = path_nuts, exdir = \"geodata\")\n\nsp_nuts3 <- readOGR(dsn = \"geodata/.\", layer = \"denmark-nuts3-espg3044\")\ngd_nuts3 <- fortify(sp_nuts3, region = \"NUTS_ID\") # to the ggplot format\n\n\n# download, unzip and read Danish old municipal geodata (6.0MB)\nurl_mun <- \"https://ikashnitsky.github.io/share/1703-nuts2-denmark/kommune2006win1252.tgz\"\npath_mun <- \"geodata/kommune2006win1252.tgz\"\nifelse(!file.exists(path_mun), download.file(url_mun, path_mun, mode=\"wb\"), 'file alredy exists')\n# If there are problems downloading the data automatically, please download it manually from\n# https://ikashnitsky.github.io/share/1703-nuts2-denmark/kommune2006utf8.tgz\nuntar(tarfile = path_mun, exdir = \"geodata\")\n\nsp_mun <- readOGR(dsn = \"geodata/.\", layer = \"kommune2006win1252\", encoding = danish_ecnoding) \ngd_mun <- fortify(sp_mun)\n\n# coordinates of the municipalities\nmun_coord <- bind_cols(as.data.frame(coordinates(sp_mun)), sp_mun@data[,1:3]) %>% \n        transmute(long = V1, lat = V2, enhedid, objectid, name = navn)\n\nSpatial matching\nLet’s first have a look at the map.\n\nggplot()+\n        geom_polygon(data = gd_nuts3, aes(long, lat, group = group), \n                     color = brbg[3], fill = \"grey90\", size = 1)+\n        geom_point(data = mun_coord, aes(long, lat), \n                   color = brbg[10], size = 1)+\n        theme_map()\n\nFigure 4. Reference map of the old municipalities and NUTS-3 regions of Denmark\nWe can easily see that the boundaries of the municipalities (light blue) are much more precise than that of the NUTS-3 regions (orange/brown). This is not a problem as long as all the centroids of the municipalities fall within the boundaries of the NUTS-3 regions, which seems to be true for all municipalities except for the easternmost one. A quick check reveals that this is Christiansø, a tiny fortified island, whose history goes back to the Middle Ages. It has a special status and is not included into the NUTS system. For further manipulations, Christiansø can safely merge it with the close-by Bornholm.\nTo identify which municipalities fall into which NUTS regions, I use the spatial overlap function (over) from sp package. Here I should thank Roger Bivand, a person who made it possible to do any spatial analysis in R.\n\n# municipality coordinates to Spatial\nmun_centr <- SpatialPoints(coordinates(sp_mun), proj4string = CRS(proj4string(sp_nuts3)))\n\n# spatial intersection with sp::over\ninter <- bind_cols(mun_coord, over(mun_centr, sp_nuts3[,\"NUTS_ID\"])) %>% \n        transmute(long, lat, objectid,\n                  nuts3 = as.character(NUTS_ID),\n                  nuts2 = substr(nuts3, 1, 4))\n\nLet’s again check visually if the spatial matching worked okay.\n\nggplot()+\n        geom_polygon(data = gd_mun, aes(long, lat, group = group), \n                     color = brbg[9], fill = \"grey90\", size = .1)+\n        geom_polygon(data = gd_nuts3, aes(long, lat, group = group), \n                     color = brbg[3], fill = NA, size = 1)+\n        geom_point(data = inter, aes(long, lat, color = nuts3), size = 1)+\n        geom_point(data = inter[is.na(inter$nuts3),], \n                   aes(long, lat), color = \"red\", size = 7, pch = 1)+\n        theme_map(base_size = 15)+\n        theme(legend.position = c(1, 1),\n              legend.justification = c(1, 1))\n\nFigure 5. Checking the spatial intersection between the old municipalities and NUTS-3 regions of Denmark\nNot bad. But there is an “NA” category that represents all the cases where the spatial match failed. How many such cases do we have?\n\n# how many failed cases do we have\nsum(is.na(inter$nuts3))\n\n\n## [1] 3\n\n\n# where the intersection failed\ninter[is.na(inter$nuts3),]\n\n\n##         long     lat objectid nuts3 nuts2\n## 23  892474.0 6147918    46399  <NA>  <NA>\n## 65  504188.4 6269329   105319  <NA>  <NA>\n## 195 533446.8 6312770    47071  <NA>  <NA>\n\nAs there are only 3 cases, I decided to fix them manually.\n\n# fix the three cases manually\nfixed <- inter\nfixed[fixed$objectid==\"46399\", 4:5] <- c(\"DK014\", \"DK01\")\nfixed[fixed$objectid==\"105319\", 4:5] <- c(\"DK041\", \"DK04\")\nfixed[fixed$objectid==\"47071\", 4:5] <- c(\"DK050\", \"DK05\")\n\nThe final visual check.\n\nggplot()+\n        geom_polygon(data = gd_mun, aes(long, lat, group = group), \n                     color = brbg[9], fill = \"grey90\", size = .1)+\n        geom_polygon(data = gd_nuts3, aes(long, lat, group = group), \n                     color = brbg[3], fill = NA, size = 1)+\n        geom_point(data = fixed, aes(long, lat, color = nuts3), size = 1)+\n        theme_map(base_size = 15)+\n        theme(legend.position = c(1, 1),\n              legend.justification = c(1, 1))\n\nFigure 6. Re-checking the spatial intersection between the old municipalities and NUTS-3 regions of Denmark\nNow everything seems okay.\nJoining spatial and statistical data (fuzzy join)\nThe next task is to join the spatial data and statistical data together. The spatial layer for municipalities does not contain the codes that are used by Statistics Denmark, so I have to match municipalities in the two datasets by their names. This is quite a difficult task. Names can be written slightly differently, there are some special characters in Danish alphabet, and some municipalities may have experienced a change of name. To solve the task most efficiently, I used the ‘Fuzzy String Matching’ approach which is implemented in the fuzzyjoin package by David Robinson.\nFirst, I simplify the names in both datasets turning them into lowercase, replacing the character “å” with “aa”, and removing the “Kommune” word in the spatial dataset names. Please note that I downloaded (separately) a small selection from Statistics Denmark to have a lightweight dataframe with municipal codes and names.\n\n# simplify municipalities names\nmun_geo <- mun_coord %>%         \n        transmute(name = sub(x = name, \" Kommune\", replacement = \"\"), objectid) %>% \n        mutate(name = gsub(x = tolower(name), \"å\", \"aa\"))\n\nmun_stat <- read.csv2(\"https://ikashnitsky.github.io/share/1703-nuts2-denmark/stat-codes-names.csv\", \n                      fileEncoding = danish_ecnoding) %>% \n        select(name) %>% \n        separate(name, into = c(\"code\", \"name\"), sep = \" \", extra = \"merge\") %>% \n        mutate(name = gsub(\"\\\\s*\\\\([^\\\\)]+\\\\)\", \"\", x = name)) %>% \n        mutate(name = gsub(x = tolower(name), \"å\", \"aa\"))\n\nLet’s try fuzzy join.\n\n# first attempt\nfuz_joined_1 <- regex_left_join(mun_geo, mun_stat, by = \"name\")\n\nThe resulting dataframe has 278 rows instead of 271. That means that for some municipalities in the spatial dataset there was more than one match. Let’s identify them.\n\n# identify more that 1 match (7 cases) and select which to drop\nfuz_joined_1 %>% group_by(objectid) %>% mutate(n = n()) %>% filter(n > 1)\n\n\n## Source: local data frame [14 x 5]\n## Groups: objectid [7]\n## \n##         name.x objectid  code      name.y     n\n##          <chr>    <dbl> <chr>       <chr> <int>\n## 1       haslev   105112   313      haslev     2\n## 2       haslev   105112   403       hasle     2\n## 3  brønderslev    47003   739       rønde     2\n## 4  brønderslev    47003   805 brønderslev     2\n## 5    hirtshals    47037   817        hals     2\n## 6    hirtshals    47037   819   hirtshals     2\n## 7      rønnede    46378   385     rønnede     2\n## 8      rønnede    46378   407       rønne     2\n## 9     hvidebæk    46268   317    hvidebæk     2\n## 10    hvidebæk    46268   681     videbæk     2\n## 11    ryslinge    46463   477    ryslinge     2\n## 12    ryslinge    46463   737          ry     2\n## 13     aarslev    46494   497     aarslev     2\n## 14     aarslev    46494   861        aars     2\n\nSo, for 7 municipalities, two matches were found. I will drop the imperfect match variants in the next iteration of fuzzy join.\nThe other issue is the municipalities for which no match was found in that statistical data.\n\n# show the non-matched cases\nfuz_joined_1 %>% filter(is.na(code))\n\n\n##              name.x objectid code name.y\n## 1              faxe   105120 <NA>   <NA>\n## 2 nykøbing falsters    46349 <NA>   <NA>\n## 3       herstederne    46101 <NA>   <NA>\n\nAs there are only three such cases, I corrected them manually in the spatial data to match the statistical data. There are two cases of a difference in the way the name of municipality are written and one case of name change.\n\n# correct the 3 non-matching geo names\nmun_geo_cor <- mun_geo\n\nmun_geo_cor[mun_geo_cor$name==\"faxe\", \"name\"] <- \"fakse\"\nmun_geo_cor[mun_geo_cor$name==\"nykøbing falsters\", \"name\"] <- \"nykøbing f.\"\nmun_geo_cor[mun_geo_cor$name==\"herstederne\", \"name\"] <- \"albertslund\"\n\nNow the second attempt to match the datasets (spatial dataset is corrected).\n\n# second attempt\nfuz_joined_2 <- regex_left_join(mun_geo_cor, mun_stat, by = \"name\")\n\n# drop non-perfect match\nfuz_joined_2 <- fuz_joined_2 %>% \n        group_by(objectid) %>% \n        mutate(n = n()) %>% \n        ungroup() %>% \n        filter(n < 2 | name.x==name.y)\n\nfuz_joined_2 <- fuz_joined_2 %>% transmute(name = name.x, objectid, code)\n\nThe output looks perfect. Now, the last step – using the matched “objectid” field, I will finally attach the NUTS data to statistical codes.\n\n# finally, attach the NUTS info to matched table\nkey <- left_join(fuz_joined_2, fixed, \"objectid\")\n\nAggregate old municipal data to NUTS levels\nThe previous manipulations yielded a dataframe that links statistical codes of the old municipalities with the corresponding NUTS regions. The last thing that has to be done is aggregation. I will attach the “key” dataset to a statistical dataset and aggregate the data at NUTS-3 and NUTS-2 levels.\n\n# finally, we only need to aggregate the old stat data\ndf_agr <- left_join(key, df, \"code\") %>% \n        filter(!is.na(name)) %>% \n        gather(\"year\", \"value\", y2001:y2006)\n\ndf_nuts3 <- df_agr %>% \n        group_by(year, sex, age, nuts3) %>% \n        summarise(value = sum(value)) %>% \n        ungroup()\n\ndf_nuts2 <- df_agr %>% \n        group_by(year, sex, age, nuts2) %>% \n        summarise(value = sum(value)) %>% \n        ungroup()\n\nLet’s now calculate the shares of working age population in Danish NUTS-3 regions in 2001 and map the information.\n\n# total population in 2001 by NUTS-3 regions\ntot_01 <- df_nuts3 %>% \n        filter(year==\"y2001\") %>% \n        group_by(nuts3) %>% \n        summarise(tot = sum(value, na.rm = TRUE)) %>% \n        ungroup()\n\n# working-age population in 2001 by NUTS-3 regions\nworking_01 <- df_nuts3 %>% \n        filter(year==\"y2001\", age %in% paste0(\"a0\", 15:64)) %>% \n        group_by(nuts3) %>% \n        summarise(work = sum(value, na.rm = TRUE)) %>% \n        ungroup()\n\n# calculate the shares of working age population\nsw_01 <- left_join(working_01, tot_01, \"nuts3\") %>% \n        mutate(sw = work / tot * 100)\n\n\n# map the shares of working age population in 2001 by NUTS-3 regions\nggplot()+\n        geom_polygon(data = gd_nuts3 %>% left_join(sw_01, c(\"id\" = \"nuts3\")),\n                     aes(long, lat, group = group, fill = sw), \n                     color = \"grey50\", size = 1) +\n        scale_fill_viridis()+\n        theme_map(base_size = 15)+\n        theme(legend.position = c(1, 1),\n              legend.justification = c(1, 1))\n\nFigure 7. The share of working age (15-64) population by NUTS-3 regions of Denmark in 2001\nThe result (thankfully!) looks realistic, with higher shares of the working-age population in the capital region, and in other regions that have relatively big cities.\n\n\n\n\n\n\n\nThis post is written for Demotrends"
  },
  {
    "objectID": "2017/gender-gap-in-swedish-mortality/index.html",
    "href": "2017/gender-gap-in-swedish-mortality/index.html",
    "title": "Gender gap in Swedish mortality",
    "section": "",
    "text": "Swedish context\nSweden, with its high quality statistical record since 1748, is the natural choice for any demographic study that aims to cover population dynamics during a long period of time.\nData\nThe data used for this visualization comes from Human Mortality Database. It can be easily accessed from an R session using HMDHFDplus package by Tim Riffe (for examples see my previous posts - one and two). For this exercise, I will use the dataset for Sweden that was provided for an application task for Rostock Retreat Visualization1.1 By using this data, I agree to the user agreement\n\nlibrary(tidyverse)\nlibrary(viridis)\nlibrary(extrafont)\n\n# download data\ndf_swe <- read_csv(\"http://www.rostock-retreat.org/files/application2017/SWE.csv\")\n# copy at https://ikashnitsky.github.io/doc/misc/application-rostock-retreat/SWE.csv\n\nyears <- c(1751, 1800, 1850, 1900, 1925, 1950, 1960, 1970, 1980, 1990, 2000, 2010)\n\n# select years and calculate male-to-female arte-ratio of mortality\ndf_selected <- df_swe %>% select(Year, Sex, Age, mx) %>% \n        filter(Year %in% years) %>% \n        spread(Sex, mx) %>% \n        transmute(year = Year, age = Age, value = m / f)\n\nVisualization\n\nggplot(df_selected)+\n        geom_hline(yintercept = 1, color = 'grey25', size = .5)+\n        geom_point(aes(age, value), size = 2, pch=1, color = 'grey50')+\n        stat_smooth(aes(age, value, group = 1, color = factor(year)), se = F)+\n        facet_wrap(~year, ncol = 3)+\n        labs(title = \"Male-to-female age-specific mortality rate ratio, Sweden\",\n             subtitle = \"Untill quite recent times, mortality of females was not much lower than that of males\",\n             caption = \"\\nData: Human Mortality Database (https://mortality.org)\n             Note: Colored lines are produced with loess smoothing\",\n             x = \"Age\", y = \"Rate ratio\")+\n        theme_minimal(base_size = 15, base_family = \"Roboto Condensed\") +\n        theme(legend.position = 'none',\n              plot.title = element_text(family = \"Roboto Mono\"))\n\n\nComment\nToday it is common knowledge that male mortality is always higher than female. There are more males being born, then eventually the sex ratio levels due to higher male mortality (see my previous post). Though, male mortality was not always much higher. Back in the days, when infant mortality was much higher and women used to have much higher fertility, there was almost no gender gap in age-specific mortality levels. Constant pregnancy and frequent childbirths had a strong negative impact on female health and survival statistics. We can see that only in the second half of the 20-th century gender gap in mortality became substantial in Sweden.\n\n\n\n\n\n\n\nThis post is based on my earlier tweet and gist"
  },
  {
    "objectID": "2017/ggplot2-microbenchmark/index.html",
    "href": "2017/ggplot2-microbenchmark/index.html",
    "title": "Accelerating ggplot2: use a canvas to speed up plots creation",
    "section": "",
    "text": "One of the nice features of the ggapproach to plotting is that one can save plots as R objects at any step and use later to render and/or modify. I used that feature extensively while creating maps with ggplot2 (see my previous posts: one, two, three, four, five). It is just convenient to first create a canvas with all the theme parameters appropriate for a map, and then overlay the map layer. At some point I decided to check if that workflow was computationally efficient or not. To my surprise, the usage of canvas reduces the creation time of a ggplot quite a lot. To my further surprise, this finding holds for simple plots as well as maps.\nLet’s start with a simple check.\nFigure 1. Microbenchmark output for a simple plot\nThe median time of execution is 3.24 milliseconds for the plot without canvas and 2.29 milliseconds for the plot with canvas.\nNext, let’s do the same check for a map. For that, I will use the data prepared for one of my earlier posts and recreate the simple map that shows the division of European Union 27 into three subregions.\nFigure 2. The map we use to test the plot creation speed\nFigure 3. Microbenchmark output for a map\nThe median time of execution is 18.8 milliseconds for the map without canvas and 6.3 milliseconds for the map with canvas."
  },
  {
    "objectID": "2017/ggplot2-microbenchmark/index.html#conclusion-use-canvas-with-ggplot2",
    "href": "2017/ggplot2-microbenchmark/index.html#conclusion-use-canvas-with-ggplot2",
    "title": "Accelerating ggplot2: use a canvas to speed up plots creation",
    "section": "Conclusion: Use canvas with ggplot2\n",
    "text": "Conclusion: Use canvas with ggplot2\n\n\n\n\n\n\n\n\nFor the full script to reproduce the results check out this gist"
  },
  {
    "objectID": "2017/global-male-life-expectancy-convergence/index.html",
    "href": "2017/global-male-life-expectancy-convergence/index.html",
    "title": "Global convergence in male life expectancy at birth",
    "section": "",
    "text": "In the modern history, the world has seen unprecedented decrease in human mortality – the result of the Demographic Transition. Initially, the improvements occurred only in the most developed societies, and by the mid XX century the world population was roughly divided in two parts according to mortality patterns (see the bi-modal distribution). After the 2nd World War, the developing countries started to catch up, and there was a clear convergence in life expectancy at birth, the most common summary measure of mortality.\n\n\n\n\n\n\n\n\nTo reproduce the plot from the scratch please see the gist"
  },
  {
    "objectID": "2017/hello-r-world/index.html",
    "href": "2017/hello-r-world/index.html",
    "title": "Hello R world post",
    "section": "",
    "text": "Welcome to my blog!\nMy name is Ilya, I am a demographer. Here I plan to post some research related stuff. As I am crazy about R, my post will touch upon this tool/environment – I will share some tricks and cool visualizations. Eventually, I hope to contribute to the amazing R-bloggers project. First, I am going to post older bits – to gain momentum.\n\nJust to start with, let me show you a small and handy self-written R function.\nQuite often, visualizing data in R, we compose color palettes manually. It is nice to have a function that shows the actual colors of a vector with color values. Here it is.\n\nglimpse_colors <- function(colors_string){\n        n <- length(colors_string)\n        hist(1:n, breaks = 0:n, col = colors_string)\n}\n\nThe function takes a vector of colors as input and produces a basic uniform histogram with one bar for each color, filled accordingly. Let’s try it out. First, we create some colors, and then visualize them.\n\nlibrary(RColorBrewer)\npal <- brewer.pal(n = 9, name = 'BrBG')\nglimpse_colors(pal)\n\n\n\n\nEnjoy!\nP.S. If something similar exists in one of the well known packages, please tell me."
  },
  {
    "objectID": "2017/hmd-all-sex-ratio/index.html",
    "href": "2017/hmd-all-sex-ratio/index.html",
    "title": "Sex ratios in all countries from Human Mortality Database",
    "section": "",
    "text": "Sex ratios reflect the two basic regularities of human demographics: 1) there are always more boys being born; 2) males experience higher mortality throughout their life-course. The sex ratio at birth does not vary dramatically1 and is more or less constant at the level of 105-106 boys per 100 girls. Hence, differences in the sex ratio profiles of countries mainly reflect gender gap in mortality. In this post I will compare sex ratios age profiles in all countries included in Human Mortality Database.1 There are cases of big deviations from this natural constant. The most well known one is the skewed sex ratio in China, where decades of One Child Policy together with strong traditional son preference resulted in selective abortions. Read more: Frejka et al. (2010); Feng (2011); Basten and Verropoulou (2013).\nR gives amazing opportunities to grab data fast and easy. Thanks to Tim Riffe’s HMDHFDplus package, one can now download HMD data with just a couple of lines of R code.\nThere is a handy function in HMDHFDplus package – getHMDcountries(). It lists the codes for all countries in HMD. So it becomes really easy to loop through the database and download data for all countries.\n\n# load required packages\nlibrary(tidyverse) # version 1.0.0\nlibrary(HMDHFDplus) # version 1.1.8\n\ncountry <- getHMDcountries()\n\nexposures <- list()\nfor (i in 1: length(country)) {\n        cnt <- country[i]\n        exposures[[cnt]] <- readHMDweb(cnt, \"Exposures_1x1\",\n                                       ik_user_hmd, ik_pass_hmd)\n        \n        # let's print the progress\n        paste(i,'out of',length(country)) \n}\n\n\n\n\n\n\n\nUse own credentials\n\n\n\nPlease note, the arguments ik_user_hmd and ik_pass_hmd are my login credentials at the website of Human Mortality Database, which are stored locally at my computer. In order to access the data, one needs to create an account at www.mortality.org and provide his own credentials to the readHMDweb() function.\n\n\nNext, I select 2012 for comparison – it is quite recent, and for most of the HMD countries there are data for 2012. The loop goes through each of the countries’ dataframe in exposures list, selects data for 2012 and calculates sex ratio at each age. I also remove data for several populations (like East and West Germany separately).\n\nsr_age <- list()\n\nfor (i in 1:length(exposures)) {\n        di <- exposures[[i]]\n        sr_agei <- di %>% select(Year,Age,Female,Male) %>% \n                filter(Year %in% 2012) %>%\n                select(-Year) %>%\n                transmute(country = names(exposures)[i],\n                          age = Age, sr_age = Male / Female * 100)\n        sr_age[[i]] <- sr_agei\n}\nsr_age <- bind_rows(sr_age)\n\n# remove optional populations\nsr_age <- sr_age %>% filter(!country %in% c(\"FRACNP\",\"DEUTE\",\"DEUTW\",\"GBRCENW\",\"GBR_NP\"))\n\nAfter age 90, sex ratios become quite jerky due to the relatively small numbers of survivors. I decided to aggregate data after the age 90.\n\n# summarize all ages older than 90 (too jerky)\nsr_age_90 <- sr_age %>% filter(age %in% 90:110) %>% \n        group_by(country) %>% summarise(sr_age = mean(sr_age, na.rm = T)) %>%\n        ungroup() %>% transmute(country, age=90, sr_age)\n\ndf_plot <- bind_rows(sr_age %>% filter(!age %in% 90:110), sr_age_90)\n\nFinally, I plot the resulting sex ratios.\n\n# get nice font\nlibrary(extrafont)\nmyfont <- \"Roboto Condensed\"\n\n# finaly - plot\ngg <- ggplot(df_plot, aes(age, sr_age, color = country, group = country))+\n        geom_hline(yintercept = 100, color = 'grey50', size = 1)+\n        geom_line(size = 1)+\n        scale_y_continuous(limits = c(0, 120), expand = c(0, 0), breaks = seq(0, 120, 20))+\n        scale_x_continuous(limits = c(0, 90), expand = c(0, 0), breaks = seq(0, 80, 20))+\n        xlab('Age')+\n        ylab('Sex ratio, males per 100 females')+\n        facet_wrap(~country, ncol=6)+\n        theme_minimal(base_family = myfont, base_size = 15)+\n        theme(legend.position='none',\n              panel.border = element_rect(size = .5, fill = NA))\n\ngg\n\n\nThere is quite a variety in the sex ratio profiles. If the initial prevalence of males equalizes in Japan, Sweden, or Norway at around 60, in Russia, Belarus, and Ukraine this happens at around 30 due to very high male mortality. In many countries there are pronounced bumps in the sex ratio at ages 20-30, that are likely to be caused by international migration. For example, Scotland, Northern Ireland, Portugal, and New Zealand are experiencing substantial outflow of young men.\nWhat happened in Taiwan?\n\n\n\n\n\n\n\nThis post is based on my earlier tweet and gist\n\n\n\n\n\n\n\n\n\n\n\nReferences\n\nBasten S, Verropoulou G. 2013. “Maternity migration” and the increased sex ratio at birth in Hong Kong SAR. Population Studies 67: 323–334 DOI: 10.1080/00324728.2013.826372\n\n\nFeng W. 2011. The Future of a Demographic Overachiever: Long-Term Implications of the Demographic Transition in China. Population and Development Review 37: 173–190 DOI: 10.1111/j.1728-4457.2011.00383.x\n\n\nFrejka T, Jones GW, Sardon J-P. 2010. East Asian Childbearing Patterns and Policy Developments. Population and Development Review 36: 579–606 DOI: 10.1111/j.1728-4457.2010.00347.x"
  },
  {
    "objectID": "2017/hmd-male-mortality-rus-jpn/index.html",
    "href": "2017/hmd-male-mortality-rus-jpn/index.html",
    "title": "Male mortality in Russia and Japan",
    "section": "",
    "text": "Russia is sadly notorious for its ridiculously high adult male mortality. According to Human Mortality Database data (2010), the probability for a Russian men to survive from 20 to 60 was just 0.64 1. For women the probability is 0.87. This huge gender disproportion in mortality results in a peculiar sex ratio profile (see my old DemoTrends post and the previous blog post).1 To compare, the same probabilities for males in some developed countries are: France (0.89), Japan (0.92), US (0.87), UK (0.91).\nNow let’s compare age-specific mortality rates of Russian men to that of the Japanese. For years and years Japan performs best in reducing mortality. It became standard to compare mortality schedules of other countries to the Japanese one 2.2 See for example the recent NIDI working paper of Balachandran et. al (2017).\nFirst, I need to get HMD data for both Russian and Japanese males. Again, I am using the amazing R package HMDHFDplus of Tim Riffe to download HMD data with just a couple of lines of R code.\n\n# load required packages\nlibrary(tidyverse) # version 1.0.0\nlibrary(HMDHFDplus) # version 1.1.8\n\n# load life tables for men, RUS and JPN\nrus <- readHMDweb('RUS', \"mltper_1x1\", ik_user_hmd, ik_pass_hmd)\njpn <- readHMDweb('JPN', \"mltper_1x1\", ik_user_hmd, ik_pass_hmd)\n\n\n\n\n\n\n\nUse own credentials\n\n\n\nPlease note, the arguments ik_user_hmd and ik_pass_hmd are my login credentials at the website of Human Mortality Database, which are stored locally at my computer. In order to access the data, one needs to create an account at www.mortality.org and provide his own credentials to the readHMDweb() function.\n\n\nNext, I select the most recent year for comparison, 2014, and compute the rate ratio of age specific mortality rates.\n\n# compare mortality rates for 2014\nru <- rus %>% filter(Year == 2014) %>% transmute(age = Age, rus = mx)\njp <- jpn %>% filter(Year == 2014) %>% transmute(age = Age, jpn = mx)\ndf <- left_join(jp, ru, 'age') %>% mutate(ru_rate = rus / jpn)\n\nFinally, I plot the resulting rate ratio of male mortality in Russia and Japan.\n\n# get nice font\nlibrary(extrafont)\nmyfont <- \"Roboto Condensed\"\n\n# plot\ngg <- ggplot(df, aes(age, ru_rate)) + \n        geom_hline(yintercept = 1, color = 'red') +\n        geom_line(aes(group=1)) + \n        scale_y_continuous('mortality rate ratio',\n                           breaks = 0:10, labels = 0:10, limits = c(0, 10)) +\n        annotate('text',x=c(0, 55), y = c(1.75,5), \n                 color = c('red','black'), hjust = 0, vjust = 1, size = 7,\n                 label = c('Japan','Russia'), family = myfont) +\n        ggtitle('Compare age-specific mortality of males',\n                subtitle = \"Russia and Japan, 2014, HMD\")+\n        theme_bw(base_size = 15, base_family = myfont)\n\n\nIn the middle ages, male mortality in Russian is up to 10 times higher than in Japan!\n\n\n\n\n\n\n\nThis post is based on my earlier tweet and gist"
  },
  {
    "objectID": "2017/ice-hockey-players-height/index.html",
    "href": "2017/ice-hockey-players-height/index.html",
    "title": "Evolution of ice hockey players’ height: IIHF world championships 2001-2016",
    "section": "",
    "text": "The 2017 Ice Hockey World Championship has started. Thus I want to share a small research on the height of ice hockey players that I did almost a year ago and published in Russian.\nWhen the TV camera shows the players returning to the changing rooms, it is difficult not to notice just how huge the players are compared to the surrounding people – fans, journalists, coaches, or the ice arena workers. For example, here are the rising stars of the Finnish hockey – Patrik Laine and Aleksander Barkov – with the two fans in between.\nSource\nSo the questions arise. Are ice hockey players really taller than average people? How is the height of ice hockey players evolving over time? Are there any lasting differences between countries?\nData\nIIHF, the organization that is in charge for the ice hockey world championships, publishes detailed information on the squads, including the data on player’s height and weight. The raw data files are here. I gathered the data of all players that participated in the 16 world championships between 2001 and 2016. The formatting of the data files changes from year to year complicating the data processing. So I did the data cleaning manually which took a bit more than 3 hours. The unifies dataset is here. Let’s load the data and prepare the R session.\n\n# load required packages\nlibrary(tidyverse) # data manipulation and viz\nlibrary(lubridate) # easy manipulations with dates\nlibrary(ggthemes) # themes for ggplot2\nlibrary(texreg) # easy export of regression tables\nlibrary(xtable) # export a data frame into an html table\nlibrary(sysfonts) # change the font in figures\n\n\n# download the IIHF data set; if there are some problems, you can download manually\n# using the stable URL (https://dx.doi.org/10.6084/m9.figshare.3394735.v2)\ndf <- read.csv(\"https://ndownloader.figshare.com/files/5303173\")\n\n# color palette\nbrbg11 <- RColorBrewer::brewer.pal(11, \"BrBG\")\n\nDo the players become taller? (a crude comparison)\nLet’s first have a look at the pulled average height of all the players that participated.\n\n# mean height by championship\ndf_per <- df %>% group_by(year) %>%\n        summarise(height = mean(height))\n\ngg_period_mean <- ggplot(df_per, aes(x = year, y = height))+\n        geom_point(size = 3, color = brbg11[9])+\n        stat_smooth(method = \"lm\", size = 1, color = brbg11[11])+\n        ylab(\"height, cm\")+\n        xlab(\"year of competition\")+\n        scale_x_continuous(breaks = seq(2005, 2015, 5), labels = seq(2005, 2015, 5))+\n        theme_few(base_size = 15, base_family = \"mono\")+\n        theme(panel.grid = element_line(colour = \"grey75\", size = .25))\n\n\ngg_period_jitter <- ggplot(df, aes(x = year, y = height))+\n        geom_jitter(size = 2, color = brbg11[9], alpha = .25, width = .75)+\n        stat_smooth(method = \"lm\", size = 1, se = F, color = brbg11[11])+\n        ylab(\"height, cm\")+\n        xlab(\"year of competition\")+\n        scale_x_continuous(breaks = seq(2005, 2015, 5), labels = seq(2005, 2015, 5))+\n        theme_few(base_size = 15, base_family = \"mono\")+\n        theme(panel.grid = element_line(colour = \"grey75\", size = .25))\n\ngg_period <- cowplot::plot_grid(gg_period_mean, gg_period_jitter)\n\n\nFigure 1. The dynamics of the average height of the ice hockey players at the world championships, 2001–2016\nThe positive trend is evident. In the 15 years the average height of a player increased by almost 2 cm (left panel). Is that a lot? To have an idea, we will compare this growth to the dynamics in the population, later in the post.\nCohort approach\nA more correct way to study the dynamics of players’ height is to do the comparison between birth cohorts. Here we face an interesting data preparation issue – some of the players participated in more that one championships. The question is: do we need to clean the duplicate records? If the goal is to see the average height of a player at the certain championship (as in Figure 1), it is reasonable to keep all the records. Alternatively, if the aim is to analyze the dynamics of players’ height itself, I argue, it would be wrong to assign bigger weight to those players that participated in more that one championship. Thus, for the further cohort analysis, I cleaned the dataset from the duplicates.\n\ndfu_h <- df %>% select(year, name, country, position, birth, cohort, height) %>%\n        spread(year, height)\ndfu_h$av.height <- apply(dfu_h[, 6:21], 1, mean, na.rm = T)\ndfu_h$times_participated <- apply(!is.na(dfu_h[, 6:21]), 1, sum)\n\ndfu_w <- df %>% select(year, name, country, position, birth, cohort, weight) %>%\n        spread(year, weight)\ndfu_w$av.weight <- apply(dfu_w[, 6:21], 1, mean, na.rm = T)\n\n\ndfu <- left_join(dfu_h %>% select(name, country, position, birth, cohort, av.height, times_participated), \n                 dfu_w %>% select(name, country, position, birth, cohort, av.weight), \n                 by = c(\"name\", \"country\", \"position\", \"birth\", \"cohort\")) %>%\n        mutate(bmi = av.weight / (av.height / 100) ^ 2)\n\nThe total number of observations decreased from 6292 to 3333. For those who participated in more that one championship, I averaged the data on height and weight as they can change during the life-course. How many times, on average, are ice hockey players honored to represent their countries in the world championships? A bit less than 2.\n\n# frequencies of participation in world championships\nmean(dfu$times_participated)\n\ndf_part <- as.data.frame(table(dfu$times_participated))\n\ngg_times_part <- ggplot(df_part, aes(y = Freq, x = Var1))+\n        geom_bar(stat = \"identity\", fill = brbg11[8])+\n        ylab(\"# of players\")+\n        xlab(\"times participated (out of 16 possible)\")+\n        theme_few(base_size = 15, base_family = \"mono\")+\n        theme(panel.grid = element_line(colour = \"grey75\", size = .25))\n\n\nFigure 2. Histogram of the players by the number of times they participated in world championships over the period 2001-2016.\nBut there are unique players that participated in a considerable number of championships. Let’s have a look at those who participated at least 10 times out of 16 possible. There were just 14 such players.\n\n# the leaders of participation in world championships\nleaders <- dfu %>% filter(times_participated > 9)\nView(leaders)\n# save the table to html\nprint(xtable(leaders), type = \"html\", file = \"table_leaders.html\")\n\nTable 1. The most frequently participated players\n\n\n\nname\n\n\ncountry\n\n\nposition\n\n\nbirth date\n\n\ncohort\n\n\nav.height\n\n\ntimes _participated\n\n\nav.weight\n\n\nbmi\n\n\n\n\novechkin alexander\n\n\nRUS\n\n\nF\n\n\n1985-09-17\n\n\n1985\n\n\n188.45\n\n\n11\n\n\n98.36\n\n\n27.70\n\n\n\n\nnielsen daniel\n\n\nDEN\n\n\nD\n\n\n1980-10-31\n\n\n1980\n\n\n182.27\n\n\n11\n\n\n79.73\n\n\n24.00\n\n\n\n\nstaal kim\n\n\nDEN\n\n\nF\n\n\n1978-03-10\n\n\n1978\n\n\n182.00\n\n\n10\n\n\n87.80\n\n\n26.51\n\n\n\n\ngreen morten\n\n\nDEN\n\n\nF\n\n\n1981-03-19\n\n\n1981\n\n\n183.00\n\n\n12\n\n\n85.83\n\n\n25.63\n\n\n\n\nmasalskis edgars\n\n\nLAT\n\n\nG\n\n\n1980-03-31\n\n\n1980\n\n\n176.00\n\n\n12\n\n\n79.17\n\n\n25.56\n\n\n\n\nambuhl andres\n\n\nSUI\n\n\nF\n\n\n1983-09-14\n\n\n1983\n\n\n176.80\n\n\n10\n\n\n83.70\n\n\n26.78\n\n\n\n\ngranak dominik\n\n\nSVK\n\n\nD\n\n\n1983-06-11\n\n\n1983\n\n\n182.00\n\n\n10\n\n\n79.50\n\n\n24.00\n\n\n\n\nmadsen morten\n\n\nDEN\n\n\nF\n\n\n1987-01-16\n\n\n1987\n\n\n189.82\n\n\n11\n\n\n86.00\n\n\n23.87\n\n\n\n\nredlihs mikelis\n\n\nLAT\n\n\nF\n\n\n1984-07-01\n\n\n1984\n\n\n180.00\n\n\n10\n\n\n80.40\n\n\n24.81\n\n\n\n\ncipulis martins\n\n\nLAT\n\n\nF\n\n\n1980-11-29\n\n\n1980\n\n\n180.70\n\n\n10\n\n\n82.10\n\n\n25.14\n\n\n\n\nholos jonas\n\n\nNOR\n\n\nD\n\n\n1987-08-27\n\n\n1987\n\n\n180.18\n\n\n11\n\n\n91.36\n\n\n28.14\n\n\n\n\nbastiansen anders\n\n\nNOR\n\n\nF\n\n\n1980-10-31\n\n\n1980\n\n\n190.00\n\n\n11\n\n\n93.64\n\n\n25.94\n\n\n\n\nask morten\n\n\nNOR\n\n\nF\n\n\n1980-05-14\n\n\n1980\n\n\n185.00\n\n\n10\n\n\n88.30\n\n\n25.80\n\n\n\n\nforsberg kristian\n\n\nNOR\n\n\nF\n\n\n1986-05-05\n\n\n1986\n\n\n184.50\n\n\n10\n\n\n87.50\n\n\n25.70\n\n\n\nAlexander Ovechkin – 11 times! But it has to be noted that not every player had a possibility to participate in all the 16 championships between 2001 and 2016. That depends on a numder of factors:\n- the birth cohort of the player; - whether his national team regularly qualified for the championship (Figure 3); - whether the player was good enough for the national team; - whether he was free from the NHL play-offs that often keep the best players off the world championships.\n\n# countries times participated\ndf_cnt_part <- df %>% select(year, country, no) %>%\n        mutate(country = factor(paste(country))) %>%\n        group_by(country, year) %>%\n        summarise(value = sum(as.numeric(no))) %>%\n        mutate(value = 1) %>%\n        ungroup() %>%\n        mutate(country = factor(country, levels = rev(levels(country))), \n               year = factor(year))\n\nd_cnt_n <- df_cnt_part %>% group_by(country) %>%\n        summarise(n = sum(value))\n\ngg_cnt_part <- ggplot(data = df_cnt_part, aes(x = year, y = country))+\n        geom_point(color = brbg11[11], size = 7)+\n        geom_text(data = d_cnt_n, aes(y = country, x = 17.5, label = n, color = n), size = 7, fontface = 2)+\n        geom_text(data = d_cnt_n, aes(y = country, x = 18.5, label = \" \"), size = 7)+\n        scale_color_gradientn(colours = brbg11[7:11])+\n        xlab(NULL)+\n        ylab(NULL)+\n        theme_bw(base_size = 25, base_family = \"mono\")+\n        theme(legend.position = \"none\", \n              axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5))\n\n\nFigure 3. Stats of the national teams participation in the world championships\nDo the ice hochey players become taller? (regression analysis)\nThe regression analysis allows to address the research question – the association between player’s height and birth cohort – accounting for the cross-national differences and player’s position. I use OLS regressions, that are quite sensitive to outliers. I removed the birth cohorts for which there are less than 10 players – 1963, 1997, and 1998.\n\n# remove small cohorts\ntable(dfu$cohort)\ndfuc <- dfu %>% filter(cohort < 1997, cohort > 1963)\n\nSo, the results. I add the variables one by one.\nDependent variable: player’s height.Explaining variables: 1) birth cohort; 2) position (compared to defenders); 3) country (compared to Russia).\n\n# relevel counrty variable to compare with Russia\ndfuc$country <- relevel(dfuc$country, ref = \"RUS\")\n\n# regression models\nm1 <- lm(data = dfuc, av.height~cohort)\nm2 <- lm(data = dfuc, av.height~cohort+position)\nm3 <- lm(data = dfuc, av.height~cohort+position+country)\n\n# export the models to html\nhtmlreg(list(m1, m2, m3), file = \"models_height.html\", single.row = T)\n\nTable2. The models\n\n\n\n\n\nModel 1\n\n\nModel 2\n\n\nModel 3\n\n\n\n\n(Intercept)\n\n\n-10.17 (27.67)\n\n\n-18.64 (27.01)\n\n\n32.59 (27.00)\n\n\n\n\ncohort\n\n\n0.10 (0.01)***\n\n\n0.10 (0.01)***\n\n\n0.08 (0.01)***\n\n\n\n\npositionF\n\n\n\n\n-2.59 (0.20)***\n\n\n-2.59 (0.20)***\n\n\n\n\npositionG\n\n\n\n\n-1.96 (0.31)***\n\n\n-1.93 (0.30)***\n\n\n\n\ncountryAUT\n\n\n\n\n\n\n-0.94 (0.55)\n\n\n\n\ncountryBLR\n\n\n\n\n\n\n-0.95 (0.53)\n\n\n\n\ncountryCAN\n\n\n\n\n\n\n1.13 (0.46)*\n\n\n\n\ncountryCZE\n\n\n\n\n\n\n0.56 (0.49)\n\n\n\n\ncountryDEN\n\n\n\n\n\n\n-0.10 (0.56)\n\n\n\n\ncountryFIN\n\n\n\n\n\n\n0.20 (0.50)\n\n\n\n\ncountryFRA\n\n\n\n\n\n\n-2.19 (0.69)**\n\n\n\n\ncountryGER\n\n\n\n\n\n\n-0.61 (0.51)\n\n\n\n\ncountryHUN\n\n\n\n\n\n\n-0.61 (0.86)\n\n\n\n\ncountryITA\n\n\n\n\n\n\n-3.58 (0.61)***\n\n\n\n\ncountryJPN\n\n\n\n\n\n\n-5.24 (0.71)***\n\n\n\n\ncountryKAZ\n\n\n\n\n\n\n-1.16 (0.57)*\n\n\n\n\ncountryLAT\n\n\n\n\n\n\n-1.38 (0.55)*\n\n\n\n\ncountryNOR\n\n\n\n\n\n\n-1.61 (0.62)**\n\n\n\n\ncountryPOL\n\n\n\n\n\n\n0.06 (1.12)\n\n\n\n\ncountrySLO\n\n\n\n\n\n\n-1.55 (0.58)**\n\n\n\n\ncountrySUI\n\n\n\n\n\n\n-1.80 (0.53)***\n\n\n\n\ncountrySVK\n\n\n\n\n\n\n1.44 (0.50)**\n\n\n\n\ncountrySWE\n\n\n\n\n\n\n1.18 (0.48)*\n\n\n\n\ncountryUKR\n\n\n\n\n\n\n-1.82 (0.59)**\n\n\n\n\ncountryUSA\n\n\n\n\n\n\n0.54 (0.45)\n\n\n\n\nR2\n\n\n0.01\n\n\n0.06\n\n\n0.13\n\n\n\n\nAdj. R2\n\n\n0.01\n\n\n0.06\n\n\n0.12\n\n\n\n\nNum. obs.\n\n\n3319\n\n\n3319\n\n\n3319\n\n\n\n\nRMSE\n\n\n5.40\n\n\n5.27\n\n\n5.10\n\n\n\nModel 1. One year change in the birth cohort year is associated with an increase of 0.1 cm in height. The coefficient is statistically significant, yet the variable explains only 1% of the variance. That’s not a big problem since the aim of the modeling is to document the differences, rather than predict based on the model. Nevertheless, the low coefficient of determination means that there are other variables that explain the differences in players’ height better than just the birth cohort.\nModel 2. Defenders are the tallest ice hockey players: goalkeepers are 2 cm shorter, forwards are 2.6 cm shorter. All the coefficients are significant; R squared rose to 6%. It is worth noting that the coefficient for the birth cohort did not change when we added the new variable.\nModel 3. It is interesting to control for countries for two reasons. First, some of the differences are significant themselves. For example, Swedes, Slovaks, and Canadians are higher than Russians. In contrast, Japanese are 5.2 cm shorter, Italians – 3.6 cm, French – 2.2 cm (figure 4). Second, once the country controls are introduced, the coefficient for birth cohort decreased slightly meaning that some of the differences in height are explained by persisting cross-country differences. R squared rose to 13%.\n\n# players' height by country\ngg_av.h_country <- ggplot(dfuc , aes(x = factor(cohort), y = av.height))+\n        geom_point(color = \"grey50\", alpha = .25)+\n        stat_summary(aes(group = country), geom = \"line\", fun.y = mean, size = .5, color = \"grey50\")+\n        stat_smooth(aes(group = country, color = country), geom = \"line\", size = 1)+\n        facet_wrap(~country, ncol = 4)+\n        coord_cartesian(ylim = c(170, 195))+\n        scale_x_discrete(labels = paste(seq(1970, 1990, 10)), breaks = paste(seq(1970, 1990, 10)))+\n        labs(x = \"birth cohort\", y = \"height, cm\")+\n        theme_few(base_size = 15, base_family = \"mono\")+\n        theme(legend.position = \"none\", \n              panel.grid = element_line(colour = \"grey75\", size = .25))\n\n\nFigure 4. The height of ice hockey players by nations\nThe last model indicates that from one birth cohort cohort to the other the height of ice hockey players increases 0.08 cm. That means an increase of 0.8 cm in a decade or a growth of 2.56 cm in the 32 years between 1964 and 1996. It is worth mentioning that once we run the analysis in cohorts and controlling for positions and nations, the speed of the player’s height increase becomes much humbler than in the crude pulled analysis (Figure 1): 0.8 cm per decade compared to 1.2 cm per decade.\nBefore we go further and compare the growth in player’s height to that of the population, let’s do the modeling separately for defenders, goalkeepers, and forwards. The exploratory plot (Figure 5) suggests that the correlation is stronger for goalkeepers and weaker for defenders.\n\ndfuc_pos <- dfuc\nlevels(dfuc_pos$position) <- c(\"Defenders\", \"Forwards\", \"Goalkeeprs\")\n\ngg_pos <- ggplot(dfuc_pos , aes(x = cohort, y = av.height))+\n        geom_jitter(aes(color = position), alpha = .5, size = 2)+\n        stat_smooth(method = \"lm\", se = T, color = brbg11[11], size = 1)+\n        scale_x_continuous(labels = seq(1970, 1990, 10), breaks = seq(1970, 1990, 10))+\n        scale_color_manual(values = brbg11[c(8, 9, 10)])+\n        facet_wrap(~position, ncol = 3)+\n        xlab(\"birth cohort\")+\n        ylab(\"height, cm\")+\n        theme_few(base_size = 15, base_family = \"mono\")+\n        theme(legend.position = \"none\", \n              panel.grid = element_line(colour = \"grey75\", size = .25))\n\n\nFigure 5. Correlation between height and birth cohort by position\n\n# separate models for positions\nm3d <- lm(data = dfuc %>% filter(position == \"D\"), av.height~cohort+country)\nm3f <- lm(data = dfuc %>% filter(position == \"F\"), av.height~cohort+country)\nm3g <- lm(data = dfuc %>% filter(position == \"G\"), av.height~cohort+country)\nhtmlreg(list(m3d, m3f, m3g), file = \"models_height_pos.html\", single.row = T, \n        custom.model.names = c(\"Model 3 D\", \"Model 3 F\", \"Model 3 G\"))\n\nTable 3. Model 3 – separately for defenders, forwards, and goalkeepers\n\n\n\n\n\nModel 3 D\n\n\nModel 3 F\n\n\nModel 3 G\n\n\n\n\n(Intercept)\n\n\n108.45 (46.46)*\n\n\n49.32 (36.73)\n\n\n-295.76 (74.61)***\n\n\n\n\ncohort\n\n\n0.04 (0.02)\n\n\n0.07 (0.02)***\n\n\n0.24 (0.04)***\n\n\n\n\ncountryAUT\n\n\n0.14 (0.96)\n\n\n-2.01 (0.75)**\n\n\n0.47 (1.47)\n\n\n\n\ncountryBLR\n\n\n0.30 (0.87)\n\n\n-1.53 (0.73)*\n\n\n-2.73 (1.55)\n\n\n\n\ncountryCAN\n\n\n1.55 (0.78)*\n\n\n0.39 (0.62)\n\n\n3.45 (1.26)**\n\n\n\n\ncountryCZE\n\n\n0.87 (0.84)\n\n\n0.30 (0.67)\n\n\n0.63 (1.36)\n\n\n\n\ncountryDEN\n\n\n-0.60 (0.95)\n\n\n0.10 (0.75)\n\n\n-0.19 (1.62)\n\n\n\n\ncountryFIN\n\n\n-0.55 (0.89)\n\n\n-0.04 (0.67)\n\n\n2.40 (1.32)\n\n\n\n\ncountryFRA\n\n\n-3.34 (1.15)**\n\n\n-2.06 (0.93)*\n\n\n1.39 (2.07)\n\n\n\n\ncountryGER\n\n\n0.48 (0.85)\n\n\n-1.40 (0.72)\n\n\n-0.65 (1.33)\n\n\n\n\ncountryHUN\n\n\n-1.32 (1.47)\n\n\n-0.70 (1.16)\n\n\n0.65 (2.39)\n\n\n\n\ncountryITA\n\n\n-2.08 (1.08)\n\n\n-4.78 (0.82)***\n\n\n-2.02 (1.62)\n\n\n\n\ncountryJPN\n\n\n-4.13 (1.26)**\n\n\n-6.52 (0.94)***\n\n\n-2.27 (1.98)\n\n\n\n\ncountryKAZ\n\n\n-1.23 (0.95)\n\n\n-1.82 (0.79)*\n\n\n1.79 (1.58)\n\n\n\n\ncountryLAT\n\n\n-0.73 (0.95)\n\n\n-1.39 (0.75)\n\n\n-3.42 (1.49)*\n\n\n\n\ncountryNOR\n\n\n-3.25 (1.07)**\n\n\n-1.06 (0.85)\n\n\n-0.10 (1.66)\n\n\n\n\ncountryPOL\n\n\n0.82 (1.89)\n\n\n-0.58 (1.55)\n\n\n0.37 (2.97)\n\n\n\n\ncountrySLO\n\n\n-1.57 (0.99)\n\n\n-1.54 (0.79)\n\n\n-2.25 (1.66)\n\n\n\n\ncountrySUI\n\n\n-1.98 (0.91)*\n\n\n-2.36 (0.71)***\n\n\n1.12 (1.47)\n\n\n\n\ncountrySVK\n\n\n2.94 (0.87)***\n\n\n0.81 (0.67)\n\n\n-0.70 (1.50)\n\n\n\n\ncountrySWE\n\n\n0.75 (0.81)\n\n\n1.24 (0.65)\n\n\n1.37 (1.33)\n\n\n\n\ncountryUKR\n\n\n-1.37 (1.01)\n\n\n-1.77 (0.80)*\n\n\n-3.71 (1.66)*\n\n\n\n\ncountryUSA\n\n\n0.76 (0.78)\n\n\n-0.08 (0.62)\n\n\n2.58 (1.26)*\n\n\n\n\nR2\n\n\n0.09\n\n\n0.10\n\n\n0.24\n\n\n\n\nAdj. R2\n\n\n0.07\n\n\n0.09\n\n\n0.20\n\n\n\n\nNum. obs.\n\n\n1094\n\n\n1824\n\n\n401\n\n\n\n\nRMSE\n\n\n5.08\n\n\n5.08\n\n\n4.87\n\n\n\nThe separate modeling shows that the average height of ice hockey players, that were born in 1964-1996 and participated in the world championships in 2001–2016, increased with the speed of 0.4 cm per decade for defenders, 0.7 cm – for forwards, and (!) 2.4 cm – for goalies. In three decades the average height of the goalkeepers increased by 7 cm!\nFinally, let’s compare these dynamics with those in the population.\nCompare to population\nOur previous results expose significant height differences between players of various nations. Thus, it is reasonable to compare ice hockey players’ height to the corresponding male population of their countries.\nFor the data on the height of males in population in the corresponding nations I used the relevant scientific paper. I grabbed the data from the paper PDF using a nice little tool – tabula – and also deposited on figshare.\n\n# download the data from Hatton, T. J., & Bray, B. E. (2010).\n# Long run trends in the heights of European men, 19th–20th centuries.\n# Economics & Human Biology, 8(3), 405–413.\n# http://doi.org/10.1016/j.ehb.2010.03.001\n# stable URL, copied data (https://dx.doi.org/10.6084/m9.figshare.3394795.v1)\n\ndf_hb <- read.csv(\"https://ndownloader.figshare.com/files/5303878\") \n\ndf_hb <- df_hb %>%\n        gather(\"country\", \"h_pop\", 2:16) %>%\n        mutate(period = paste(period)) %>%\n        separate(period, c(\"t1\", \"t2\"), sep = \"/\")%>%\n        transmute(cohort = (as.numeric(t1)+as.numeric(t2))/2, country, h_pop)\n\n# calculate hockey players' cohort height averages for each country\ndf_hoc <- dfu %>% group_by(country, cohort) %>%\n        summarise(h_hp = mean(av.height)) %>%\n        ungroup()\n\nUnfortunately, our dataset on ice hockey players intersects with the data on population only for 8 countries: Austria, Denmark, Finland, France, Germany, Italy, Norway, and Sweden.\n\n# countries in both data sets\nboth_cnt <- levels(factor(df_hb$country))[which(levels(factor(df_hb$country)) %in% levels(df_hoc$country))]\nboth_cnt\n\n\ngg_hoc_vs_pop <- ggplot()+\n        geom_path(data = df_hb %>% filter(country %in% both_cnt), \n                  aes(x = cohort, y = h_pop), \n                  color = brbg11[9], size = 1)+\n        geom_point(data = df_hb %>% filter(country %in% both_cnt), \n                   aes(x = cohort, y = h_pop), \n                   color = brbg11[9], size = 2)+\n        geom_point(data = df_hb %>% filter(country %in% both_cnt), \n                   aes(x = cohort, y = h_pop), \n                   color = \"white\", size = 1.5)+\n        geom_point(data = df_hoc %>% filter(country %in% both_cnt), \n                   aes(x = cohort, y = h_hp), \n                   color = brbg11[3], size = 2, pch = 18)+\n        stat_smooth(data = df_hoc %>% filter(country %in% both_cnt), \n                    aes(x = cohort, y = h_hp), \n                    method = \"lm\", se = F, color = brbg11[1], size = 1)+\n        facet_wrap(~country, ncol = 2)+\n        labs(y = \"height, cm\", x = \"birth cohort\")+\n        theme_few(base_size = 20, base_family = \"mono\")+\n        theme(panel.grid = element_line(colour = \"grey75\", size = .25))\n\n\nFigure 6. The comparison of height dynamics in ice hockey players (brown) and the corresponding male populations (green)\nIn all the analyzed countries, ice hockey players are 2-5 cm higher that the nation’s average. This is not very surprising since we expect some selection in sport. What is more interesting, in the developed countries the rapid increase in the height of males mostly leveled off in the birth cohorts of 1960s. Unlike the population trend, the height of ice hockey players continued to increase with roughly the same pace in all the analyzed countries except for Denmark.\nFor the cohorts of Europeans that were born in first half of 20-th century, the height of males increased by 1.18–1.74 cm per decade (Figure 7, middle panel). Starting from the birth cohorts of 1960s, the pace decreased to 0.15–0.80 per decade.\n\n# growth in population\n\ndf_hb_w <- df_hb %>% spread(cohort, h_pop) \nnames(df_hb_w)[2:26] <- paste(\"y\", names(df_hb_w)[2:26])\n\ndiffs <- df_hb_w[, 3:26]-df_hb_w[, 2:25]\n\ndf_hb_gr<- df_hb_w %>%\n        transmute(country, \n                  gr_1961_1980 = unname(apply(diffs[, 22:24], 1, mean, na.rm = T))*2, \n                  gr_1901_1960 = unname(apply(diffs[, 9:21], 1, mean, na.rm = T))*2, \n                  gr_1856_1900 = unname(apply(diffs[, 1:8], 1, mean, na.rm = T))*2) %>%\n        gather(\"period\", \"average_growth\", 2:4) %>%\n        filter(country %in% both_cnt) %>%\n        mutate(country = factor(country, levels = rev(levels(factor(country)))), \n               period = factor(period, labels = c(\"1856-1900\", \"1901-1960\", \"1961-1980\")))\n\n\ngg_hb_growth <- ggplot(df_hb_gr, aes(x = average_growth, y = country))+\n        geom_point(aes(color = period), size = 3)+\n        scale_color_manual(values = brbg11[c(8, 3, 10)])+\n        scale_x_continuous(limits = c(0, 2.15))+\n        facet_wrap(~period)+\n        theme_few()+\n        xlab(\"average growth in men's height over 10 years, cm\")+\n        ylab(NULL)+\n        theme_few(base_size = 20, base_family = \"mono\")+\n        theme(legend.position = \"none\", \n              panel.grid = element_line(colour = \"grey75\", size = .25))\n\n\nFigure 7. Average changes in male population\nThe height increase for ice hockey players seems quite impressive if we compare it to the stagnating dynamics in the corresponding male populations. And the acceleration of goalkeepers’ height is outright amazing.\nThe diverging trends in the height of ice hockey players and normal population is likely to be driven by the strengthening selection in sport.\nSelection in ice hockey\nLooking through the literature on the selection in sport, I saw the finding that showed a notable disproportion of professional sportsmen by the month of birth. There are much more sportsmen that were born in the first half of the year. They have a lasting advantage since the kids teams are usually formed by birth cohorts. Thus, those born earlier in the year always have a bit more time lived compared to their later born team mates, which means that they are physically more mature. It is easy to test the finding on our ice hockey players dataset.\n\n# check if there are more players born in earlier months\ndf_month <- df %>% mutate(month = month(birth)) %>%\n        mutate(month = factor(month))\n\ngg_month <- ggplot(df_month, aes(x = factor(month)))+\n        geom_bar(stat = \"count\", fill = brbg11[8])+\n        scale_x_discrete(breaks = 1:12, labels = month.abb)+\n        labs(x = \"month of birth\", y = \"# of players\")+\n        theme_few(base_size = 20, base_family = \"mono\")+\n        theme(legend.position = \"none\", \n              panel.grid = element_line(colour = \"grey75\", size = .25))\n\n\nFigure 8. The distribution of ice hockey players by month of birth\nTrue, the distribution is notably skewed – there are much more players born in earlier months. When I further split the dataset by the decades of birth, it becomes clear that the effect becomes more evident with time (Figure 9). Indirectly, that means that the selection in ice hockey becomes tougher.\n\n# facet by decades\ndf_month_dec <- df_month %>%\n        mutate(dec = substr(paste(cohort), 3, 3) %>% \n                       factor(labels = paste(\"born in\", c(\"1960s\", \"1970s\", \"1980s\", \"1990s\"))))\n\ngg_month_dec <- ggplot(df_month_dec, aes(x = factor(month)))+\n        geom_bar(stat = \"count\", fill = brbg11[8])+\n        scale_x_discrete(breaks = 1:12, labels = month.abb)+\n        labs(x = \"month of birth\", y = \"# of players\")+\n        facet_wrap(~dec, ncol = 2, scales = \"free\")+\n        theme_few(base_size = 20, base_family = \"mono\")+\n        theme(legend.position = \"none\", \n              panel.grid = element_line(colour = \"grey75\", size = .25))\n\n\nFigure 9. The distribution of ice hockey players by month of birth – separately by decades of birth\n\n\n\n\n\n\n\nThe full R script can be downloaded here"
  },
  {
    "objectID": "2017/map-hacking/index.html",
    "href": "2017/map-hacking/index.html",
    "title": "Hacking maps with ggplot2",
    "section": "",
    "text": "This is a very short post on mapping with ggplot2.\nQuite often, mapping some data, we do not need to follow scrupulously the formal requirements to geographical maps – the idea is just to show the spatial dimension of the data. For instance, the network of rivers is not the most important information when we map the elections outcome. Thus, the simplified mapping allows quite some freedom in transforming the geodata. The classical example of such geodata transformation is the replacement and scaling of Alaska and Hawaii to be mapped alongside the mainland of the US. As one may see in this example, usually such geodata transformations utilize quite complex GIS tools in order to reposition an object in the coordinate system.\nThe interesting feature of mapping with ggplot2 is that, before the actual plotting, geodata has to be fotrified (ggplot2::fortify) – transformed to a simple dataframe object. Since fortified geodata is basically a dataframe, some simple transformations could be made really easily.\nIn my last paper, I needed to show a two-dimensional grouping of the European NUTS-2 regions in 4 quadrants according to GDP per capita and the share of working-age population (see Figure 8 in the preprint). In line with the study setting, I did the grouping separately for Western, Southern, and Eastern Europe. I decided that the most straightforward way to show that on map would be to visually separate the 3 subregions of Europe. The task is easily doable through triggering the fortified geodata object – see the code below.\nFirst, the code to prepare the R session and load the (already prepared) data.\n\nlibrary(tidyverse) # version 1.1.1\nlibrary(extrafont) # version 0.17\nlibrary(ggthemes) # version 3.4.0\nfont <- \"Roboto Condensed\"\nlibrary(hrbrthemes) # version 0.1.0\n# The code is tested on a PC-win7-x64\n# R version 3.3.3\n\n\n# load the prepared geodata and stat data\nload(url(\"https://ikashnitsky.github.io/share/1704-map-hacking/map-hacking.Rdata\"))\n\n# fortify the spatial objects\nbord <- fortify(Sborders)\nfort <- fortify(Sn2, region = 'id')\n\nNext, I hack the geodata (long and lat variables) moving groups of NUTS-2 regions (Western, Southern, and Eastern Europe) apart. The appropriate values to move the groups of regions were found empirically.\n\n# hack geodata to separate macro-regions\nfort_hack <- fort %>% \n        left_join(df %>% select(id, subregion), 'id') %>% \n        mutate(long = ifelse(subregion=='E', long + 5e5, long),\n               long = ifelse(subregion=='S', long + 2e5, long),\n               lat = ifelse(subregion=='S', lat - 5e5, lat),\n               long = ifelse(subregion=='W', long - 2e5, long))\n\nFinally, we are ready to create the schematic map.\n\n# create color pallete\nbrbg <- RColorBrewer::brewer.pal(11,\"BrBG\")\nbrbg4 <- brbg[c(4,9,2,11)]\n\n# create the two-dim legend\nggleg <- ggplot()+\n        coord_equal(xlim = c(0,1), ylim = c(0,1), expand = c(0,0))+\n        annotate('rect', xmin = .45, xmax = .6, ymin = .1, ymax = .25, \n                 fill = brbg4[1], color = NA)+\n        annotate('rect', xmin = .45, xmax = .6, ymin = .4, ymax = .55, \n                 fill = brbg4[2], color = NA)+\n        annotate('rect', xmin = .75, xmax = .9, ymin = .1, ymax = .25, \n                 fill = brbg4[3], color = NA)+\n        annotate('rect', xmin = .75, xmax = .9, ymin = .4, ymax = .55, \n                 fill = brbg4[4], color = NA)+\n        annotate('rect', xmin = .05, xmax = .95, ymin = .05, ymax = .95, \n                 fill = NA, color = \"grey20\")+\n        \n        annotate('text', x = .35, y = c(.175, .475), vjust = .5, hjust = 1,\n                 size = 6, fontface = 2, label = c('POOR', 'RICH'), family = font) + \n        annotate('text', x = c(.525, .825), y = .65, vjust = 0, hjust = .5,\n                 size = 6, fontface = 2, label = c('LOW', 'HIGH'), family = font)+\n        annotate('text', x = .1, y = .9, vjust = 1, hjust = 0,\n                 size = 7, fontface = 2, label = \"LEGEND\", family = font)+\n        theme_map()\n\n# create the blank map\nbasemap <- ggplot()+\n        coord_equal(ylim=c(900000,5400000), xlim=c(2500000, 7000000), expand = c(0,0))+\n        theme_map()+\n        theme(panel.border=element_rect(color = 'black',size=.5,fill = NA),\n              legend.position = 'none')\n\n# the main map\nmap_temp <- basemap + \n        geom_map(map = fort_hack, data = df, aes(map_id=id, fill=group))+\n        scale_fill_manual(values = brbg4[c(3, 1, 4, 2)])\n\n# now combine the map and the legend\nmap <- ggplot() + \n        coord_equal(xlim = c(0,1), ylim = c(0,1), expand = c(0,0))+\n        annotation_custom(ggplotGrob(map_temp), xmin = 0, xmax = 1, ymin = 0, ymax = 1)+\n        annotation_custom(ggplotGrob(ggleg), xmin = 0.72, xmax = 0.99, ymin = 0.72, ymax = 0.99)+\n        labs(title = \"Labour force and income in EU-27 NUTS-2 regions\",\n             subtitle = \"Within each of the three macro-regions of Europe - Westren, Southern, and Eastern -\\nNUTS-2 regions are classified in 4 groups according to the level of GDP per capita\\nand the share of working age population in 2008\",\n             caption = \"Data: Eurostat\\nAuthor: Ilya Kashnitsky (ikashnitsky.github.io)\")+\n        theme_ipsum_rc(plot_title_size = 30, subtitle_size = 20, caption_size = 15)\n\nAnd here is the result."
  },
  {
    "objectID": "2017/neet-in-europe/index.html",
    "href": "2017/neet-in-europe/index.html",
    "title": "Young people neither in employment nor in education and training in Europe, 2000-2016",
    "section": "",
    "text": "As an example of Eurostat data usage I chose to show the dynamics of NEET (Young people neither in employment nor in education and training) in European countries. The example is using the brilliant geofact package.\n\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(forcats)\nlibrary(eurostat)\nlibrary(geofacet)\nlibrary(viridis)\nlibrary(ggthemes)\nlibrary(extrafont)\n\n# Find the needed dataset code \n# http://ec.europa.eu/eurostat/web/regions/data/database\n\n# download fertility rates for countries\nneet <- get_eurostat(\"edat_lfse_22\")\n\n# if the automated download does not work, the data can be grabbed manually at\n# http://ec.europa.eu/eurostat/estat-navtree-portlet-prod/BulkDownloadListing\n\nneet %>% \n        filter(geo %>% paste %>% nchar == 2,\n               sex == \"T\", age == \"Y18-24\") %>%\n        group_by(geo) %>% \n        mutate(avg = values %>% mean()) %>% \n        ungroup() %>% \n        ggplot(aes(x = time %>% year(),\n                   y = values))+\n        geom_path(aes(group = 1))+\n        geom_point(aes(fill = values), pch = 21)+\n        scale_x_continuous(breaks = seq(2000, 2015, 5),\n                           labels = c(\"2000\", \"'05\", \"'10\", \"'15\"))+\n        scale_y_continuous(expand = c(0, 0), limits = c(0, 40))+\n        scale_fill_viridis(\"NEET, %\", option = \"B\")+\n        facet_geo(~ geo, grid = \"eu_grid1\")+\n        labs(x = \"Year\",\n             y = \"NEET, %\",\n             title = \"Young people neither in employment nor in education and training in Europe\",\n             subtitle = \"Data: Eurostat Regional Database, 2000-2016\",\n             caption = \"ikashnitsky.github.io\")+\n        theme_few(base_family =  \"Roboto Condensed\", base_size = 15)+\n        theme(axis.text = element_text(size = 10),\n              panel.spacing.x = unit(1, \"lines\"),\n              legend.position = c(0, 0),\n              legend.justification = c(0, 0))\n\n\n\n\n\n\n\n\n\nThe whole code may be downloaded from the gist"
  },
  {
    "objectID": "2017/subplots-in-maps/index.html",
    "href": "2017/subplots-in-maps/index.html",
    "title": "Subplots in maps with ggplot2",
    "section": "",
    "text": "Following the surprising success of my latest post, I decided to show yet another use case of the handy annotation custom function. Here I will show how to add small graphical information to maps – just like putting a stamp on an envelope.\nThe example comes from my current work on a paper, in which I study the effect of urban/rural differences on the relative differences in population ageing (I plan to tell a bit more in one of the next posts). Let’s have a look at the map we are going to reproduce in this post:\n\nSo, with this map I want to show the location of more and less urbanized NUTS-2 regions of Europe. But I also want to show – with subplots – how I defined the three subregions of Europe (Eastern, Southern, and Western) and what is the relative frequency of the three categories of regions (Predominantly Rural, Intermediate, and Predominantly Rural) within each of the subregions. The logic of actions is simple: first prepare all the components, then assemble them in a composite plot. Let’s go!\nThe code to prepare R session and load the data.\n\n# additional packages\nlibrary(tidyverse)\nlibrary(ggthemes)\nlibrary(rgdal)\nlibrary(viridis)\nlibrary(RColorBrewer)\nlibrary(extrafont)\nmyfont <- \"Roboto Condensed\"\n\n# load the already prepared data\nload(url(\"https://ikashnitsky.github.io/share/1705-map-subplots/df-27-261-urb-rur.RData\"))\nload(url(\"https://ikashnitsky.github.io/share/1705-map-subplots/spatial-27-261.RData\"))\n\nNow, I prepare the spatial objects to be plotted with ggplot2 and create a blank map of Europe – our canvas.\n\n# fortify spatial objects\nbord <- fortify(Sborders)\nfort <- fortify(Sn2, region = \"id\")\n\n# join spatial and statistical data\nfort_map <- left_join(df,fort,\"id\")\n\n# create a blank map\nbasemap <- ggplot()+\n        geom_polygon(data = fortify(Sneighbors),aes(x = long, y = lat, group = group),\n                     fill = \"grey90\",color = \"grey90\")+\n        coord_equal(ylim = c(1350000,5450000), xlim = c(2500000, 6600000))+\n        theme_map(base_family = myfont)+\n        theme(panel.border = element_rect(color = \"black\",size = .5,fill = NA),\n              legend.position = c(1, 1),\n              legend.justification = c(1, 1),\n              legend.background = element_rect(colour = NA, fill = NA),\n              legend.title = element_text(size = 15),\n              legend.text = element_text(size = 15))+\n        scale_x_continuous(expand = c(0,0)) +\n        scale_y_continuous(expand = c(0,0)) +\n        labs(x = NULL, y = NULL)\n\n\nOkay, now the envelope is ready. It’s time to prepare the stamps. Let’s create a nice mosaic plot showing the distribution of NUTS-2 regions by subregions and the urb/rur categories. I found the simplest way to create a nice mosaic plot on Stack Overflow.\n\n# create a nice mosaic plot; solution from SO:\n# http://stackoverflow.com/a/19252389/4638884\nmakeplot_mosaic <- function(data, x, y, ...){\n        xvar <- deparse(substitute(x))\n        yvar <- deparse(substitute(y))\n        mydata <- data[c(xvar, yvar)];\n        mytable <- table(mydata);\n        widths <- c(0, cumsum(apply(mytable, 1, sum)));\n        heights <- apply(mytable, 1, function(x){c(0, cumsum(x/sum(x)))});\n        \n        alldata <- data.frame();\n        allnames <- data.frame();\n        for(i in 1:nrow(mytable)){\n                for(j in 1:ncol(mytable)){\n                        alldata <- rbind(alldata, c(widths[i], \n                                                    widths[i+1], \n                                                    heights[j, i], \n                                                    heights[j+1, i]));\n                }\n        }\n        colnames(alldata) <- c(\"xmin\", \"xmax\", \"ymin\", \"ymax\")\n        \n        alldata[[xvar]] <- rep(dimnames(mytable)[[1]], \n                               rep(ncol(mytable), nrow(mytable)));\n        alldata[[yvar]] <- rep(dimnames(mytable)[[2]], nrow(mytable));\n        \n        ggplot(alldata, aes(xmin=xmin, xmax=xmax, ymin=ymin, ymax=ymax)) + \n                geom_rect(color=\"white\", aes_string(fill=yvar)) +\n                xlab(paste(xvar, \"(count)\")) + \n                ylab(paste(yvar, \"(proportion)\"));\n}\n\ntyp_mosaic <- makeplot_mosaic(data = df %>% mutate(type = as.numeric(type)), \n                              x = subregion, y = type)+\n        theme_void()+\n        scale_fill_viridis(option = \"B\", discrete = T, end = .8)+\n        scale_y_continuous(limits = c(0, 1.4))+\n        annotate(\"text\",x = c(27, 82.5, 186), y = 1.05, \n                 label=c(\"EAST\", \"SOUTH\", \"WEST\"), \n                 size = 4, fontface = 2, \n                 vjust = 0.5, hjust = 0,\n                 family = myfont) + \n        coord_flip()+\n        theme(legend.position = \"none\")\n\n\nJust what we needed. The next step is to build a small map showing the three subregions of Europe. But before we proceed to the maps, one thing has to be fixed. ggplot2 fails rendering nested polygons. With our regional dataset, London, for example, will not be shown if we do not account for this unpleasant feature. Luckily, there is quite a simple solution to fix that problem.\n\n# a nice small function to overcome some mapping problems with nested polygons\n# see more at SO\n# https://stackoverflow.com/questions/21748852\ngghole <- function (fort) {\n        poly <- fort[fort$id %in% fort[fort$hole, ]$id, ]\n        hole <- fort[!fort$id %in% fort[fort$hole, ]$id, ]\n        out <- list(poly, hole)\n        names(out) <- c(\"poly\", \"hole\")\n        return(out)\n}\n\nNow I build the small map of subregions.\n\n# pal for the subregions\nbrbg3 <- brewer.pal(11,\"BrBG\")[c(8,2,11)]\n\n# annotate a small map of the subregions of Europe\nan_sub <- basemap +\n        geom_polygon(data = gghole(fort_map)[[1]], \n                     aes(x = long, y = lat, group = group, fill = subregion),\n                     color = NA)+\n        geom_polygon(data  =  gghole(fort_map)[[2]], \n                     aes(x = long, y = lat, group = group, fill = subregion),\n                     color = NA)+\n        scale_fill_manual(values = rev(brbg3)) +\n        theme(legend.position = \"none\")\n\n\nFinally, everything is ready to build the main map and stick the two subplots on top of it.\n\n# finally the map of Urb/Rur typology\n\ncaption <- \"Classification: De Beer, J., Van Der Gaag, N., & Van Der Erf, R. (2014). New classification of urban and rural NUTS 2 regions in Europe. NIDI Working Papers, 2014/3. Retrieved from http://www.nidi.nl/shared/content/output/papers/nidi-wp-2014-03.pdf\n\\nIlya Kashnitsky (ikashnitsky.github.io)\"\n\ntyp <-  basemap + \n        \n        geom_polygon(data = gghole(fort_map)[[1]], \n                     aes(x=long, y=lat, group=group, fill=type),\n                     color=\"grey30\",size=.1)+\n        geom_polygon(data = gghole(fort_map)[[2]], \n                     aes(x=long, y=lat, group=group, fill=type),\n                     color=\"grey30\",size=.1)+\n        scale_fill_viridis(\"NEUJOBS\\nclassification of\\nNUTS-2 regions\", \n                           option = \"B\", discrete = T, end = .8)+\n        geom_path(data = bord, aes(x = long, y = lat, group = group),\n                  color = \"grey20\",size = .5) + \n        \n        annotation_custom(grob = ggplotGrob(typ_mosaic), \n                          xmin = 2500000, xmax = 4000000, \n                          ymin = 4450000, ymax = 5450000)+\n        annotation_custom(grob = ggplotGrob(an_sub), \n                          xmin = 5400000, xmax = 6600000, \n                          ymin = 2950000, ymax = 4150000)+\n        labs(title = \"Urban / Rural classification of NUTS-2 regions of Europe\\n\",\n             caption = paste(strwrap(caption, width = 95), collapse = '\\n'))+\n        theme(plot.title = element_text(size = 20),\n              plot.caption = element_text(size = 12))\n\n\nDone!\nOf course, it takes several iterations to position each element in its proper place. Then, one also needs to play with export parameters to finally get the desired output.\n\n\n\n\n\n\n\nThe full R script for this post is here"
  },
  {
    "objectID": "2017/who-is-old/index.html",
    "href": "2017/who-is-old/index.html",
    "title": "Who is old? Visualizing the concept of prospective ageing with animated population pyramids",
    "section": "",
    "text": "This post is about illustrating the concept of prospective ageing, a relatively fresh approach in demography to refine our understanding of population ageing. This visualization was created in collaboration with my colleague Michael Boissonneault: (mostly) his idea and (mostly) my implementation. The animated visualization builds upon Michael’s viz prepared for the submission to the highly anticipated event at the end June 2017 – Rostock Retreat Visualization. My visualization of the provided Swedish dataset can be found in the previous post."
  },
  {
    "objectID": "2017/who-is-old/index.html#prospective-ageing",
    "href": "2017/who-is-old/index.html#prospective-ageing",
    "title": "Who is old? Visualizing the concept of prospective ageing with animated population pyramids",
    "section": "Prospective ageing",
    "text": "Prospective ageing\nOver the past decades the alarmist views of the upcoming population ageing disaster became widely spread. True, with the growing number of countries approaching the ending of the Demographic Transition, the average/median age of their population increases rapidly, which is something unprecedented in the documented human history. But does that imply an unbearable burden of elderly population in the nearest future? Not necessarily.\nThe demographic prospects depend a lot on how we define ageing. Quite recently Waren Sanderson and Sergei Scherbov proposed 1 2 a new way to look at population ageing, they called it Prospective Ageing. The underlying idea is really simple – age is not static: a person aged 65 (the conventional border deliminating elderly population) today is in many aspects not the same as a person ages 65 half a century ago. Health and lifespan improved a lot in the last decades, meaning that today people generally have much more remaining years of life at the moment of being recognized as elderly by the conventional standards. Thus, Sanderson and Scherbov proposed to define elderly population based on the estimation of the expected remaining length of life rather than years lived. Such a refined view of population ageing disqualifies the alarmist claims of the approaching demographic collapse. The would be paradoxical title of one the latest papers of Sanderson and Scherbov 3 summarizes the phenomenon nicely: Faster Increases in Human Life Expectancy Could Lead to Slower Population Aging.1 Sanderson W, Scherbov S. 2005. Average remaining lifetimes can increase as human populations age. Nature 435: 811–813 DOI: 10.1038/nature035932 Sanderson W, Scherbov S. 2010. Remeasuring Aging. Science 329: 1287–1288 DOI: 10.1126/science.11936473 Sanderson WC, Scherbov S. 2015. Faster Increases in Human Life Expectancy Could Lead to Slower Population Aging. PLoS ONE 10: e0121922 DOI: 10.1371/journal.pone.0121922\nOf course, the choice of the new ageing threshold is a rather arbitrary question 4. It became usual to define this threshold at the remaining life expectancy of 15 years.4 See the working paper of my colleagues devoted to this question"
  },
  {
    "objectID": "2018/ddd-poster/index.html",
    "href": "2018/ddd-poster/index.html",
    "title": "Compare population age structures of Europe NUTS-3 regions and the US counties using ternary color-coding",
    "section": "",
    "text": "On 28 November 2018 I presented a poster at Dutch Demography Day in Utrecht. Here it is:\n\nThe poster compares population age structures, represented as ternary compositions in three broad age groups, of European NUTS-3 regions and the United States counties. I used ternary color-coding, a dataviz approach that Jonas Schöley and me recently brought to R in tricolore package.\nIn these maps, each region’s population age composition is uniquely color-coded. Colors show direction and magnitude of deviation from the center point, which represents the average age composition. Hue component of a color encodes the direction of deviation: towards yellow – more elderly population (65+); cyan – more people at working ages (15–64); magenta–more kids (<15).\nOf course, NUTS-3 regions and the US counties are not perfect to compare; on average, NUTS-3 regions are roughly ten times bigger. That’s why the colors for European regions look quite muted, they are closer to the grey average composition.\nThe poster won NVD Poster Award via online voting of the conference participants.\n\n\n\n\n\n\n\n\nReplication\n\n\n\nThis time I layouted the poster in Inkscape rather than arranging everything with hundreds of R code lines. But all the elements of the posted are reproducible with code from this github repo.\n\n\n\n\n\n\n\n\nSee also\n\n\n\n\nKashnitsky, I., & Schöley, J. (2018). Regional population structures at a glance. The Lancet, 392(10143), 209–210.\nMy PhD project – Regional demographic convergence in Europe\nPaper (Schöley & Willekens 2017) with the initial ideas for tricolore package\nAn example of ternary colorcoding used to visualize cause-of-death data"
  },
  {
    "objectID": "2018/deep-catalan-roots/index.html#here-is-the-table-for-guys",
    "href": "2018/deep-catalan-roots/index.html#here-is-the-table-for-guys",
    "title": "Deep Catalan roots: playing with stringdist",
    "section": "Here is the table for guys:",
    "text": "Here is the table for guys:\n\nMy personal favorite here is Hanbo Wo becoming Antonio Duc."
  },
  {
    "objectID": "2018/deep-catalan-roots/index.html#and-a-similar-table-for-girls",
    "href": "2018/deep-catalan-roots/index.html#and-a-similar-table-for-girls",
    "title": "Deep Catalan roots: playing with stringdist",
    "section": "And a similar table for girls:",
    "text": "And a similar table for girls:\n\nHere I like Elena Bastianelli turning to Elena Albanell."
  },
  {
    "objectID": "2018/perfect-rstudio-layout/index.html",
    "href": "2018/perfect-rstudio-layout/index.html",
    "title": "A perfect RStudio layout",
    "section": "",
    "text": "Tiny things can separate life into “before” and “after”. Here is one. For almost a year I’ve been daily sending mental “thank you” to Ugo who showed me how to re-organize panes in RStudio. Since then I’ve been spreading this tiny improvement so many times that I thought the tiny advise deserved a separate tiny post. Please note, below is an opinionated view of a comfortable UI improvement; feel free to ignore it if you don’t like. This advise is highly subjective, though, I really believe it is useful.\nI find the default 4-pane layout of RStudio is not perfect. One needs more space for the “Source” pane. Especially when RStudio is used as the main text editor, i.e. the program to write code, papers, blog posts, prepare presentations… Thus, the perfect solution is to move “Console” to the top-right position, leave least useful “History” in the bottom-left corner and collapse it, and move everything else to the bottom-right corner (see the screenshot).\n\nJust go to “Tools” –> “Global options” –> “Pane layout” and fix it.\n\nThat’s it!\nJust enjoy your improved RStudio, the program.\nP.S. It is also very handy to memorize and use the hot keys for panes. CTRL + # moves focus to the pane, CTRL + SHIFT + # maximizes the pane."
  },
  {
    "objectID": "2018/sjrdata-package/index.html",
    "href": "2018/sjrdata-package/index.html",
    "title": "sjrdata: all SCImago Journal & Country Rank data, ready for R",
    "section": "",
    "text": "SCImago Journal & Country Rank provides valuable estimates of academic journals’ prestige. The data is freely available at the project website and is distributed for deeper analysis in forms of .csv and .xlsx files. I downloaded all the files and pooled them together, ready to be used in R.\nBasically, all the package gives you three easily accessible data frames: sjr_journals (Journal Rank), sjr_countries (Country Rank, year-by-year), and sjr_countries_1996_2017 (Country Rank, all years together).\nThe whole process of data acquisition can be found in the github repo (dev directory) or this gist.\nHow to use sjrdata\n\nInstall the package from github, load it and use the data.\nThe installation will take a while since the main dataset sjr_journals is pretty heavy (15.7MB compressed).\n\n# install\ndevtools::install_github(\"ikashnitsky/sjrdata\")\n\n# load\nlibrary(sjrdata)\n\n# use\nView(sjr_countries)\n\nA couple of examples\nLet’s compare Nature and Science.\n\nlibrary(tidyverse)\nlibrary(sjrdata)\n\nsjr_journals %>%\n    filter(title %in% c(\"Nature\", \"Science\")) %>%\n    ggplot(aes(cites_doc_2years, sjr, color = title))+\n    geom_path(size = 1, alpha = .5)+\n    geom_label(aes(label = year %>% str_sub(3, 4)),\n              size = 3, label.padding = unit(.15, \"line\"))\n\n\nSeveral demographic journals.\n\nsjr_journals %>%\n    filter(title %in% c(\n        \"Demography\",\n        \"Population and Development Review\",\n        \"European Journal of Population\",\n        \"Population Studies\",\n        \"Demographic Research\",\n        \"Genus\"\n    )) %>%\n    ggplot(aes(cites_doc_2years, sjr, color = title))+\n    geom_point()+\n    stat_ellipse()+\n    scale_color_brewer(palette = \"Dark2\")+\n    coord_cartesian(expand = F)"
  },
  {
    "objectID": "2018/the-lancet-paper/index.html",
    "href": "2018/the-lancet-paper/index.html",
    "title": "Regional population structures at a glance",
    "section": "",
    "text": "I am happy to announce that our paper is published today in The Lancet.\n\nKashnitsky I, Schöley J. 2018. Regional population structures at a glance. The Lancet 392: 209–210. https://doi.org/10.1016/S0140-6736(18)31194-2\n\nAt a glance\nDemographic history of a population is imprinted in its age structure. A meaningful representation of regional population age structures can tell numerous demographic stories – at a glance. To produce such a snapshot of regional populations, we use an innovative approach of ternary colour coding.\nHere is the map:\n\nWe let the data speak colours\nWith ternary colour coding, each element of a three-dimensional array of compositional data is represented with a unique colour. The resulting colours show direction and magnitude of deviations from the centrepoint, which represents the average age of the European population, and is dark grey. The hue component of a colour encodes the direction of deviation: yellow indicates an elderly population (>65 years), cyan indicates people of working age (15–64 years), and magenta indicates children (0–14 years).\nThe method is very flexible, and one can easily produce these meaningful colours using our R package tricolore. Just explore the capabilities of the package in a built-in shiny app using the following lines of code:\n\ninstall.packages(\"tricolore\")\nlibrary(tricolore)\nDemoTricolore()\n\n\n\n\n\n\n\n\nReplication materials at github\n\n\n\n\n\n\n\n\n\n\n\n\nFolow us on Twitter: ikashnitsky, jschoeley\n\n\n\n\n\n\n\n\n\n\n\n\nSee also\n\n\n\n\nMy PhD project – Regional demographic convergence in Europe\nBlog post on the first version of the map presented at Rostock Retreat Visualization in June 2017\nPaper (Schöley & Willekens 2017) with the initial ideas for tricolore package\nAn example of ternary colorcoding used to visualize cause-of-death data\nMy other paper , which explores regional differences in population age structures"
  },
  {
    "objectID": "2019/barcelona-summer-school-of-demography/index.html",
    "href": "2019/barcelona-summer-school-of-demography/index.html",
    "title": "See you in Barcelona this summer",
    "section": "",
    "text": "Have you been feeling lately that you are missing out the coolest skill-set in academia?\n\n\nHere is you chance to cut in and dive into R.\nIn July BaRacelona Summer School of Demography welcomes dedicated scholars, aspiring or established, to help them migrate to the world of new oppoRtunities.\nThe school consists of 4 modules. You can take them all or choose specific ones. The first, instructed by Tim Riffe, introduces the basics of R. The second, instructed by myself, focuses on visualizing data, very general with a slight tilt towards population data. The third, instructed by Marie-Pier Bergeron Boucher, presents the foundations of demographic analysis with R. Even if you are not (yet) a demographer these methods are very general and are usable in a wide range of social science disciplines. Finally, the fourth course, instructed by Juan Galeano, teaches the powerful ways to unleash the spatial dimension of data analysis.\nThere are still several places available, the call closes on March 31st.\nI’ll be happy to see you in sunny Catalonia!"
  },
  {
    "objectID": "2019/dotplot/index.html",
    "href": "2019/dotplot/index.html",
    "title": "Dotplot – the single most useful yet largely neglected dataviz type",
    "section": "",
    "text": "Important\n\n\n\nI have to confess that the core message of this post is not really a fresh saying. But if I was given a chance to deliver one dataviz advise to every (ha-ha-ha) listening mind, I’d choose this: forget multi-category bar plots and use dotplots instead.\n\n\nI was converted several years ago after reading this brilliant post. Around the same time, as I figured out later, demographer Griffith Feeney wrote a similar post. From fresher examples, there are chapters discussing dotplot visualizations in Claus Wilke’s book and Kieran Healy’s book. So, what are dotplots and what’s so special about them?\nBasically, dotplots are the same regular barplots rotated 90 degrees and optimized on “ink usage”, i.e. dots represent values along the continuous horizontal axis. This leaves the vertical axis for the categorical variable allowing to write out long labels for the categories as normal horizontally aligned text (nobody likes to read that awful 45 degrees labels). The use of dots instead of bars allows to represent several comparable values for each category.\nHere is a sample of some ugly plots that I found in the recent papers in my personal library of papers.\n\nIt was not difficult to compose such a selection as these plots are, unfortunately, super popular. I think, the guy to blame here is (surprise-surprise) Excel. I’m failing to understand how such a basic plot type can be missed out.\nJust to be clear, I’m guilty too. Before the happy rstats-ggplot switch, I’ve been producing the same Excel plots. Here is an example from my 2014 working paper on internal youth migration in the central regions of Russia.\n\nThese two figures became one in the final paper recently published in GeoJournal. I think it’s a nice example how information dense, pretty looking (very subjective here), and easy to read a dotplot can be.\n\nThe figure shows discrepancy in migration estimates based on statistical record and census indirect estimates.\nThe code to replicate this figure is in this gist. A couple of coding solutions might be interesting here. First, the trick to offset vertically the dots for the two cohorts. Yet I realise that the way I implemented it is a bit clumsy; if you know a more elegant solution please let me know. Also, I believe in some cases composing the legend manually pays off, especially when we are dealing with two-dimensional legend. If the plot has some empty area it’s always a good idea to move the legend in the plotting area thus cutting away the margins."
  },
  {
    "objectID": "2019/zotero/index.html",
    "href": "2019/zotero/index.html",
    "title": "Zotero hacks: unlimited synced storage and its smooth use with rmarkdown",
    "section": "",
    "text": "About this tutorial\n\n\n\nHere is a bit refreshed translation of my 2015 blog post, initially published on Russian blog platform habr.com. The post shows how to organize a personal academic library of unlimited size for free. This is a funny case of a self written manual which I came back to multiple times myself and many many more times referred my friends to it, even non-Russian speakers who had to use Google Translator and infer the rest from screenshots. Finally, I decided to translate it adding some basic information on how to use Zotero with rmarkdown.\n\n\n\n\nA brief (and hopefully unnecessary for you) intro of bibliographic managers\nBibliographic manager is a life saver in everyday academic life. I suffer almost physical pain just thinking about colleagues who for some reason never started using one – all those excel spreadsheets with favorite citations, messy folders with PDFs, constant hours lost for the joy-killing task of manual reference list formatting. Once you start using a reference manager this all becomes a happily forgotten nightmare.\nI tend to think of bibliographic metadata as LEGO.\n\nFor each paper (book chapter / pre-print / R package) we have a number of metadata pieces – title, authors, date published, etc. These are the LEGO blocks. For different bibliographic styles we just need to re-shuffle those blocks inserting various commas, semicolons, and quotation marks.\nBibliographic manager keeps track of all the LEGO blocks and knows (learns easily) how to compose proper citation styles out of them. All we need is to download a specific journal’s citation style. There are more than six thousand bibliographic styles! [This is my #1 argument against the conspiracy ideas of some centralized power that rules our world .)]\n\n\nWhy Zotero?\nThere are dozens of bibliographic managers out there (see a comparative table). Some of them are free, the others require paid subscriptions. Probably, the most popular two are Zotero and Mendeley. Both are free to use and make money by offering cloud storage to sync PDFs of the papers. Yet, both give some limited storage for free – Zotero gives 300MB, and Mendeley gives 2GB.\nWhy do I choose and recommend Zotero then? Because it’s fairly easy to set-up Zotero so that the free 300MB are only used to sync metadata (which in practice means almost infinite storage), and the PDFs are synced separately using a cloud solution of one’s choice (I use Google Drive). It’s the main set-up hack that I’m showing in this blog post. There is no similar hack for Mendeley, and with them at some point one is bound to pay for extra storage.\nAnother consideration in favor of Zotero is that it’s an open-source program with strong community and outspoken commitment to stay free forever, while Mendeley is an Elsevier for-profit product. Academic community knows a lot about Elsevier in particular and for-profit products in general. Here the story of Academia.edu is very indicative. Have a look at this Forbes piece. As a career-long decision I’m confident in choosing Zotero. And the project keeps developing nicely – just look at the recent Zotero blog entries on the new features such as Google Docs integration, Unpaywall integration and a new web service for quick citations.\nFinally, an example of how strong Zotero community is. Once I figured out there the style repository does not have a style for Demographic Research, one of the best journals in demography. I’ve opened a request on Zotero forum and in two days the style was created.\n\n\nPrerequisites\n\nDownload and install Zotero. It’s cross-platform and works smoothly with various systems, even when the same database is sycned in parallel on machines with different operation systems. I’ve used win+linux and win+mac – no sync problems ever.\nFrom the same download page go to install Zotero Connector, a browser extension that helps to retrieve bibliographic metadata.\nCreate an account on Zotero website. It will be used later on to sync the database of bibliographic metadata.\nDownload and install the two plugins we’ll need – ZotFile (organizes the database of PDFs) and Better BibTeX (exports the library to .bib, we’ll use it later with rmarkdown). The plugins for Zotero are .xpi archives. To install the plugins open Zotero and click Tools --> Add-ons. A separate window for Add-ons manager will pop-up.\n\n\nThere we need to click the options gear button and select Install Add-on From File option. Finally navigate to the .xpi file and install. Zotero will ask to restart, please do.\nWe are ready to go through the setup step-by-step.\n\n\nZotero preferences\nFirst, let’s walk though Zotero Preferences. To edit them click Edit --> Preferences. A window with several tabs pops up.\nGeneral. I only uncheck the option to create automatic web page snapshots which I find not so useful compared with all the cluttering effect of all those multiple small files needed to replicate an html page locally.\n\nSync. Here we need to specify the account details to sync our database. It is important to uncheck the option of full-text sync otherwise the 300MB storage will quickly get filled. We’ll have the solution for full text a bit later.\n\nSearch. Defines the database for internal search engine. Defaults are reasonable.\nExport. Choose the style for quick export using Shift+Ctrl+C shortcut.\nCite. Citation styles stored locally. One nice feature here is the Get additional styles link which brings an integrated selection from the whole Zotero Styles Database. Styles can also be installed from local .csl files, for that press the + button. Don’t miss the Word Processors sub-tab. There we can get the plugins that integrate Zotero to Microsoft Word and Libre Office.\n\nAdvanced. Here we are most interested in the sub-tab Files and Folders. This is the most important step to separate the storage of metadata and files.\n\nThe first path should lead to a directory which stores the full-text PDFs, I call it zotero-library. This directory should be somewhere in the part of the local file system that is synced. In my case it’s the directory named ikashnitsky, which I sync with Google Drive. The second path leads to the system files of Zotero, I call it zotero-system. This directory should be placed somewhere in the non-synced part of the local file system. It will be updated by the native Zotero sync, and it’s better if those system files are not overwritten by any external sync software.\nBetter BibTeX. This tab appears after we install the Better BibTeX extension. The extension is needed to export the whole bibliographic library (or some of its parts) as a plain .bib text file. This step is needed to use Zotero in RStudio while writing academic papers with rmarkdown.\n\nThe most important option here is to define the rules for creating citation keys. There are almost infinite number of ways one can define these keys (check the manual). My personal choice is [auth:lower][year][journal:lower:abbr], which means that a key consists of the first author’s name, publication year, and the first letters abbreviation of the journal’s title, everything in lower case. Thus the key for my most recent paper published in Tijdschrift voor economische en sociale geografie is kashnitsky2019tveesg.\n\n\nZotFile Preferences\nNext we need to setup ZotFile. This extension helps to rename PDFs according to pre-defined rules and store them in a hierarchical database with meaningful names of the sub-directories. To open the setup window click Tools --> ZotFile Preferences. Again, the window has several tabs.\nGeneral. Here we define two paths. The first is the default location of the files downloaded by your browser. This option tells ZotFile where to look for the PDFs to process when you import a paper from the publisher’s website (recall that earlier we installed Zotero Connector). The second path leads to the local directory created for the full-text PDFs, the one that I named zotero-library and which is synced with an external cloud solution of our choice.\n\nTo navigate easier in this database of PDFs check the option Use subfolder defined by. Here again we have a wide choice of the ways to define the rules to name the sub-directories. Click the info icon to learn the options. I choose to simply have a separate folder for each first author.\nTablet Settings. Apparently, this menu allows to setup an exchange of PDFs with a tablet. I’ve never used it, thus omit.\nRenaming Rules. Here it’s important to make sure that ZotFile is responsible for renaming. Then we define how to rename the PDFs based on the bibliographic metadata available. Again, here we have many many options. My choice is {% raw %}{%a_}{%y_}{%t}{% endraw %} which yields file names like kashnitsky_2018_russian_periphery_is_dying_in_movement.pdf (again an example for my recent paper in GeoJournal).\n\nAdvanced Settings. I only checked the option to replace all the non-standard symbols with plain ASCII.\nA very important note on ZotFile!.\nIf you parse the metadata manually from a PDF, make sure to rename the file using ZotFile. For that right-click the metadata record Manage Attachments --> Rename Attachments. This action explicitly tells to use ZotFile for renaming and will move the renamed PDF to a proper sub-directory. The attachment in Zotero should not look like a PDF file…\n\n… but rather should be a link to the renamed file.\n\nIn these screenshot I also show the location of the actual PDFs in both cases (right-click the metadata record Show File). As you can see, in the first case the PDF is located in a meaninglessly named folder somewhere in the zotero-system directory. In contrast, the renamed by ZotFile PDF is located in a properly named sub-directory in zotero-library. Thus, in the latter case the PDF is synced to my Google Drive and can be accessed from anywhere.\n\nMore importantly, when I need to restore my whole database of academic papers on another machine, I just need to go through these steps. As long as the system metadata data base is synced by Zotero and I provide Zotero the link to a PDFs storage, it will recognize all the relative paths to the files, and the whole library is restored. This setup also makes it possible to have the same continuously synced library on multiple machines. The hack is in ZotFile which adds a file path line to the metadata of the papers.\n\nAs long as I keep the settings unchanged, everything will be synced fine across multiple devices. In the end, I enjoy the unlimited storage of my PDFs with the very nice and reliable native sync of metadata form Zotero.\nFinal remark on Zotero. Feel free to clean from time to time all the clutter from zotero-system/storage.\n\n\nUse Zotero library in RStudio with rmarkdown\nZotero has a very nice built-in integration with Microsoft Word and Libre Office. A bit of magic is needed if one wants to use it with LaTeX or (like me) with rmarkdown. The magic part is the Better BibTeX plugin, which we’ve installed and set up earlier.\nBetter BibTeX offers an easy way to export bibliographic records from Zotero as plain .bib text and keep the file updated once the records are changed. Just right-click on the collection in Zotero and choose Export Collection.\n\nThen in the next window choose to export as Better BibTeX and check the option to Keep updated.\n\nThe output .bib file should be placed in the directory from which we are going to knit the .rmd file. The name of the .bib is specified in YAML header of the .rmd. Here is an example from my running project with (jm_aburto?).\n\nNote that the exact YAML functions may vary depending on the rmarkdown template package. In this case I’m using bookdown, which also allows to specify the desired bibliographic style, .csl file should also be copied to the knit directory.\nThen, everything is ready to use the citation keys to generate citations throughout the text. For details on rmarkdown citation syntax, it’s better to refer to RStudio’s manual (see below) or the relevant chapter of (xieyihui?)’s book on bookdown.\n\n\nThe final hint here is to use citr package, which brings an easy and interactive way to select citations from the .bib file. Once the package in installed, an RStudio addin Insert citation appears which executes the citr:::insert_citation() command (you can assign a short-key to the addin). This function brings a shiny app to select a citation interactively. More details in the github repo.\n\n\n\n\n\n\nHappy paper writing with Zotero and RStudio!"
  },
  {
    "objectID": "2021/life-expectancy-101/index.html",
    "href": "2021/life-expectancy-101/index.html",
    "title": "What is life expectancy? And, even more important, what it isn’t",
    "section": "",
    "text": "It really is a remarkable achievement and maybe a lot of luck that the world mundanely operates with such a complex indicator as life expectancy. Unlike many statistics and quantities of general use that are being monitored and reported regularly, life expectancy is not observed directly. It’s an output of a mathematical model called life table. And as any model it comes with a certain load of assumptions and limitations, which are easily forgotten and omitted in the everyday interpretations and misinterpretations of the indicator.\nSo, why do we need any mathematical modelling in the first place? Consider a seemingly simple task: you want to know how long people live. What can be easier? Let’s just see how many years lived those who died recently. Why not? Such a metric would be massively driven by population age structure. For the most of the recent history human populations were rapidly growing, which means that each next generation was bigger than the previous one. Relative differences in the size of generations affect the age composition of those dying.\nOkay. Then why don’t we simply take a group of people born in the same year and see how long on average they live? We could (demographers call such groups cohorts). But it takes remarkably long to wait until the last person in the cohort dies. And we want to know what’s happening now.\nHow can we learn what’s happening now? Well, for that we need a mathematical model. We cannot observe the unfulfilled lifespans directly but we can construct/imagine an artificial population to help us understand the current mortality. The idea is simple: let’s take those dying now and divide them by the size of their age groups. This yields age-specific death rates – the key input for the life table needed to calculate life expectancy. Now, let’s take an imaginary cohort and see how long would they live on average if they experience these observed age-specific death rates. The imaginary population is know as a synthetic cohort. And here comes the main assumption of the life table: The model assumes that the observed age-specific death rates stay unchanged throughout the hypothetical lives of the hypothetical people in the synthetic cohort.\nThis big assumption of unchanging age profile of death rates almost never holds in real life. Mortality in human populations keeps improving beyond the most optimistic expectations. For decades the best demographers were systematically underestimating the progress in mortality reduction (Oeppen and Vaupel, 2002).\n\nThe horizontal lines are the limits of human population-level life expectancy as anticipated by renowned scholars; points are the actual data in the world leading countries. Source: (Oeppen and Vaupel, 2002)\nLife expectancy is a snapshot of the current mortality and is not a projection/forecast of the actual experience of the newborn cohorts. The current nature of period life expectancy is nicely illustrated by Dr. Robert Chung: “I have a car that can display”driving range” given its estimate of fuel level and how I’m driving. When climbing a steep hill, the range can decrease a lot; when descending, the range can increase. That’s what period e(x) is like.”\nBut if life expectancy talk about now and not the future, why is it called “expectancy” in the first place? This rather unfortunate and confusing naming comes from statistics, where “expected value” is a standard term for the mean of a distribution. The connotation crossing is rather unfortunate and as it strongly nudges the common future oriented misinterpretation of life expectancy.\nThe most popular error in public perception of period life expectancy forgets about the heavy assumption of the synthetic cohort (constant age-specific death rates throughout their hypothetic lives anchored in current year) and talks about the future of kids being born now. In normal years this large interpretation error is somewhat masked by the gradual and often close to linear improvements in mortality. A rule of thumb is to simply add ~6 years to period life expectancy to obtain a reasonable cohort estimate (Goldstein and Wachter, 2006). Mortality shocks like 2020 are a different story though. Here the “forward looking” (mis)interpretation of period life expectancy projects the shock levels of mortality into the future. Of course this doesn’t happen. Shocks are called shocks because they are temporal.\nAnother important detail that often misses public attention is that life expectancy is not a single value – it can be estimated for every age. Most often and by default life expectancy is reported “at birth”. But we can estimate remaining life expectancy for various ages. And here comes another popular misunderstanding of life expectancy. Too often we come across the references to human age and longevity in the past that sound something like: “She was 40, a very elderly lady by the standards of that time as people lived on average about 30 years back the”. True, there were times when life expectancy at birth was about 30 years even in the most developed now countries. This doesn’t mean though that those who outlived this threshold age were getting old at young (by our current standards) ages. Let me illustrate.\nLet’s take Italian male population in 1872, the first available year in Human Mortality Database. Have a look at the survival of this synthetic cohort – the proportion of the initial cohort that is still alive by certain age. Half of the synthetic cohort died by age 15!\n\nAnd here is how remaining period life expectancy looked by age. Infant and child mortality was sooo high that those escaping early deaths had higher remaining life expectancy.\n\nAt age 34 remaining life expectancy was the same as at birth. Only, it applied to the 41% survivors. And I guess the perception of age was not radically different among those survivors. It was all about selection and luck getting there. The high early life mortality is responsible for another popular demographic myth, which postulates that everybody used to have many kids in the past. No, people used to have many births, and only a fraction of those kids survived to adult life.\n\n\n\n\n\n\n\nThis post is based on my previous Twitter thread\n\n\n\n\n\n\n\n\n\n\n\n\nReferences\n\nGoldstein JR, Wachter KW. 2006. Relationships between Period and Cohort Life Expectancy: Gaps and Lags. Population Studies 60: 257–269 DOI: 10.1080/00324720600895876\n\n\nOeppen J, Vaupel JW. 2002. Broken limits to life expectancy. Science 296: 1029–1031 DOI: 10.1126/science.1069675"
  },
  {
    "objectID": "2022/exceptional-world-cup/index.html",
    "href": "2022/exceptional-world-cup/index.html",
    "title": "Were there too many unlikely results at the FIFA World Cup 2022 in Qatar?",
    "section": "",
    "text": "FIFA World Cup 2022 in Qatar saw many surprising results. In fact, too many – some would argue. From the unbelievable loss of Argentina to Saudi Arabia at the very beginning of the group stage, via the loss of the magnificent Brazil to Cameroon at the end of the group stage, to the groundbreaking performance of Morocco who were competitive playing against all the usual grands.\nSomewhere towards the middle of the group stage I started wondering – is it ordinary to have so many surprises at the World Cup? What if this particular World Cup is really exceptional by the number of unlikely match outcomes? 1 If so, how should I measure it?1 Through discussions of my earlier postings of this analysis I figured out that the usual term to describe these unlikely outcomes is “upset”. I don’t like the term since it seems to suggest inherently that one always supports the favorite, which is definitely no my case, I tend to cheer for the underdogs. The term I used initially was “sensations”, which apparently makes little sense in English and reveals the direct translation from Russian happening in my head in this case. Throughout this post I’ll use the term “unlikely outcome”.\nJust a bit of thinking yielded an answer that was really on the surface – bookmakers. They are the people who use all available knowledge to make money on the outcome expectations. Pretty soon I figured out that there is one website [oddsportal.com][oddsportal] that offers long historical data series of betting odds and match outcomes. The real challenge came at the step of scraping the website. Stack Overflow and other similar places are filled with questions about scraping this specific tricky website. Having finally figured out how to do this (after countless trials and failures) I wrapped up the working solution into an R package oddor. The idea is that the package provides both scraping tools and the cleaned extracted datasets. 22 For now I only scraped and added several football tournaments and leagues. Please, feel free to add more results via GitHub pull requests.\nOkay, so the data issue is solved. Now, it’s time for the experiment, a really simple one. I simulate the scenario where I consistently bet on the least likely outcome and track how my fictional balance changes over time. Out of the three possible outcomes of the games – (1) home team wins, (2) draw in main time, and (3) away team wins 3 – I always select the one that promises the highest odds, meaning that this outcome is considered the least likely of the three by the bookmakers.3 Of course, “home” and “away” designation is very arbitrary at world cups, still I use the terms for consistency.\nHere are the results for the latest World Cup 2022 (darkest line with all the unlikely outcomes annotated) in comparison with three previous World Cups, 2018, 2014, and 2010.\n\nDecreasing step lines in the plot represent my decreasing fictional balance. Each game I bet 1 coin on the least likely outcome. Most often I lose this bet, and my balance decreases. Though, sometimes the least likely outcome happens, then my balance increases substantially by the size of the unlikely outcome odds. For example, in case of Argentina losing to Saudi Arabia the odds for this outcome was 25.\nWe can see that the 2022 World Cup was really exceptional – too often the outcomes that were considered the least likely happened. It’s also evident that surprises happen more often at the group stage, especially in the third round when many leaders apparently have already reached their group-stage goals (think of the recent game Brazil–Cameroon, where Brazilians literally played with the second team). I would say it’s really surprising – if one bets consistently against the odds at all the group-stage games, at least in the last 4 World Cups (for which we have odds and outcomes data) this dead-simple strategy turns out to be beneficial. My wild guess is that the World Cups see masses of inexperienced new betters who are placing bets on their national teams whatever, which at the global scale is disbalancing the whole system. Alternatively, maybe we are just slow and bad at recognizing how football is becoming more international, and now more underdogs are able to give a decent fight to the traditional grands.\nIn contrast, Play-offs are apparently less chaotic and more predictable. Betting on the underdogs at the Play-offs stage would guarantee to lose money in all 4 recent World Cups.\nOf course, the surprising beneficial result of the betting-on-the-underdog experiment at the group-stage games at World Cups made me curious about other competitions. So I did a similar analysis for the Champions League (from season 2004–05) and for English Premier League (from season 2003–04). Predictably, no miracle happened – betting consistently on the underdogs would almost always bring financial losses. So, either World Cups are just different, or all 4 last tournaments were exceptional with the latest being a crazy outlier. Would it be reasonable to try this no-brainer betting strategy at the group-stage of the next World Cup? I’m not sure. But let’s see in 4 years. 44 I guess I have to say that this is not a financial advise =)\n\n\n\n\n\n\n\nReplicate this analysis using the R code from this gist"
  },
  {
    "objectID": "cv.html",
    "href": "cv.html",
    "title": "Academic CV",
    "section": "",
    "text": "Download current CV"
  },
  {
    "objectID": "dd.html",
    "href": "dd.html",
    "title": ".",
    "section": "",
    "text": "Demographic Digest is a regular column at Demoscope Weekly which publishes (in Russian) brief summaries of fresh demographic papers from the best academic journals.\n\nДемографический Дайджест – регулярная рубрика журнала Демоскоп Weekly, в которой приводятся краткие обзоры демографических статей, публикуемых в ведущих зарубежных журналах.\n\n\n[PROJECT WEBSITE]\n\n\n\n[ARCHIVE]"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "demographeR’s notes",
    "section": "",
    "text": "December 25, 2022\n        \n        \n            Were there too many unlikely results at the FIFA World Cup 2022 in Qatar?\n            \n                \n                    r\n                \n                \n                    oddor\n                \n                \n                    football\n                \n                \n                    sport\n                \n            \n            \n        \n        \n            \n        \n    \n\n\nNo matching items"
  },
  {
    "objectID": "index.html#section-1",
    "href": "index.html#section-1",
    "title": "demographeR’s notes",
    "section": "2021",
    "text": "2021\n\n\n    \n                  \n            March 5, 2021\n        \n        \n            What is life expectancy? And, even more important, what it isn't\n            \n                \n                    r\n                \n                \n                    tw\n                \n                \n                    101\n                \n                \n                    life expectancy\n                \n            \n            \n        \n        \n            \n        \n    \n\n\nNo matching items"
  },
  {
    "objectID": "index.html#section-2",
    "href": "index.html#section-2",
    "title": "demographeR’s notes",
    "section": "2019",
    "text": "2019\n\n\n    \n        \n            \n                \n                    \n                \n            \n            Dotplot – the single most useful yet largely neglected dataviz type\n            \n                \n                    r\n                \n                \n                    dataviz\n                \n                \n                    tip\n                \n            \n            \n                July 19, 2019\n            \n            \n        \n    \n    \n        \n            \n                \n                    \n                \n            \n            Zotero hacks: unlimited synced storage and its smooth use with rmarkdown\n            \n                \n                    r\n                \n                \n                    tutorial\n                \n                \n                    zotero\n                \n            \n            \n                March 14, 2019\n            \n            \n        \n    \n    \n        \n            \n                \n                    \n                \n            \n            See you in Barcelona this summer\n            \n                \n                    r\n                \n                \n                    dataviz\n                \n                \n                    teaching\n                \n            \n            \n                March 7, 2019\n            \n            \n        \n    \n\n\nNo matching items"
  },
  {
    "objectID": "index.html#section-3",
    "href": "index.html#section-3",
    "title": "demographeR’s notes",
    "section": "2018",
    "text": "2018\n\n\n    \n        \n            \n                \n                    \n                \n            \n            Compare population age structures of Europe NUTS-3 regions and the US counties using ternary color-coding\n            \n                \n                    r\n                \n                \n                    poster\n                \n                \n                    tricolore\n                \n                \n                    rspatial\n                \n                \n                    award\n                \n            \n            \n                December 3, 2018\n            \n            \n        \n    \n    \n        \n            \n                \n                    \n                \n            \n            sjrdata: all SCImago Journal & Country Rank data, ready for R\n            \n                \n                    r\n                \n                \n                    package\n                \n                \n                    sjr\n                \n                \n                    bibliometrics\n                \n            \n            \n                September 23, 2018\n            \n            \n        \n    \n    \n        \n            \n                \n                    \n                \n            \n            Regional population structures at a glance\n            \n                \n                    r\n                \n                \n                    paper\n                \n                \n                    dataviz\n                \n                \n                    tricolore\n                \n            \n            \n                July 21, 2018\n            \n            \n        \n    \n    \n        \n            \n                \n                    \n                \n            \n            Deep Catalan roots: playing with stringdist\n            \n                \n                    r\n                \n                \n                    stringdist\n                \n            \n            \n                June 4, 2018\n            \n            \n        \n    \n    \n        \n            \n                \n                    \n                \n            \n            A perfect RStudio layout\n            \n                \n                    r\n                \n                \n                    rstudio\n                \n                \n                    tip\n                \n            \n            \n                May 22, 2018\n            \n            \n        \n    \n\n\nNo matching items"
  },
  {
    "objectID": "index.html#section-4",
    "href": "index.html#section-4",
    "title": "demographeR’s notes",
    "section": "2017",
    "text": "2017\n\n\n    \n        \n            \n                \n                    \n                \n            \n            Data acquisition in R (3/4)\n            \n                \n                    r\n                \n                \n                    tutorial\n                \n                \n                    data acquisition\n                \n            \n            \n                December 10, 2017\n            \n            \n        \n    \n    \n        \n            \n                \n                    \n                \n            \n            Data acquisition in R (2/4)\n            \n                \n                    r\n                \n                \n                    tutorial\n                \n                \n                    data acquisition\n                \n            \n            \n                November 7, 2017\n            \n            \n        \n    \n    \n        \n            \n                \n                    \n                \n            \n            Data acquisition in R (1/4)\n            \n                \n                    r\n                \n                \n                    tutorial\n                \n                \n                    data acquisition\n                \n            \n            \n                October 17, 2017\n            \n            \n        \n    \n    \n        \n            \n                \n                    \n                \n            \n            Young people neither in employment nor in education and training in Europe, 2000-2016\n            \n                \n                    r\n                \n                \n                    eurostat\n                \n                \n                    neet\n                \n                \n                    geofacet\n                \n            \n            \n                July 18, 2017\n            \n            \n        \n    \n    \n        \n            \n                \n                    \n                \n            \n            Global convergence in male life expectancy at birth\n            \n                \n                    r\n                \n                \n                    life expectancy\n                \n                \n                    global\n                \n                \n                    convergence\n                \n            \n            \n                July 17, 2017\n            \n            \n        \n    \n    \n        \n            \n                \n                    \n                \n            \n            Accelerating ggplot2: use a canvas to speed up plots creation\n            \n                \n                    r\n                \n                \n                    ggplot2\n                \n                \n                    speed\n                \n            \n            \n                July 4, 2017\n            \n            \n        \n    \n    \n        \n            \n                \n                    \n                \n            \n            Colorcoded map: regional population structures at a glance\n            \n                \n                    r\n                \n                \n                    ggplot2\n                \n                \n                    rspatial\n                \n                \n                    tricolore\n                \n            \n            \n                June 30, 2017\n            \n            \n        \n    \n    \n        \n            \n                \n                    \n                \n            \n            Evolution of ice hockey players' height: IIHF world championships 2001-2016\n            \n                \n                    r\n                \n                \n                    hockey\n                \n                \n                    sport\n                \n                \n                    height\n                \n            \n            \n                May 27, 2017\n            \n            \n        \n    \n    \n        \n            \n                \n                    \n                \n            \n            Subplots in maps with ggplot2\n            \n                \n                    r\n                \n                \n                    ggplot2\n                \n                \n                    rspatial\n                \n            \n            \n                May 25, 2017\n            \n            \n        \n    \n    \n        \n            \n                \n                    \n                \n            \n            Arranging subplots with ggplot2\n            \n                \n                    r\n                \n                \n                    ggplot2\n                \n            \n            \n                May 22, 2017\n            \n            \n        \n    \n    \n        \n            \n                \n                    \n                \n            \n            Hacking maps with ggplot2\n            \n                \n                    r\n                \n                \n                    ggplot2\n                \n                \n                    rspatial\n                \n            \n            \n                April 24, 2017\n            \n            \n        \n    \n    \n        \n            \n                \n                    \n                \n            \n            Who is old? Visualizing the concept of prospective ageing with animated population pyramids\n            \n                \n                    r\n                \n                \n                    animaion\n                \n                \n                    pyramid\n                \n                \n                    sweden\n                \n                \n                    prospective ageing\n                \n            \n            \n                March 31, 2017\n            \n            \n        \n    \n    \n        \n            \n                \n                    \n                \n            \n            R, GIS, and fuzzyjoin to reconstruct demographic data for NUTS regions of Denmark\n            \n                \n                    r\n                \n                \n                    rspatial\n                \n                \n                    denmark\n                \n                \n                    nuts\n                \n            \n            \n                March 16, 2017\n            \n            \n        \n    \n    \n        \n            \n                \n                    \n                \n            \n            Gender gap in Swedish mortality\n            \n                \n                    r\n                \n                \n                    tw\n                \n                \n                    death rates\n                \n                \n                    sweden\n                \n                \n                    sex gap\n                \n            \n            \n                February 25, 2017\n            \n            \n        \n    \n    \n        \n            \n                \n                    \n                \n            \n            30 issues of Demographic Digest - the most frequent journals\n            \n                \n                    r\n                \n                \n                    demography\n                \n                \n                    bibliometrics\n                \n            \n            \n                February 14, 2017\n            \n            \n        \n    \n    \n        \n            \n                \n                    \n                \n            \n            Male mortality in Russia and Japan\n            \n                \n                    r\n                \n                \n                    tw\n                \n                \n                    hmd\n                \n                \n                    death rates\n                \n                \n                    russia\n                \n            \n            \n                February 6, 2017\n            \n            \n        \n    \n    \n        \n            \n                \n                    \n                \n            \n            Sex ratios in all countries from Human Mortality Database\n            \n                \n                    r\n                \n                \n                    tw\n                \n                \n                    hmd\n                \n                \n                    death rates\n                \n            \n            \n                February 5, 2017\n            \n            \n        \n    \n    \n        \n            \n                \n                    \n                \n            \n            Hello R world post\n            \n                \n                    r\n                \n            \n            \n                January 29, 2017\n            \n            \n        \n    \n\n\nNo matching items\n\n\n\n\n\n  Subscribe for email updates"
  },
  {
    "objectID": "me.html",
    "href": "me.html",
    "title": ".",
    "section": "",
    "text": "CV"
  },
  {
    "objectID": "me.html#bio",
    "href": "me.html#bio",
    "title": ".",
    "section": "BIO",
    "text": "BIO\nI was born in Israel, grew up in Moscow, during my PhD years lived in The Hague, and now live in Odense. Married and have two daughters. I got a bachelors in geography from Moscow State University in 2012, masters in demography from the National Research University Higher School of Economics (Moscow) in 2014, and PhD from University of Groningen in 2020.\n\n\nFeel free to contact me via email\nilya.kashnitsky at gmail dot com\nIlya\n\n\n\n  Subscribe for email updates"
  },
  {
    "objectID": "subscribe.html",
    "href": "subscribe.html",
    "title": "Ilya Kashnitsky",
    "section": "",
    "text": "Loading…\n\n\n\n\n\n\n\nNo need to log into your Google account to submit the form"
  },
  {
    "objectID": "2023/map-borders/index.html",
    "href": "2023/map-borders/index.html",
    "title": "The easiest way to radically improve map aesthetics",
    "section": "",
    "text": "Since R community developed brilliant tools to deal with spatial data, producing maps is no longer the privilege of a narrow group of people with very specific almost esoteric knowledge, skillset, and often super expensive software. With #rspatial packages, maps (at least the relatively simple ones) became just another type of dataviz.\n\n\n\n\nJust a few lines of code can reveal the eye-catching and visually pleasant spatial dimension of the data. Similarly, a few more lines of code can radically improve the pleasantness of a simple map – just add borders as lines in a separate spatial layer.\n\n\n\n\nAn often “quick and dirty” solution when composing a simple choropleth map is to use polygons outline as the borders. While this works okay to distinguish the polygons, the map quickly becomes unnecessarily overloaded. All the non-bordering outlines – complicated coastal lines and islands’ outlines – look ugly and add nothing to the map.\nLet’s illustrate the ease of this trick mapping Greece with its numerous small islands. We’ll use the beautiful eurostat package that has a built in spatial dataset with NUTS-3 regions of Europe.\n\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(cowplot)\n\nset.seed(911)\n\n# subset Greence, NUTS-3 regions\nlibrary(eurostat)\ngreece <- eurostat_geodata_60_2016 %>% \n    filter(LEVL_CODE==3,\n           str_sub(geo, 1, 2) == \"EL\") %>% \n    # create random values for filling the polygons\n    mutate(random = runif(length(id))) %>% \n    select(id, geometry, random) %>% \n    st_transform(crs = 3035)\n\nFirst, here’s the typical lazy (or rather no-brainer) way of using the polygons’ outlines to show the borders between our spatial units.\n\n# plot with polygon outlines\ngreece %>% \n    ggplot()+\n    geom_sf(aes(fill = random), color = 2, size = 1)+\n    labs(title = \"Polygons outlined\")+\n    scale_fill_viridis_c(begin = .5)+\n    theme_map()+\n    theme(plot.background = element_rect(color = NA, fill = \"#eeffff\"))\n\n\n\ngg_outline <- last_plot()\n\nLook at all the islands, especially the small ones – what are all these red outlines for? Insted, we can add only the borders between the polygons as lines. For this we need to add another geospatial layer with lines. Where do we get it? This is extremely easy to produce thanks to the marvelous little package rmapshaper that has a function ms_innerlines() exactly for the task. 11 Before I found rmapshaper the task seemed overly complicated, I even asked Stack Overflow\n\n# produce border lines with rmapshaper::ms_innerlines()\nlibrary(rmapshaper)\nbord <- greece %>% ms_innerlines()\n\nNow, let’s plot the same map with proper borders between the polygons. Note that for the sf layer with polygons I set color = NA to get rid of the polygons outline. Then with the next call to geom_sf() I draw the line borders as a separate layer.\n\n# now plot without polygon outlines and with borders as lines\ngreece %>% \n    ggplot()+\n    geom_sf(aes(fill = random), color = NA)+\n    geom_sf(data = bord, color = 2, size = 1)+\n    labs(title = \"Borders as lines\")+\n    scale_fill_viridis_c(begin = .5)+\n    theme_map()+\n    theme(plot.background = element_rect(color = NA, fill = \"#eeffff\"))\n\n\n\ngg_bord <- last_plot()\n\nThat’s it! This is the simplest dataviz trick I know that can radically improve the outlook of simple choropleth maps. It’s only one additional line of code. You can even create the borders sf object on the fly within the ggplot map creation code specifying the data parameter as . %>% ms_innerlines(), like this:\n\ngeom_sf(data = . %>% ms_innerlines(), color = 2, size = 1)\n\nFinally, let’s put the two maps side by side.\n\n# put side by side\nlibrary(patchwork)\n(\n    gg_outline + gg_bord \n)  + \n    plot_layout(guides = \"collect\")+\n    plot_annotation(caption = \"! Look at the islands\")\n\n\n\n\n\n\n\n\n\n\n\nReplicate this analysis using the R code from this gist. This post is partially based on my previous Twitter thread\n\n\n\n\n\n\n\n\n\n\n\n\nAbout this post\n\n\n\nPublishing this post is my personal gestalt closure – it spent more than three years in planning and then in drafts. Somehow, with this post I hit the wall of writer’s block and it coincided with Twitter threads substituting blogging for me. Now, it’s time to get back to blogging."
  }
]