[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "demographeR’s notes",
    "section": "",
    "text": "No matching items"
  },
  {
    "objectID": "index.html#section",
    "href": "index.html#section",
    "title": "demographeR’s notes",
    "section": "",
    "text": "No matching items"
  },
  {
    "objectID": "index.html#section-1",
    "href": "index.html#section-1",
    "title": "demographeR’s notes",
    "section": "2024",
    "text": "2024\n\n\n    \n    \n\n        \n\n            \n                \n                \n                    \n                \n                \n            \n\n            Rotate the damn plot\n\n            \n            \n                \n                \n                    r\n                \n                \n                \n                    dataviz\n                \n                \n                \n                    trick\n                \n                \n            \n            \n\n            \n                December 18, 2024\n            \n\n            \n        \n\n    \n    \n    \n\n        \n\n            \n                \n                \n                    \n                \n                \n            \n\n            [UPD] Zotero hacks: reliably setup unlimited storage for you personal academic library\n\n            \n            \n                \n                \n                    r\n                \n                \n                \n                    tutorial\n                \n                \n            \n            \n\n            \n                August 16, 2024\n            \n\n            \n        \n\n    \n    \n\n\nNo matching items"
  },
  {
    "objectID": "index.html#section-2",
    "href": "index.html#section-2",
    "title": "demographeR’s notes",
    "section": "2023",
    "text": "2023\n\n\n    \n    \n\n        \n\n            \n                \n                \n                    \n                \n                \n            \n\n            30DayMapChallenge my 25/30 contributions\n\n            \n            \n                \n                \n                    r\n                \n                \n                \n                    rspatial\n                \n                \n                \n                    ggplot2\n                \n                \n            \n            \n\n            \n                December 21, 2023\n            \n\n            \n        \n\n    \n    \n    \n\n        \n\n            \n                \n                \n                    \n                \n                \n            \n\n            Improve your maps in one line of code changing map projections\n\n            \n            \n                \n                \n                    r\n                \n                \n                \n                    rspatial\n                \n                \n                \n                    ggplot2\n                \n                \n            \n            \n\n            \n                November 4, 2023\n            \n\n            \n        \n\n    \n    \n    \n\n        \n\n            \n                \n                \n                    \n                \n                \n            \n\n            Geocode address text strings using `tidygeocoder`\n\n            \n            \n                \n                \n                    r\n                \n                \n                \n                    rspatial\n                \n                \n                \n                    ggplot2\n                \n                \n            \n            \n\n            \n                November 1, 2023\n            \n\n            \n        \n\n    \n    \n    \n\n        \n\n            \n                \n                \n                    \n                \n                \n            \n\n            Save space in faceted plots\n\n            \n            \n                \n                \n                    r\n                \n                \n                \n                    dataviz\n                \n                \n                \n                    trick\n                \n                \n                \n                    faceting\n                \n                \n            \n            \n\n            \n                February 27, 2023\n            \n\n            \n        \n\n    \n    \n    \n\n        \n\n            \n                \n                \n                    \n                \n                \n            \n\n            Easily re-using self-written functions: the power of gist + code snippet duo\n\n            \n            \n                \n                \n                    r\n                \n                \n                \n                    rstudio\n                \n                \n                \n                    trick\n                \n                \n            \n            \n\n            \n                January 3, 2023\n            \n\n            \n        \n\n    \n    \n    \n\n        \n\n            \n                \n                \n                    \n                \n                \n            \n\n            The easiest way to radically improve map aesthetics\n\n            \n            \n                \n                \n                    r\n                \n                \n                \n                    rspatial\n                \n                \n                \n                    dataviz\n                \n                \n                \n                    trick\n                \n                \n            \n            \n\n            \n                January 1, 2023\n            \n\n            \n        \n\n    \n    \n\n\nNo matching items"
  },
  {
    "objectID": "index.html#section-3",
    "href": "index.html#section-3",
    "title": "demographeR’s notes",
    "section": "2022",
    "text": "2022\n\n\n    \n    \n\n        \n\n            \n                \n                \n                    \n                \n                \n            \n\n            Were there too many unlikely results at the FIFA World Cup 2022 in Qatar?\n\n            \n            \n                \n                \n                    r\n                \n                \n                \n                    sport\n                \n                \n                \n                    package\n                \n                \n            \n            \n\n            \n                December 25, 2022\n            \n\n            \n        \n\n    \n    \n\n\nNo matching items"
  },
  {
    "objectID": "index.html#section-4",
    "href": "index.html#section-4",
    "title": "demographeR’s notes",
    "section": "2021",
    "text": "2021\n\n\n    \n    \n\n        \n\n            \n                \n                \n                    \n                \n                \n            \n\n            What is life expectancy? And, even more important, what it isn't\n\n            \n            \n                \n                \n                    r\n                \n                \n                \n                    101\n                \n                \n                \n                    demography\n                \n                \n            \n            \n\n            \n                March 5, 2021\n            \n\n            \n        \n\n    \n    \n\n\nNo matching items"
  },
  {
    "objectID": "index.html#section-5",
    "href": "index.html#section-5",
    "title": "demographeR’s notes",
    "section": "2020",
    "text": "2020\n\n\n    \n    \n\n        \n\n            \n                \n                \n                    \n                \n                \n            \n\n            Show all data in the background of your faceted ggplot\n\n            \n            \n                \n                \n                    r\n                \n                \n                \n                    faceting\n                \n                \n                \n                    dataviz\n                \n                \n                \n                    ggplot2\n                \n                \n                \n                    trick\n                \n                \n            \n            \n\n            \n                June 19, 2020\n            \n\n            \n        \n\n    \n    \n\n\nNo matching items"
  },
  {
    "objectID": "index.html#section-6",
    "href": "index.html#section-6",
    "title": "demographeR’s notes",
    "section": "2019",
    "text": "2019\n\n\n    \n    \n\n        \n\n            \n                \n                \n                    \n                \n                \n            \n\n            Dotplot – the single most useful yet largely neglected dataviz type\n\n            \n            \n                \n                \n                    r\n                \n                \n                \n                    dataviz\n                \n                \n                \n                    trick\n                \n                \n            \n            \n\n            \n                July 19, 2019\n            \n\n            \n        \n\n    \n    \n    \n\n        \n\n            \n                \n                \n                    \n                \n                \n            \n\n            Zotero hacks: unlimited synced storage and its smooth use with rmarkdown\n\n            \n            \n                \n                \n                    r\n                \n                \n                \n                    tutorial\n                \n                \n            \n            \n\n            \n                March 14, 2019\n            \n\n            \n        \n\n    \n    \n    \n\n        \n\n            \n                \n                \n                    \n                \n                \n            \n\n            See you in Barcelona this summer\n\n            \n            \n                \n                \n                    r\n                \n                \n                \n                    dataviz\n                \n                \n                \n                    demography\n                \n                \n            \n            \n\n            \n                March 7, 2019\n            \n\n            \n        \n\n    \n    \n\n\nNo matching items"
  },
  {
    "objectID": "index.html#section-7",
    "href": "index.html#section-7",
    "title": "demographeR’s notes",
    "section": "2018",
    "text": "2018\n\n\n    \n    \n\n        \n\n            \n                \n                \n                    \n                \n                \n            \n\n            Compare population age structures of Europe NUTS-3 regions and the US counties using ternary color-coding\n\n            \n            \n                \n                \n                    r\n                \n                \n                \n                    demography\n                \n                \n                \n                    dataviz\n                \n                \n                \n                    rspatial\n                \n                \n            \n            \n\n            \n                December 3, 2018\n            \n\n            \n        \n\n    \n    \n    \n\n        \n\n            \n                \n                \n                    \n                \n                \n            \n\n            sjrdata: all SCImago Journal & Country Rank data, ready for R\n\n            \n            \n                \n                \n                    r\n                \n                \n                \n                    package\n                \n                \n                \n                    bibliometrics\n                \n                \n            \n            \n\n            \n                September 23, 2018\n            \n\n            \n        \n\n    \n    \n    \n\n        \n\n            \n                \n                \n                    \n                \n                \n            \n\n            Regional population structures at a glance\n\n            \n            \n                \n                \n                    r\n                \n                \n                \n                    dataviz\n                \n                \n                \n                    demography\n                \n                \n            \n            \n\n            \n                July 21, 2018\n            \n\n            \n        \n\n    \n    \n    \n\n        \n\n            \n                \n                \n                    \n                \n                \n            \n\n            Deep Catalan roots: playing with stringdist\n\n            \n            \n                \n                \n                    r\n                \n                \n                \n                    demography\n                \n                \n            \n            \n\n            \n                June 4, 2018\n            \n\n            \n        \n\n    \n    \n    \n\n        \n\n            \n                \n                \n                    \n                \n                \n            \n\n            A perfect RStudio layout\n\n            \n            \n                \n                \n                    r\n                \n                \n                \n                    rstudio\n                \n                \n                \n                    trick\n                \n                \n            \n            \n\n            \n                May 22, 2018\n            \n\n            \n        \n\n    \n    \n\n\nNo matching items"
  },
  {
    "objectID": "index.html#section-8",
    "href": "index.html#section-8",
    "title": "demographeR’s notes",
    "section": "2017",
    "text": "2017\n\n\n    \n    \n\n        \n\n            \n                \n                \n                    \n                \n                \n            \n\n            Data acquisition in R (3/4)\n\n            \n            \n                \n                \n                    r\n                \n                \n                \n                    tutorial\n                \n                \n                \n                    data acquisition\n                \n                \n            \n            \n\n            \n                December 10, 2017\n            \n\n            \n        \n\n    \n    \n    \n\n        \n\n            \n                \n                \n                    \n                \n                \n            \n\n            Data acquisition in R (2/4)\n\n            \n            \n                \n                \n                    r\n                \n                \n                \n                    tutorial\n                \n                \n                \n                    data acquisition\n                \n                \n            \n            \n\n            \n                November 7, 2017\n            \n\n            \n        \n\n    \n    \n    \n\n        \n\n            \n                \n                \n                    \n                \n                \n            \n\n            Data acquisition in R (1/4)\n\n            \n            \n                \n                \n                    r\n                \n                \n                \n                    tutorial\n                \n                \n                \n                    data acquisition\n                \n                \n            \n            \n\n            \n                October 17, 2017\n            \n\n            \n        \n\n    \n    \n    \n\n        \n\n            \n                \n                \n                    \n                \n                \n            \n\n            Young people neither in employment nor in education and training in Europe, 2000-2016\n\n            \n            \n                \n                \n                    r\n                \n                \n                \n                    faceting\n                \n                \n            \n            \n\n            \n                July 18, 2017\n            \n\n            \n        \n\n    \n    \n    \n\n        \n\n            \n                \n                \n                    \n                \n                \n            \n\n            Global convergence in male life expectancy at birth\n\n            \n            \n                \n                \n                    r\n                \n                \n                \n                    demography\n                \n                \n            \n            \n\n            \n                July 17, 2017\n            \n\n            \n        \n\n    \n    \n    \n\n        \n\n            \n                \n                \n                    \n                \n                \n            \n\n            Accelerating ggplot2: use a canvas to speed up plots creation\n\n            \n            \n                \n                \n                    r\n                \n                \n                \n                    ggplot2\n                \n                \n            \n            \n\n            \n                July 4, 2017\n            \n\n            \n        \n\n    \n    \n    \n\n        \n\n            \n                \n                \n                    \n                \n                \n            \n\n            Colorcoded map: regional population structures at a glance\n\n            \n            \n                \n                \n                    r\n                \n                \n                \n                    ggplot2\n                \n                \n                \n                    rspatial\n                \n                \n                \n                    dataviz\n                \n                \n                \n                    demography\n                \n                \n            \n            \n\n            \n                June 30, 2017\n            \n\n            \n        \n\n    \n    \n    \n\n        \n\n            \n                \n                \n                    \n                \n                \n            \n\n            Evolution of ice hockey players' height: IIHF world championships 2001-2016\n\n            \n            \n                \n                \n                    r\n                \n                \n                \n                    sport\n                \n                \n            \n            \n\n            \n                May 27, 2017\n            \n\n            \n        \n\n    \n    \n    \n\n        \n\n            \n                \n                \n                    \n                \n                \n            \n\n            Subplots in maps with ggplot2\n\n            \n            \n                \n                \n                    r\n                \n                \n                \n                    ggplot2\n                \n                \n                \n                    rspatial\n                \n                \n            \n            \n\n            \n                May 25, 2017\n            \n\n            \n        \n\n    \n    \n    \n\n        \n\n            \n                \n                \n                    \n                \n                \n            \n\n            Arranging subplots with ggplot2\n\n            \n            \n                \n                \n                    r\n                \n                \n                \n                    ggplot2\n                \n                \n            \n            \n\n            \n                May 22, 2017\n            \n\n            \n        \n\n    \n    \n    \n\n        \n\n            \n                \n                \n                    \n                \n                \n            \n\n            Hacking maps with ggplot2\n\n            \n            \n                \n                \n                    r\n                \n                \n                \n                    ggplot2\n                \n                \n                \n                    rspatial\n                \n                \n            \n            \n\n            \n                April 24, 2017\n            \n\n            \n        \n\n    \n    \n    \n\n        \n\n            \n                \n                \n                    \n                \n                \n            \n\n            Who is old? Visualizing the concept of prospective ageing with animated population pyramids\n\n            \n            \n                \n                \n                    r\n                \n                \n                \n                    animaion\n                \n                \n            \n            \n\n            \n                March 31, 2017\n            \n\n            \n        \n\n    \n    \n    \n\n        \n\n            \n                \n                \n                    \n                \n                \n            \n\n            R, GIS, and fuzzyjoin to reconstruct demographic data for NUTS regions of Denmark\n\n            \n            \n                \n                \n                    r\n                \n                \n                \n                    rspatial\n                \n                \n            \n            \n\n            \n                March 16, 2017\n            \n\n            \n        \n\n    \n    \n    \n\n        \n\n            \n                \n                \n                    \n                \n                \n            \n\n            Gender gap in Swedish mortality\n\n            \n            \n                \n                \n                    r\n                \n                \n                \n                    demography\n                \n                \n            \n            \n\n            \n                February 25, 2017\n            \n\n            \n        \n\n    \n    \n    \n\n        \n\n            \n                \n                \n                    \n                \n                \n            \n\n            30 issues of Demographic Digest - the most frequent journals\n\n            \n            \n                \n                \n                    r\n                \n                \n                \n                    demography\n                \n                \n                \n                    bibliometrics\n                \n                \n            \n            \n\n            \n                February 14, 2017\n            \n\n            \n        \n\n    \n    \n    \n\n        \n\n            \n                \n                \n                    \n                \n                \n            \n\n            Male mortality in Russia and Japan\n\n            \n            \n                \n                \n                    r\n                \n                \n                \n                    hmd\n                \n                \n                \n                    demography\n                \n                \n            \n            \n\n            \n                February 6, 2017\n            \n\n            \n        \n\n    \n    \n    \n\n        \n\n            \n                \n                \n                    \n                \n                \n            \n\n            Sex ratios in all countries from Human Mortality Database\n\n            \n            \n                \n                \n                    r\n                \n                \n                \n                    hmd\n                \n                \n                \n                    demography\n                \n                \n            \n            \n\n            \n                February 5, 2017\n            \n\n            \n        \n\n    \n    \n    \n\n        \n\n            \n                \n                \n                    \n                \n                \n            \n\n            Hello R world post\n\n            \n            \n                \n                \n                    r\n                \n                \n            \n            \n\n            \n                January 29, 2017\n            \n\n            \n        \n\n    \n    \n\n\nNo matching items\n\n\n\n\n\n  Subscribe for email updates"
  },
  {
    "objectID": "2017/ice-hockey-players-height/index.html",
    "href": "2017/ice-hockey-players-height/index.html",
    "title": "Evolution of ice hockey players’ height: IIHF world championships 2001-2016",
    "section": "",
    "text": "The 2017 Ice Hockey World Championship has started. Thus I want to share a small research on the height of ice hockey players that I did almost a year ago and published in Russian.\nWhen the TV camera shows the players returning to the changing rooms, it is difficult not to notice just how huge the players are compared to the surrounding people – fans, journalists, coaches, or the ice arena workers. For example, here are the rising stars of the Finnish hockey – Patrik Laine and Aleksander Barkov – with the two fans in between.\nSource\nSo the questions arise. Are ice hockey players really taller than average people? How is the height of ice hockey players evolving over time? Are there any lasting differences between countries?\nData\nIIHF, the organization that is in charge for the ice hockey world championships, publishes detailed information on the squads, including the data on player’s height and weight. The raw data files are here. I gathered the data of all players that participated in the 16 world championships between 2001 and 2016. The formatting of the data files changes from year to year complicating the data processing. So I did the data cleaning manually which took a bit more than 3 hours. The unifies dataset is here. Let’s load the data and prepare the R session.\n\n# load required packages\nlibrary(tidyverse) # data manipulation and viz\nlibrary(lubridate) # easy manipulations with dates\nlibrary(ggthemes) # themes for ggplot2\nlibrary(texreg) # easy export of regression tables\nlibrary(xtable) # export a data frame into an html table\nlibrary(sysfonts) # change the font in figures\n\n\n# download the IIHF data set; if there are some problems, you can download manually\n# using the stable URL (https://dx.doi.org/10.6084/m9.figshare.3394735.v2)\ndf &lt;- read.csv(\"https://ndownloader.figshare.com/files/5303173\")\n\n# color palette\nbrbg11 &lt;- RColorBrewer::brewer.pal(11, \"BrBG\")\n\nDo the players become taller? (a crude comparison)\nLet’s first have a look at the pulled average height of all the players that participated.\n\n# mean height by championship\ndf_per &lt;- df |&gt; group_by(year) |&gt;\n        summarise(height = mean(height))\n\ngg_period_mean &lt;- ggplot(df_per, aes(x = year, y = height))+\n        geom_point(size = 3, color = brbg11[9])+\n        stat_smooth(method = \"lm\", size = 1, color = brbg11[11])+\n        ylab(\"height, cm\")+\n        xlab(\"year of competition\")+\n        scale_x_continuous(breaks = seq(2005, 2015, 5), labels = seq(2005, 2015, 5))+\n        theme_few(base_size = 15, base_family = \"mono\")+\n        theme(panel.grid = element_line(colour = \"grey75\", size = .25))\n\n\ngg_period_jitter &lt;- ggplot(df, aes(x = year, y = height))+\n        geom_jitter(size = 2, color = brbg11[9], alpha = .25, width = .75)+\n        stat_smooth(method = \"lm\", size = 1, se = F, color = brbg11[11])+\n        ylab(\"height, cm\")+\n        xlab(\"year of competition\")+\n        scale_x_continuous(breaks = seq(2005, 2015, 5), labels = seq(2005, 2015, 5))+\n        theme_few(base_size = 15, base_family = \"mono\")+\n        theme(panel.grid = element_line(colour = \"grey75\", size = .25))\n\ngg_period &lt;- cowplot::plot_grid(gg_period_mean, gg_period_jitter)\n\n\nFigure 1. The dynamics of the average height of the ice hockey players at the world championships, 2001–2016\nThe positive trend is evident. In the 15 years the average height of a player increased by almost 2 cm (left panel). Is that a lot? To have an idea, we will compare this growth to the dynamics in the population, later in the post.\nCohort approach\nA more correct way to study the dynamics of players’ height is to do the comparison between birth cohorts. Here we face an interesting data preparation issue – some of the players participated in more that one championships. The question is: do we need to clean the duplicate records? If the goal is to see the average height of a player at the certain championship (as in Figure 1), it is reasonable to keep all the records. Alternatively, if the aim is to analyze the dynamics of players’ height itself, I argue, it would be wrong to assign bigger weight to those players that participated in more that one championship. Thus, for the further cohort analysis, I cleaned the dataset from the duplicates.\n\ndfu_h &lt;- df |&gt; select(year, name, country, position, birth, cohort, height) |&gt;\n        spread(year, height)\ndfu_h$av.height &lt;- apply(dfu_h[, 6:21], 1, mean, na.rm = T)\ndfu_h$times_participated &lt;- apply(!is.na(dfu_h[, 6:21]), 1, sum)\n\ndfu_w &lt;- df |&gt; select(year, name, country, position, birth, cohort, weight) |&gt;\n        spread(year, weight)\ndfu_w$av.weight &lt;- apply(dfu_w[, 6:21], 1, mean, na.rm = T)\n\n\ndfu &lt;- left_join(\n    dfu_h |&gt; select(\n        name, country, position, birth, cohort, av.height, times_participated\n    ), \n    dfu_w |&gt; select(name, country, position, birth, cohort, av.weight), \n    by = c(\"name\", \"country\", \"position\", \"birth\", \"cohort\")\n) |&gt;\n    mutate(bmi = av.weight / (av.height / 100) ^ 2)\n\nThe total number of observations decreased from 6292 to 3333. For those who participated in more that one championship, I averaged the data on height and weight as they can change during the life-course. How many times, on average, are ice hockey players honored to represent their countries in the world championships? A bit less than 2.\n\n# frequencies of participation in world championships\nmean(dfu$times_participated)\n\ndf_part &lt;- as.data.frame(table(dfu$times_participated))\n\ngg_times_part &lt;- ggplot(df_part, aes(y = Freq, x = Var1))+\n        geom_bar(stat = \"identity\", fill = brbg11[8])+\n        ylab(\"# of players\")+\n        xlab(\"times participated (out of 16 possible)\")+\n        theme_few(base_size = 15, base_family = \"mono\")+\n        theme(panel.grid = element_line(colour = \"grey75\", size = .25))\n\n\nFigure 2. Histogram of the players by the number of times they participated in world championships over the period 2001-2016.\nBut there are unique players that participated in a considerable number of championships. Let’s have a look at those who participated at least 10 times out of 16 possible. There were just 14 such players.\n\n# the leaders of participation in world championships\nleaders &lt;- dfu |&gt; filter(times_participated &gt; 9)\nView(leaders)\n# save the table to html\nprint(xtable(leaders), type = \"html\", file = \"table_leaders.html\")\n\nTable 1. The most frequently participated players\n\n\n\nname\n\n\ncountry\n\n\nposition\n\n\nbirth date\n\n\ncohort\n\n\nav.height\n\n\ntimes _participated\n\n\nav.weight\n\n\nbmi\n\n\n\n\novechkin alexander\n\n\nRUS\n\n\nF\n\n\n1985-09-17\n\n\n1985\n\n\n188.45\n\n\n11\n\n\n98.36\n\n\n27.70\n\n\n\n\nnielsen daniel\n\n\nDEN\n\n\nD\n\n\n1980-10-31\n\n\n1980\n\n\n182.27\n\n\n11\n\n\n79.73\n\n\n24.00\n\n\n\n\nstaal kim\n\n\nDEN\n\n\nF\n\n\n1978-03-10\n\n\n1978\n\n\n182.00\n\n\n10\n\n\n87.80\n\n\n26.51\n\n\n\n\ngreen morten\n\n\nDEN\n\n\nF\n\n\n1981-03-19\n\n\n1981\n\n\n183.00\n\n\n12\n\n\n85.83\n\n\n25.63\n\n\n\n\nmasalskis edgars\n\n\nLAT\n\n\nG\n\n\n1980-03-31\n\n\n1980\n\n\n176.00\n\n\n12\n\n\n79.17\n\n\n25.56\n\n\n\n\nambuhl andres\n\n\nSUI\n\n\nF\n\n\n1983-09-14\n\n\n1983\n\n\n176.80\n\n\n10\n\n\n83.70\n\n\n26.78\n\n\n\n\ngranak dominik\n\n\nSVK\n\n\nD\n\n\n1983-06-11\n\n\n1983\n\n\n182.00\n\n\n10\n\n\n79.50\n\n\n24.00\n\n\n\n\nmadsen morten\n\n\nDEN\n\n\nF\n\n\n1987-01-16\n\n\n1987\n\n\n189.82\n\n\n11\n\n\n86.00\n\n\n23.87\n\n\n\n\nredlihs mikelis\n\n\nLAT\n\n\nF\n\n\n1984-07-01\n\n\n1984\n\n\n180.00\n\n\n10\n\n\n80.40\n\n\n24.81\n\n\n\n\ncipulis martins\n\n\nLAT\n\n\nF\n\n\n1980-11-29\n\n\n1980\n\n\n180.70\n\n\n10\n\n\n82.10\n\n\n25.14\n\n\n\n\nholos jonas\n\n\nNOR\n\n\nD\n\n\n1987-08-27\n\n\n1987\n\n\n180.18\n\n\n11\n\n\n91.36\n\n\n28.14\n\n\n\n\nbastiansen anders\n\n\nNOR\n\n\nF\n\n\n1980-10-31\n\n\n1980\n\n\n190.00\n\n\n11\n\n\n93.64\n\n\n25.94\n\n\n\n\nask morten\n\n\nNOR\n\n\nF\n\n\n1980-05-14\n\n\n1980\n\n\n185.00\n\n\n10\n\n\n88.30\n\n\n25.80\n\n\n\n\nforsberg kristian\n\n\nNOR\n\n\nF\n\n\n1986-05-05\n\n\n1986\n\n\n184.50\n\n\n10\n\n\n87.50\n\n\n25.70\n\n\n\nAlexander Ovechkin – 11 times! But it has to be noted that not every player had a possibility to participate in all the 16 championships between 2001 and 2016. That depends on a numder of factors:\n- the birth cohort of the player; - whether his national team regularly qualified for the championship (Figure 3); - whether the player was good enough for the national team; - whether he was free from the NHL play-offs that often keep the best players off the world championships.\n\n# countries times participated\ndf_cnt_part &lt;- df |&gt; select(year, country, no) |&gt;\n        mutate(country = factor(paste(country))) |&gt;\n        group_by(country, year) |&gt;\n        summarise(value = sum(as.numeric(no))) |&gt;\n        mutate(value = 1) |&gt;\n        ungroup() |&gt;\n        mutate(country = factor(country, levels = rev(levels(country))), \n               year = factor(year))\n\nd_cnt_n &lt;- df_cnt_part |&gt; group_by(country) |&gt;\n        summarise(n = sum(value))\n\ngg_cnt_part &lt;- ggplot(data = df_cnt_part, aes(x = year, y = country))+\n        geom_point(color = brbg11[11], size = 7)+\n        geom_text(\n            data = d_cnt_n, aes(y = country, x = 17.5, label = n, color = n), \n            size = 7, fontface = 2\n        )+\n        geom_text(data = d_cnt_n, aes(y = country, x = 18.5, label = \" \"), size = 7)+\n        scale_color_gradientn(colours = brbg11[7:11])+\n        xlab(NULL)+\n        ylab(NULL)+\n        theme_bw(base_size = 25, base_family = \"mono\")+\n        theme(legend.position = \"none\", \n              axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5))\n\n\nFigure 3. Stats of the national teams participation in the world championships\nDo the ice hochey players become taller? (regression analysis)\nThe regression analysis allows to address the research question – the association between player’s height and birth cohort – accounting for the cross-national differences and player’s position. I use OLS regressions, that are quite sensitive to outliers. I removed the birth cohorts for which there are less than 10 players – 1963, 1997, and 1998.\n\n# remove small cohorts\ntable(dfu$cohort)\ndfuc &lt;- dfu |&gt; filter(cohort &lt; 1997, cohort &gt; 1963)\n\nSo, the results. I add the variables one by one.\nDependent variable: player’s height.Explaining variables: 1) birth cohort; 2) position (compared to defenders); 3) country (compared to Russia).\n\n# relevel counrty variable to compare with Russia\ndfuc$country &lt;- relevel(dfuc$country, ref = \"RUS\")\n\n# regression models\nm1 &lt;- lm(data = dfuc, av.height~cohort)\nm2 &lt;- lm(data = dfuc, av.height~cohort+position)\nm3 &lt;- lm(data = dfuc, av.height~cohort+position+country)\n\n# export the models to html\nhtmlreg(list(m1, m2, m3), file = \"models_height.html\", single.row = T)\n\nTable2. The models\n\n\n\n\n\nModel 1\n\n\nModel 2\n\n\nModel 3\n\n\n\n\n(Intercept)\n\n\n-10.17 (27.67)\n\n\n-18.64 (27.01)\n\n\n32.59 (27.00)\n\n\n\n\ncohort\n\n\n0.10 (0.01)***\n\n\n0.10 (0.01)***\n\n\n0.08 (0.01)***\n\n\n\n\npositionF\n\n\n\n\n-2.59 (0.20)***\n\n\n-2.59 (0.20)***\n\n\n\n\npositionG\n\n\n\n\n-1.96 (0.31)***\n\n\n-1.93 (0.30)***\n\n\n\n\ncountryAUT\n\n\n\n\n\n\n-0.94 (0.55)\n\n\n\n\ncountryBLR\n\n\n\n\n\n\n-0.95 (0.53)\n\n\n\n\ncountryCAN\n\n\n\n\n\n\n1.13 (0.46)*\n\n\n\n\ncountryCZE\n\n\n\n\n\n\n0.56 (0.49)\n\n\n\n\ncountryDEN\n\n\n\n\n\n\n-0.10 (0.56)\n\n\n\n\ncountryFIN\n\n\n\n\n\n\n0.20 (0.50)\n\n\n\n\ncountryFRA\n\n\n\n\n\n\n-2.19 (0.69)**\n\n\n\n\ncountryGER\n\n\n\n\n\n\n-0.61 (0.51)\n\n\n\n\ncountryHUN\n\n\n\n\n\n\n-0.61 (0.86)\n\n\n\n\ncountryITA\n\n\n\n\n\n\n-3.58 (0.61)***\n\n\n\n\ncountryJPN\n\n\n\n\n\n\n-5.24 (0.71)***\n\n\n\n\ncountryKAZ\n\n\n\n\n\n\n-1.16 (0.57)*\n\n\n\n\ncountryLAT\n\n\n\n\n\n\n-1.38 (0.55)*\n\n\n\n\ncountryNOR\n\n\n\n\n\n\n-1.61 (0.62)**\n\n\n\n\ncountryPOL\n\n\n\n\n\n\n0.06 (1.12)\n\n\n\n\ncountrySLO\n\n\n\n\n\n\n-1.55 (0.58)**\n\n\n\n\ncountrySUI\n\n\n\n\n\n\n-1.80 (0.53)***\n\n\n\n\ncountrySVK\n\n\n\n\n\n\n1.44 (0.50)**\n\n\n\n\ncountrySWE\n\n\n\n\n\n\n1.18 (0.48)*\n\n\n\n\ncountryUKR\n\n\n\n\n\n\n-1.82 (0.59)**\n\n\n\n\ncountryUSA\n\n\n\n\n\n\n0.54 (0.45)\n\n\n\n\nR2\n\n\n0.01\n\n\n0.06\n\n\n0.13\n\n\n\n\nAdj. R2\n\n\n0.01\n\n\n0.06\n\n\n0.12\n\n\n\n\nNum. obs.\n\n\n3319\n\n\n3319\n\n\n3319\n\n\n\n\nRMSE\n\n\n5.40\n\n\n5.27\n\n\n5.10\n\n\n\nModel 1. One year change in the birth cohort year is associated with an increase of 0.1 cm in height. The coefficient is statistically significant, yet the variable explains only 1% of the variance. That’s not a big problem since the aim of the modeling is to document the differences, rather than predict based on the model. Nevertheless, the low coefficient of determination means that there are other variables that explain the differences in players’ height better than just the birth cohort.\nModel 2. Defenders are the tallest ice hockey players: goalkeepers are 2 cm shorter, forwards are 2.6 cm shorter. All the coefficients are significant; R squared rose to 6%. It is worth noting that the coefficient for the birth cohort did not change when we added the new variable.\nModel 3. It is interesting to control for countries for two reasons. First, some of the differences are significant themselves. For example, Swedes, Slovaks, and Canadians are higher than Russians. In contrast, Japanese are 5.2 cm shorter, Italians – 3.6 cm, French – 2.2 cm (figure 4). Second, once the country controls are introduced, the coefficient for birth cohort decreased slightly meaning that some of the differences in height are explained by persisting cross-country differences. R squared rose to 13%.\n\n# players' height by country\ngg_av.h_country &lt;- ggplot(dfuc , aes(x = factor(cohort), y = av.height))+\n        geom_point(color = \"grey50\", alpha = .25)+\n        stat_summary(\n            aes(group = country), geom = \"line\", fun.y = mean, \n            size = .5, color = \"grey50\"\n        )+\n        stat_smooth(aes(group = country, color = country), geom = \"line\", size = 1)+\n        facet_wrap(~country, ncol = 4)+\n        coord_cartesian(ylim = c(170, 195))+\n        scale_x_discrete(\n            labels = paste(seq(1970, 1990, 10)), breaks = paste(seq(1970, 1990, 10))\n        )+\n        labs(x = \"birth cohort\", y = \"height, cm\")+\n        theme_few(base_size = 15, base_family = \"mono\")+\n        theme(legend.position = \"none\", \n              panel.grid = element_line(colour = \"grey75\", size = .25))\n\n\nFigure 4. The height of ice hockey players by nations\nThe last model indicates that from one birth cohort cohort to the other the height of ice hockey players increases 0.08 cm. That means an increase of 0.8 cm in a decade or a growth of 2.56 cm in the 32 years between 1964 and 1996. It is worth mentioning that once we run the analysis in cohorts and controlling for positions and nations, the speed of the player’s height increase becomes much humbler than in the crude pulled analysis (Figure 1): 0.8 cm per decade compared to 1.2 cm per decade.\nBefore we go further and compare the growth in player’s height to that of the population, let’s do the modeling separately for defenders, goalkeepers, and forwards. The exploratory plot (Figure 5) suggests that the correlation is stronger for goalkeepers and weaker for defenders.\n\ndfuc_pos &lt;- dfuc\nlevels(dfuc_pos$position) &lt;- c(\"Defenders\", \"Forwards\", \"Goalkeeprs\")\n\ngg_pos &lt;- ggplot(dfuc_pos , aes(x = cohort, y = av.height))+\n        geom_jitter(aes(color = position), alpha = .5, size = 2)+\n        stat_smooth(method = \"lm\", se = T, color = brbg11[11], size = 1)+\n        scale_x_continuous(labels = seq(1970, 1990, 10), breaks = seq(1970, 1990, 10))+\n        scale_color_manual(values = brbg11[c(8, 9, 10)])+\n        facet_wrap(~position, ncol = 3)+\n        xlab(\"birth cohort\")+\n        ylab(\"height, cm\")+\n        theme_few(base_size = 15, base_family = \"mono\")+\n        theme(\n            legend.position = \"none\", \n            panel.grid = element_line(colour = \"grey75\", size = .25)\n        )\n\n\nFigure 5. Correlation between height and birth cohort by position\n\n# separate models for positions\nm3d &lt;- lm(data = dfuc |&gt; filter(position == \"D\"), av.height~cohort+country)\nm3f &lt;- lm(data = dfuc |&gt; filter(position == \"F\"), av.height~cohort+country)\nm3g &lt;- lm(data = dfuc |&gt; filter(position == \"G\"), av.height~cohort+country)\nhtmlreg(\n    list(m3d, m3f, m3g), file = \"models_height_pos.html\", single.row = T, \n    custom.model.names = c(\"Model 3 D\", \"Model 3 F\", \"Model 3 G\")\n)\n\nTable 3. Model 3 – separately for defenders, forwards, and goalkeepers\n\n\n\n\n\nModel 3 D\n\n\nModel 3 F\n\n\nModel 3 G\n\n\n\n\n(Intercept)\n\n\n108.45 (46.46)*\n\n\n49.32 (36.73)\n\n\n-295.76 (74.61)***\n\n\n\n\ncohort\n\n\n0.04 (0.02)\n\n\n0.07 (0.02)***\n\n\n0.24 (0.04)***\n\n\n\n\ncountryAUT\n\n\n0.14 (0.96)\n\n\n-2.01 (0.75)**\n\n\n0.47 (1.47)\n\n\n\n\ncountryBLR\n\n\n0.30 (0.87)\n\n\n-1.53 (0.73)*\n\n\n-2.73 (1.55)\n\n\n\n\ncountryCAN\n\n\n1.55 (0.78)*\n\n\n0.39 (0.62)\n\n\n3.45 (1.26)**\n\n\n\n\ncountryCZE\n\n\n0.87 (0.84)\n\n\n0.30 (0.67)\n\n\n0.63 (1.36)\n\n\n\n\ncountryDEN\n\n\n-0.60 (0.95)\n\n\n0.10 (0.75)\n\n\n-0.19 (1.62)\n\n\n\n\ncountryFIN\n\n\n-0.55 (0.89)\n\n\n-0.04 (0.67)\n\n\n2.40 (1.32)\n\n\n\n\ncountryFRA\n\n\n-3.34 (1.15)**\n\n\n-2.06 (0.93)*\n\n\n1.39 (2.07)\n\n\n\n\ncountryGER\n\n\n0.48 (0.85)\n\n\n-1.40 (0.72)\n\n\n-0.65 (1.33)\n\n\n\n\ncountryHUN\n\n\n-1.32 (1.47)\n\n\n-0.70 (1.16)\n\n\n0.65 (2.39)\n\n\n\n\ncountryITA\n\n\n-2.08 (1.08)\n\n\n-4.78 (0.82)***\n\n\n-2.02 (1.62)\n\n\n\n\ncountryJPN\n\n\n-4.13 (1.26)**\n\n\n-6.52 (0.94)***\n\n\n-2.27 (1.98)\n\n\n\n\ncountryKAZ\n\n\n-1.23 (0.95)\n\n\n-1.82 (0.79)*\n\n\n1.79 (1.58)\n\n\n\n\ncountryLAT\n\n\n-0.73 (0.95)\n\n\n-1.39 (0.75)\n\n\n-3.42 (1.49)*\n\n\n\n\ncountryNOR\n\n\n-3.25 (1.07)**\n\n\n-1.06 (0.85)\n\n\n-0.10 (1.66)\n\n\n\n\ncountryPOL\n\n\n0.82 (1.89)\n\n\n-0.58 (1.55)\n\n\n0.37 (2.97)\n\n\n\n\ncountrySLO\n\n\n-1.57 (0.99)\n\n\n-1.54 (0.79)\n\n\n-2.25 (1.66)\n\n\n\n\ncountrySUI\n\n\n-1.98 (0.91)*\n\n\n-2.36 (0.71)***\n\n\n1.12 (1.47)\n\n\n\n\ncountrySVK\n\n\n2.94 (0.87)***\n\n\n0.81 (0.67)\n\n\n-0.70 (1.50)\n\n\n\n\ncountrySWE\n\n\n0.75 (0.81)\n\n\n1.24 (0.65)\n\n\n1.37 (1.33)\n\n\n\n\ncountryUKR\n\n\n-1.37 (1.01)\n\n\n-1.77 (0.80)*\n\n\n-3.71 (1.66)*\n\n\n\n\ncountryUSA\n\n\n0.76 (0.78)\n\n\n-0.08 (0.62)\n\n\n2.58 (1.26)*\n\n\n\n\nR2\n\n\n0.09\n\n\n0.10\n\n\n0.24\n\n\n\n\nAdj. R2\n\n\n0.07\n\n\n0.09\n\n\n0.20\n\n\n\n\nNum. obs.\n\n\n1094\n\n\n1824\n\n\n401\n\n\n\n\nRMSE\n\n\n5.08\n\n\n5.08\n\n\n4.87\n\n\n\nThe separate modeling shows that the average height of ice hockey players, that were born in 1964-1996 and participated in the world championships in 2001–2016, increased with the speed of 0.4 cm per decade for defenders, 0.7 cm – for forwards, and (!) 2.4 cm – for goalies. In three decades the average height of the goalkeepers increased by 7 cm!\nFinally, let’s compare these dynamics with those in the population.\nCompare to population\nOur previous results expose significant height differences between players of various nations. Thus, it is reasonable to compare ice hockey players’ height to the corresponding male population of their countries.\nFor the data on the height of males in population in the corresponding nations I used the relevant scientific paper. I grabbed the data from the paper PDF using a nice little tool – tabula – and also deposited on figshare.\n\n# download the data from Hatton, T. J., & Bray, B. E. (2010).\n# Long run trends in the heights of European men, 19th–20th centuries.\n# Economics & Human Biology, 8(3), 405–413.\n# http://doi.org/10.1016/j.ehb.2010.03.001\n# stable URL, copied data (https://dx.doi.org/10.6084/m9.figshare.3394795.v1)\n\ndf_hb &lt;- read.csv(\"https://ndownloader.figshare.com/files/5303878\") \n\ndf_hb &lt;- df_hb |&gt;\n        gather(\"country\", \"h_pop\", 2:16) |&gt;\n        mutate(period = paste(period)) |&gt;\n        separate(period, c(\"t1\", \"t2\"), sep = \"/\")|&gt;\n        transmute(cohort = (as.numeric(t1)+as.numeric(t2))/2, country, h_pop)\n\n# calculate hockey players' cohort height averages for each country\ndf_hoc &lt;- dfu |&gt; group_by(country, cohort) |&gt;\n        summarise(h_hp = mean(av.height)) |&gt;\n        ungroup()\n\nUnfortunately, our dataset on ice hockey players intersects with the data on population only for 8 countries: Austria, Denmark, Finland, France, Germany, Italy, Norway, and Sweden.\n\n# countries in both data sets\nboth_cnt &lt;- levels(factor(df_hb$country))[which(levels(factor(df_hb$country)) %in% levels(df_hoc$country))]\nboth_cnt\n\n\ngg_hoc_vs_pop &lt;- ggplot()+\n        geom_path(data = df_hb |&gt; filter(country %in% both_cnt), \n                  aes(x = cohort, y = h_pop), \n                  color = brbg11[9], size = 1)+\n        geom_point(data = df_hb |&gt; filter(country %in% both_cnt), \n                   aes(x = cohort, y = h_pop), \n                   color = brbg11[9], size = 2)+\n        geom_point(data = df_hb |&gt; filter(country %in% both_cnt), \n                   aes(x = cohort, y = h_pop), \n                   color = \"white\", size = 1.5)+\n        geom_point(data = df_hoc |&gt; filter(country %in% both_cnt), \n                   aes(x = cohort, y = h_hp), \n                   color = brbg11[3], size = 2, pch = 18)+\n        stat_smooth(data = df_hoc |&gt; filter(country %in% both_cnt), \n                    aes(x = cohort, y = h_hp), \n                    method = \"lm\", se = F, color = brbg11[1], size = 1)+\n        facet_wrap(~country, ncol = 2)+\n        labs(y = \"height, cm\", x = \"birth cohort\")+\n        theme_few(base_size = 20, base_family = \"mono\")+\n        theme(panel.grid = element_line(colour = \"grey75\", size = .25))\n\n\nFigure 6. The comparison of height dynamics in ice hockey players (brown) and the corresponding male populations (green)\nIn all the analyzed countries, ice hockey players are 2-5 cm higher that the nation’s average. This is not very surprising since we expect some selection in sport. What is more interesting, in the developed countries the rapid increase in the height of males mostly leveled off in the birth cohorts of 1960s. Unlike the population trend, the height of ice hockey players continued to increase with roughly the same pace in all the analyzed countries except for Denmark.\nFor the cohorts of Europeans that were born in first half of 20-th century, the height of males increased by 1.18–1.74 cm per decade (Figure 7, middle panel). Starting from the birth cohorts of 1960s, the pace decreased to 0.15–0.80 per decade.\n\n# growth in population\n\ndf_hb_w &lt;- df_hb |&gt; spread(cohort, h_pop) \nnames(df_hb_w)[2:26] &lt;- paste(\"y\", names(df_hb_w)[2:26])\n\ndiffs &lt;- df_hb_w[, 3:26]-df_hb_w[, 2:25]\n\ndf_hb_gr&lt;- df_hb_w |&gt;\n        transmute(\n            country, \n            gr_1961_1980 = unname(apply(diffs[, 22:24], 1, mean, na.rm = T))*2, \n            gr_1901_1960 = unname(apply(diffs[, 9:21], 1, mean, na.rm = T))*2, \n            gr_1856_1900 = unname(apply(diffs[, 1:8], 1, mean, na.rm = T))*2\n        ) |&gt;\n        gather(\"period\", \"average_growth\", 2:4) |&gt;\n        filter(country %in% both_cnt) |&gt;\n        mutate(\n            country = factor(country, levels = rev(levels(factor(country)))), \n            period = factor(period, labels = c(\"1856-1900\", \"1901-1960\", \"1961-1980\"))\n        )\n\n\ngg_hb_growth &lt;- ggplot(df_hb_gr, aes(x = average_growth, y = country))+\n        geom_point(aes(color = period), size = 3)+\n        scale_color_manual(values = brbg11[c(8, 3, 10)])+\n        scale_x_continuous(limits = c(0, 2.15))+\n        facet_wrap(~period)+\n        theme_few()+\n        xlab(\"average growth in men's height over 10 years, cm\")+\n        ylab(NULL)+\n        theme_few(base_size = 20, base_family = \"mono\")+\n        theme(\n            legend.position = \"none\", \n            panel.grid = element_line(colour = \"grey75\", size = .25)\n        )\n\n\nFigure 7. Average changes in male population\nThe height increase for ice hockey players seems quite impressive if we compare it to the stagnating dynamics in the corresponding male populations. And the acceleration of goalkeepers’ height is outright amazing.\nThe diverging trends in the height of ice hockey players and normal population is likely to be driven by the strengthening selection in sport.\nSelection in ice hockey\nLooking through the literature on the selection in sport, I saw the finding that showed a notable disproportion of professional sportsmen by the month of birth. There are much more sportsmen that were born in the first half of the year. They have a lasting advantage since the kids teams are usually formed by birth cohorts. Thus, those born earlier in the year always have a bit more time lived compared to their later born team mates, which means that they are physically more mature. It is easy to test the finding on our ice hockey players dataset.\n\n# check if there are more players born in earlier months\ndf_month &lt;- df |&gt; mutate(month = month(birth)) |&gt;\n        mutate(month = factor(month))\n\ngg_month &lt;- ggplot(df_month, aes(x = factor(month)))+\n        geom_bar(stat = \"count\", fill = brbg11[8])+\n        scale_x_discrete(breaks = 1:12, labels = month.abb)+\n        labs(x = \"month of birth\", y = \"# of players\")+\n        theme_few(base_size = 20, base_family = \"mono\")+\n        theme(\n            legend.position = \"none\", \n            panel.grid = element_line(colour = \"grey75\", size = .25)\n        )\n\n\nFigure 8. The distribution of ice hockey players by month of birth\nTrue, the distribution is notably skewed – there are much more players born in earlier months. When I further split the dataset by the decades of birth, it becomes clear that the effect becomes more evident with time (Figure 9). Indirectly, that means that the selection in ice hockey becomes tougher.\n\n# facet by decades\ndf_month_dec &lt;- df_month |&gt;\n        mutate(dec = substr(paste(cohort), 3, 3) |&gt; \n                       factor(labels = paste(\"born in\", c(\"1960s\", \"1970s\", \"1980s\", \"1990s\"))))\n\ngg_month_dec &lt;- ggplot(df_month_dec, aes(x = factor(month)))+\n        geom_bar(stat = \"count\", fill = brbg11[8])+\n        scale_x_discrete(breaks = 1:12, labels = month.abb)+\n        labs(x = \"month of birth\", y = \"# of players\")+\n        facet_wrap(~dec, ncol = 2, scales = \"free\")+\n        theme_few(base_size = 20, base_family = \"mono\")+\n        theme(legend.position = \"none\", \n              panel.grid = element_line(colour = \"grey75\", size = .25))\n\n\nFigure 9. The distribution of ice hockey players by month of birth – separately by decades of birth\n\n\n\n\n\n\n\nThe full R script can be downloaded here"
  },
  {
    "objectID": "2023/map-borders/index.html",
    "href": "2023/map-borders/index.html",
    "title": "The easiest way to radically improve map aesthetics",
    "section": "",
    "text": "Since R community developed brilliant tools to deal with spatial data, producing maps is no longer the privilege of a narrow group of people with very specific almost esoteric knowledge, skillset, and often super expensive software. With #rspatial packages, maps (at least the relatively simple ones) became just another type of dataviz.\n\nJust a few lines of code can reveal the eye-catching and visually pleasant spatial dimension of the data. Similarly, a few more lines of code can radically improve the pleasantness of a simple map – just add borders as lines in a separate spatial layer.\n\nAn often “quick and dirty” solution when composing a simple choropleth map is to use polygons outline as the borders. While this works okay to distinguish the polygons, the map quickly becomes unnecessarily overloaded. All the non-bordering outlines – complicated coastal lines and islands’ outlines – look ugly and add nothing to the map.\nLet’s illustrate the ease of this trick mapping Greece with its numerous small islands. We’ll use the beautiful eurostat package that has a built in spatial dataset with NUTS-3 regions of Europe.\n\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(cowplot)\n\nset.seed(911)\n\n# subset Greence, NUTS-3 regions\nlibrary(eurostat)\ngreece &lt;- eurostat_geodata_60_2016 |&gt; \n    filter(LEVL_CODE==3,\n           str_sub(geo, 1, 2) == \"EL\") |&gt; \n    # create random values for filling the polygons\n    mutate(random = runif(length(id))) |&gt; \n    select(id, geometry, random) |&gt; \n    st_transform(crs = 3035)\n\nFirst, here’s the typical lazy (or rather no-brainer) way of using the polygons’ outlines to show the borders between our spatial units.\n\n# plot with polygon outlines\ngreece |&gt; \n    ggplot()+\n    geom_sf(aes(fill = random), color = 2, size = 1)+\n    labs(title = \"Polygons outlined\")+\n    scale_fill_viridis_c(begin = .5)+\n    theme_map()+\n    theme(plot.background = element_rect(color = NA, fill = \"#eeffff\"))\n\n\n\n\n\n\ngg_outline &lt;- last_plot()\n\nLook at all the islands, especially the small ones – what are all these red outlines for? Insted, we can add only the borders between the polygons as lines. For this we need to add another geospatial layer with lines. Where do we get it? This is extremely easy to produce thanks to the marvelous little package rmapshaper that has a function ms_innerlines() exactly for the task. 1\n1 Before I found rmapshaper the task seemed overly complicated, I even asked Stack Overflow\n# produce border lines with rmapshaper::ms_innerlines()\nlibrary(rmapshaper)\nbord &lt;- greece |&gt; ms_innerlines()\n\nNow, let’s plot the same map with proper borders between the polygons. Note that for the sf layer with polygons I set color = NA to get rid of the polygons outline. Then with the next call to geom_sf() I draw the line borders as a separate layer.\n\n# now plot without polygon outlines and with borders as lines\ngreece |&gt; \n    ggplot()+\n    geom_sf(aes(fill = random), color = NA)+\n    geom_sf(data = bord, color = 2, size = 1)+\n    labs(title = \"Borders as lines\")+\n    scale_fill_viridis_c(begin = .5)+\n    theme_map()+\n    theme(plot.background = element_rect(color = NA, fill = \"#eeffff\"))\n\n\n\n\n\n\ngg_bord &lt;- last_plot()\n\nThat’s it! This is the simplest dataviz trick I know that can radically improve the outlook of simple choropleth maps. It’s only one additional line of code. You can even create the borders sf object on the fly within the ggplot map creation code specifying the data parameter as . %&gt;% ms_innerlines() 2, like this:\n2 This is one specific case where the base R pipe |&gt; cannot simply replace the {magrittr} pipe %&gt;%; see more here.\ngeom_sf(data = . %&gt;% ms_innerlines(), color = 2, size = 1)\n\nFinally, let’s put the two maps side by side.\n\n# put side by side\nlibrary(patchwork)\n(\n    gg_outline + gg_bord \n)  + \n    plot_layout(guides = \"collect\")+\n    plot_annotation(\n        caption = \"! Look at the islands\", \n        theme = theme(plot.background = element_rect(color = NA, fill = \"#eeffff\"))\n    )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReplicate this analysis using the R code from this gist. This post is partially based on my previous Twitter thread\n\n\n\n\n\n\n\n\n\n\n\n\nAbout this post\n\n\n\nPublishing this post is my personal gestalt closure – it spent more than three years in planning and then in drafts. Somehow, with this post I hit the wall of writer’s block and it coincided with Twitter threads substituting blogging for me. Now, it’s time to get back to blogging."
  },
  {
    "objectID": "2023/geocoding/index.html",
    "href": "2023/geocoding/index.html",
    "title": "Geocode address text strings using tidygeocoder\n",
    "section": "",
    "text": "Deriving coordinates from a string of text that represents a physical location on Earth is a common geo data processing task. A usual use case would be an address question in a survey. There is a way to automate queries to a special GIS service so that it takes a text string as an input and returns the geographic coordinates. This used to be quite a challenging task since it required obtaining an API access to the GIS service like Google Maps. Things changed radically with the appearance of tidygeocoder that queries the free Open Street Map.\nIn this tiny example I’m using the birth places that students of my 2022 BSSD dataviz course kindly contributed. In the class I asked students to fill a Google Form consisting of just two fields – city and country of birth. The resulting small dataset is here\n\nlibrary(tidyverse)\nlibrary(sf)\n\n# download the data\n# https://stackoverflow.com/a/28986107/4638884\nlibrary(gsheet)\n\nraw &lt;- gsheet2tbl(\n    \"https://docs.google.com/spreadsheets/d/1YlfLQc_aOOiTqaSGu5TI70OQy1ewTa_Ti0qAEOEcy58\"\n)\n\n# clean a bit and join both fields in one text string \ndf &lt;- raw |&gt; \n    janitor::clean_names() |&gt; \n    drop_na() |&gt; \n    mutate(text_to_geocode = paste(city_settlement, country, sep = \", \"))\n\nNow we are ready to unleash the power of tidygeocoder. The way the main unction in the package works is very similar to mutate – you just specify which column of the dataset contains the text string to geocode, and it return the geographic coordinates.\n\nlibrary(tidygeocoder)\n\ndf_geocoded &lt;- df |&gt; \n    geocode(text_to_geocode, method = \"osm\")\n\nThe magic has already happened. The rest is just the routines to drop the points on the map. Yes, I am submitting this as my first 2023 entry to the #30DayMapChallenge =)\n\n# convert coordinates to an sf object\ndf_plot &lt;- df_geocoded |&gt; \n    drop_na() |&gt; \n    st_as_sf(\n        coords = c(\"long\", \"lat\"),\n        crs = 4326\n    )\n\nNext are several steps to plot countries of the worlds as the background map layer. Note that I’m using the trick of producing a separate lines layer for the country borders, there is a separate post about this small dataviz trick.\n\n# get world map outline (you might need to install the package)\nworld_outline &lt;- spData::world |&gt; \n    st_as_sf()\n\n# let's use a fancy projection\nworld_outline_robinson &lt;- world_outline |&gt; \n    st_transform(crs = \"ESRI:54030\")\n\ncountry_borders &lt;- world_outline_robinson |&gt; \n    rmapshaper::ms_innerlines()\n\nNow everything is ready to map!\n\n# map!\nworld_outline_robinson |&gt; \n    filter(!iso_a2 == \"AQ\") |&gt; # get rid of Antarctica\n    ggplot()+\n    geom_sf(fill = \"#269999\", color = NA)+\n    geom_sf(\n        data = country_borders, size = .25, \n        color = \"#269999\" |&gt; prismatic::clr_lighten()\n    )+\n    geom_sf(\n        data = df_plot, fill = \"#dafa26\", \n        color = \"#dafa26\" |&gt; prismatic::clr_darken(),\n        size = 1.5, shape = 21\n    )+\n    coord_sf(datum = NA)+\n    theme_minimal(base_family = \"Atkinson Hyperlegible\")+\n    labs(\n        title = \"Birth places of the participants\",\n        subtitle = \"Barcelona Summer School of Demography \n        dataviz course at CED, July 2022\",\n        caption = \"@ikashnitsky.phd\"\n    )+\n    theme(\n        text = element_text(color = \"#ccffff\"),\n        plot.background = element_rect(fill = \"#042222\", color = NA),\n        axis.text = element_blank(),\n        plot.title = element_text(face = 2, size = 18, color = \"#ccffff\")\n    )\n\n\n\n\n\n\n\nThat’s it. Going from text to point on the map has never been easier.\n\n\n\n\n\n\n\nThis post is one in the dataviz course series. Other posts:\n\n\n\n\nThe easiest way to radically improve map aesthetics\nShow all data in the background of your faceted ggplot\nDotplot – the single most useful yet largely neglected dataviz type\nSave space in faceted plots"
  },
  {
    "objectID": "2017/subplots-in-maps/index.html",
    "href": "2017/subplots-in-maps/index.html",
    "title": "Subplots in maps with ggplot2",
    "section": "",
    "text": "Following the surprising success of my latest post, I decided to show yet another use case of the handy annotation custom function. Here I will show how to add small graphical information to maps – just like putting a stamp on an envelope.\nThe example comes from my current work on a paper, in which I study the effect of urban/rural differences on the relative differences in population ageing (I plan to tell a bit more in one of the next posts). Let’s have a look at the map we are going to reproduce in this post:\n\nSo, with this map I want to show the location of more and less urbanized NUTS-2 regions of Europe. But I also want to show – with subplots – how I defined the three subregions of Europe (Eastern, Southern, and Western) and what is the relative frequency of the three categories of regions (Predominantly Rural, Intermediate, and Predominantly Rural) within each of the subregions. The logic of actions is simple: first prepare all the components, then assemble them in a composite plot. Let’s go!\nThe code to prepare R session and load the data.\n\n# additional packages\nlibrary(tidyverse)\nlibrary(ggthemes)\nlibrary(rgdal)\nlibrary(viridis)\nlibrary(RColorBrewer)\nlibrary(extrafont)\nmyfont &lt;- \"Roboto Condensed\"\n\n# load the already prepared data\nload(url(\"https://ikashnitsky.github.io/share/1705-map-subplots/df-27-261-urb-rur.RData\"))\nload(url(\"https://ikashnitsky.github.io/share/1705-map-subplots/spatial-27-261.RData\"))\n\nNow, I prepare the spatial objects to be plotted with ggplot2 and create a blank map of Europe – our canvas.\n\n# fortify spatial objects\nbord &lt;- fortify(Sborders)\nfort &lt;- fortify(Sn2, region = \"id\")\n\n# join spatial and statistical data\nfort_map &lt;- left_join(df,fort,\"id\")\n\n# create a blank map\nbasemap &lt;- ggplot()+\n        geom_polygon(data = fortify(Sneighbors),aes(x = long, y = lat, group = group),\n                     fill = \"grey90\",color = \"grey90\")+\n        coord_equal(ylim = c(1350000,5450000), xlim = c(2500000, 6600000))+\n        theme_map(base_family = myfont)+\n        theme(panel.border = element_rect(color = \"black\",size = .5,fill = NA),\n              legend.position = c(1, 1),\n              legend.justification = c(1, 1),\n              legend.background = element_rect(colour = NA, fill = NA),\n              legend.title = element_text(size = 15),\n              legend.text = element_text(size = 15))+\n        scale_x_continuous(expand = c(0,0)) +\n        scale_y_continuous(expand = c(0,0)) +\n        labs(x = NULL, y = NULL)\n\n\nOkay, now the envelope is ready. It’s time to prepare the stamps. Let’s create a nice mosaic plot showing the distribution of NUTS-2 regions by subregions and the urb/rur categories. I found the simplest way to create a nice mosaic plot on Stack Overflow.\n\n# create a nice mosaic plot; solution from SO:\n# http://stackoverflow.com/a/19252389/4638884\nmakeplot_mosaic &lt;- function(data, x, y, ...){\n        xvar &lt;- deparse(substitute(x))\n        yvar &lt;- deparse(substitute(y))\n        mydata &lt;- data[c(xvar, yvar)];\n        mytable &lt;- table(mydata);\n        widths &lt;- c(0, cumsum(apply(mytable, 1, sum)));\n        heights &lt;- apply(mytable, 1, function(x){c(0, cumsum(x/sum(x)))});\n        \n        alldata &lt;- data.frame();\n        allnames &lt;- data.frame();\n        for(i in 1:nrow(mytable)){\n                for(j in 1:ncol(mytable)){\n                        alldata &lt;- rbind(alldata, c(widths[i], \n                                                    widths[i+1], \n                                                    heights[j, i], \n                                                    heights[j+1, i]));\n                }\n        }\n        colnames(alldata) &lt;- c(\"xmin\", \"xmax\", \"ymin\", \"ymax\")\n        \n        alldata[[xvar]] &lt;- rep(dimnames(mytable)[[1]], \n                               rep(ncol(mytable), nrow(mytable)));\n        alldata[[yvar]] &lt;- rep(dimnames(mytable)[[2]], nrow(mytable));\n        \n        ggplot(alldata, aes(xmin=xmin, xmax=xmax, ymin=ymin, ymax=ymax)) + \n                geom_rect(color=\"white\", aes_string(fill=yvar)) +\n                xlab(paste(xvar, \"(count)\")) + \n                ylab(paste(yvar, \"(proportion)\"));\n}\n\ntyp_mosaic &lt;- makeplot_mosaic(data = df |&gt; mutate(type = as.numeric(type)), \n                              x = subregion, y = type)+\n        theme_void()+\n        scale_fill_viridis(option = \"B\", discrete = T, end = .8)+\n        scale_y_continuous(limits = c(0, 1.4))+\n        annotate(\"text\",x = c(27, 82.5, 186), y = 1.05, \n                 label=c(\"EAST\", \"SOUTH\", \"WEST\"), \n                 size = 4, fontface = 2, \n                 vjust = 0.5, hjust = 0,\n                 family = myfont) + \n        coord_flip()+\n        theme(legend.position = \"none\")\n\n\nJust what we needed. The next step is to build a small map showing the three subregions of Europe. But before we proceed to the maps, one thing has to be fixed. ggplot2 fails rendering nested polygons. With our regional dataset, London, for example, will not be shown if we do not account for this unpleasant feature. Luckily, there is quite a simple solution to fix that problem.\n\n# a nice small function to overcome some mapping problems with nested polygons\n# see more at SO\n# https://stackoverflow.com/questions/21748852\ngghole &lt;- function (fort) {\n        poly &lt;- fort[fort$id %in% fort[fort$hole, ]$id, ]\n        hole &lt;- fort[!fort$id %in% fort[fort$hole, ]$id, ]\n        out &lt;- list(poly, hole)\n        names(out) &lt;- c(\"poly\", \"hole\")\n        return(out)\n}\n\nNow I build the small map of subregions.\n\n# pal for the subregions\nbrbg3 &lt;- brewer.pal(11,\"BrBG\")[c(8,2,11)]\n\n# annotate a small map of the subregions of Europe\nan_sub &lt;- basemap +\n        geom_polygon(data = gghole(fort_map)[[1]], \n                     aes(x = long, y = lat, group = group, fill = subregion),\n                     color = NA)+\n        geom_polygon(data  =  gghole(fort_map)[[2]], \n                     aes(x = long, y = lat, group = group, fill = subregion),\n                     color = NA)+\n        scale_fill_manual(values = rev(brbg3)) +\n        theme(legend.position = \"none\")\n\n\nFinally, everything is ready to build the main map and stick the two subplots on top of it.\n\n# finally the map of Urb/Rur typology\n\ncaption &lt;- \"Classification: De Beer, J., Van Der Gaag, N., & Van Der Erf, R. (2014). \nNew classification of urban and rural NUTS 2 regions in Europe. \nNIDI Working Papers, 2014/3. \nRetrieved from http://www.nidi.nl/shared/content/output/papers/nidi-wp-2014-03.pdf\n\\nIlya Kashnitsky (ikashnitsky.github.io)\"\n\ntyp &lt;-  basemap + \n        \n        geom_polygon(data = gghole(fort_map)[[1]], \n                     aes(x=long, y=lat, group=group, fill=type),\n                     color=\"grey30\",size=.1)+\n        geom_polygon(data = gghole(fort_map)[[2]], \n                     aes(x=long, y=lat, group=group, fill=type),\n                     color=\"grey30\",size=.1)+\n        scale_fill_viridis(\"NEUJOBS\\nclassification of\\nNUTS-2 regions\", \n                           option = \"B\", discrete = T, end = .8)+\n        geom_path(data = bord, aes(x = long, y = lat, group = group),\n                  color = \"grey20\",size = .5) + \n        \n        annotation_custom(grob = ggplotGrob(typ_mosaic), \n                          xmin = 2500000, xmax = 4000000, \n                          ymin = 4450000, ymax = 5450000)+\n        annotation_custom(grob = ggplotGrob(an_sub), \n                          xmin = 5400000, xmax = 6600000, \n                          ymin = 2950000, ymax = 4150000)+\n        labs(title = \"Urban / Rural classification of NUTS-2 regions of Europe\\n\",\n             caption = paste(strwrap(caption, width = 95), collapse = '\\n'))+\n        theme(plot.title = element_text(size = 20),\n              plot.caption = element_text(size = 12))\n\n\nDone!\nOf course, it takes several iterations to position each element in its proper place. Then, one also needs to play with export parameters to finally get the desired output.\n\n\n\n\n\n\n\nThe full R script for this post is here"
  },
  {
    "objectID": "2023/map-proj/index.html",
    "href": "2023/map-proj/index.html",
    "title": "Improve your maps in one line of code changing map projections",
    "section": "",
    "text": "Did you ever think why we (okay, I’m clearly biased, maybe just many of us, humans) love maps so much? Why do they often work so much better than other types of dataviz?\nI think1 what makes the maps work is the speed with which we can recognize familiar shapes, most often countries. That’s why it’s so annoying when these shapes get distorted – it hinders the smoothness of reading the map and kills the pleasure of the process. I’m sure this is the main reason why cartograms exist forever as a cool idea but are rarely actually used – people love the concept of them but hate actually looking at them. I deeply believe that immediate shape recognition is the kill feature of maps as a type of dataviz.\n1 And here I have to say that I’m not an expert in the area of visual perception or psychology of human processing of dataviz. I know this is a huge, thrilling area of research, I just never followed systemically. Strictly speaking, what I’m going to tell further is likely some digested version of what I’ve seen, read, and thought through over years of being into dataviz.Geographic projections are a huge field of research in itself. There are infinite ways to project the spherical globe (geoid) to the surface. It’s a classical challenge with no single correct solution. Specific choice depends on the features we want to preserve/represent most correctly — distances, angles, shapes, or areas. In my experience, for most of our daily basic dataviz needs the most important is the shape — it helps recognizing objects and thus navigating through maps smoothly.\nThere is a brilliant 6-min video explainer of maps projections, I heartily recommend:\n\n\nBeware, the choice of appropriate map projection can be a rabbit hole as deep as choosing fonts or colors for your dataviz – you are warned. In all practical terms, a good strategy is to check what are the canonical projections for the territories one plans to map – that would help not to repel the reader through instantaneous non-recognition of the shape. One good resource for the task is http://epsg.io.\nJust accept that the perfect projection does not exist – there’s no such thing as free lunch perfect projection. Map projections always excel or fail in specific contexts. Here is an example when a very non-standard geographic projection is just perfect for the data it is showing and the story it is telling.\n\nHere is another animated example.\nEven the most famous and often ridiculed Mercator projection has its major advantages – it preserves the angles and that’s why it was perfect for the early age of navigation. It’s also pretty safe at preserving coordinates and thus most often is used as the basic projection in which geodata is stored and distributed. And this is the reason why we see it in published maps so often. Honestly, I’m really allergic to the view of Europe in Mercator projection.\n\nAnd it’s just unbelievable how often these repulsing maps come across in academic papers. Especially when you know that it really takes one line of code to fix it. So let’s see how it’s done.\n\nFor the illustration below I will produce maps of Europe in (A) Mercator and (B) Lambert Equal Area Azimuthal projections, as usual using the beautiful geodata stored in the eurostat package.\n\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(eurostat)\n\n# the built-in dataset of EU boundaries\ngd &lt;- eurostat_geodata_60_2016 |&gt; \n    janitor::clean_names() \n\n# filters out only NUTS-2 regions \ngd_n2 &lt;- gd |&gt;\n    filter(levl_code == 2) \n\n# let's build the most basic map\ngd_n2 |&gt; \n    ggplot() +\n    geom_sf()\n\n\n\n\n\n\n\nNote all the overseas territories of France, Spain, and Portugal. At the next step I will remove them to zoom in to the usual scope of mainland Europe. I will also add borders between the countries as lines.\n\n# remove overseas territories\ngd_n2_main &lt;- gd_n2 |&gt; \n    filter(\n        !id %in% c(\n            paste0('ES', c(63, 64, 70)), \n            paste('FRY', 1:5, sep = ''), \n            'PT20', 'PT30'\n        )\n    ) \n\n# the lines level with borders at country level\nbord &lt;- gd |&gt;\n    filter(levl_code == 0) |&gt; \n    rmapshaper::ms_innerlines()\n\nNow, back to reprojecting. There are two ways of changing the projections with {sf}: either apply st_transform(crs = [EPSG code]) to change your the projection of the geodata object OR fix it directly at the plotting stage via coord_sf(crs = [EPSG code]). I think it’s generally easier to reproject the data once and work with it (panel B). But for completeness I will also show the on-the-fly coord_sf() when plotting in Mercator projection (panel A).\n\n# transform the projection to the one suitable for Europe\ngd_n2_main_laea &lt;- gd_n2_main |&gt; \n    st_transform(crs = 3035)\n\na &lt;- gd_n2_main |&gt; \n    ggplot() +\n    geom_sf(fill = \"#F48FB1\", color = NA)+\n    geom_sf(data = bord, color = \"#C2185B\", size = .5)+\n    coord_sf(crs = 3857)\n\nb &lt;- gd_n2_main_laea |&gt; \n    ggplot() +\n    geom_sf(fill = \"#DCE775\", color = NA)+\n    geom_sf(data = bord, color = \"#AFB42B\", size = .5)\n\nlibrary(patchwork)\n\na + b + plot_annotation(tag_levels = \"A\")\n\n\n\n\n\n\n\nThat’s it. That’s the whole one-line trick. Check out my other post based on my dataviz course materials.\n\n\n\n\n\n\n\nThis post is one in the dataviz course series. Other posts:\n\n\n\n\nThe easiest way to radically improve map aesthetics\nShow all data in the background of your faceted ggplot\nDotplot – the single most useful yet largely neglected dataviz type\nSave space in faceted plots\nGeocode address text strings using tidygeocoder"
  },
  {
    "objectID": "2023/gist-snippet/index.html",
    "href": "2023/gist-snippet/index.html",
    "title": "Easily re-using self-written functions: the power of gist + code snippet duo",
    "section": "",
    "text": "Quite often data processing or analysis needs bring us to write own functions. Sometimes these self-defined functions are only meaningful and useful within a certain workflow or even a certain script. But other self-written functions may be more generic and reusable in other circumstances. For example, one may want to have a version of ggsave() that always enforces bg = 'snow', or a theme_own() function with pre-saved preferences. Self-written functions live in {.GlobalEnv} and have to be re-defined in every new R session. Copying the same lines of code across projects can be boring. How to “bookmark” the useful little own functions and reuse them easier in other projects? This post offers an elegant solution.\nOne obvious way to store self-written functions would be to write an own package and have a easy access to these function via library() calls. While this may be quite comfortable for own coding purposes, the objective downside of this approach is replicability of the code – once the code leaves your specific machine, one would have to install your package in order to run the code. This seems an overkill to store a couple of occasional arbitrarily useful functions.\nI suggest a more convenient approach: store the functions as GitHub gists and call them using the handy devtools::source_gist(). This allows to load self-written functions from standalone R scripts. And to avoid copying manually the lines of code that source a certain gist we may use code snippets. Let me give you an example.\nI want to re-use a ggplot2 theme with certain preferred parameters. Here are the lines of code that define my theme_ik() function.\n\ntheme_ik &lt;- function(\n        base_size = 12,\n        base_family = \"sans\",\n        labs_color = \"#074949\",\n        axis_color = \"#002F2F\",\n        bg_color = \"#eeffff\",\n        grid_color = \"#ccffff\"\n){\n    theme_minimal(base_size = base_size, base_family = base_family)+\n        theme(\n            plot.title = element_text(\n                size = base_size*2, face = 2, color = labs_color\n            ),\n            plot.subtitle = element_text(color = labs_color),\n            plot.caption = element_text(color = labs_color),\n            axis.title = element_text(color = axis_color),\n            axis.text = element_text(color = axis_color),\n            plot.background = element_rect(color = NA, fill = bg_color),\n            legend.position = \"bottom\",\n            panel.spacing = unit(1, \"lines\"),\n            panel.grid.major = element_line(color = grid_color),\n            panel.grid.minor = element_blank(),\n            line = element_line(lineend = \"round\")\n        )\n}\n\nI store these lines of R code as a gist here. Next, I only need to supply the ID part of the gist URL (https://gist.github.com/ikashnitsky/653e1040a07364ae82b1bb312501a184) to the devtools::source_gist() function and it will execute the script stored in the gist, which will result in function theme_ik() appearing in my {.GlobalEnv}. With the second line of code I set the default ggplot2 theme to my self-written one.\n\ndevtools::source_gist(\"653e1040a07364ae82b1bb312501a184\")\ntheme_set(theme_ik())\n\nNow, the final element of this recipe is to save the two lines above as a code snippet. I’m using RStudio, but code snippets are available in any decent IDE. To add a custom snippet we need to navigate to Tools --&gt; Edit Code Snippets.... In the new window just add a custom snippet making sure to respect the indentation. 1\n1 Note that in the left tabset one can choose what kind of snippets to add. In R scripts snippets are called with TAB, in rmarkdown documents the hot-key for snippets is SHIFT+TAB.\nThat’s it. Save the modified snippets, the new one is ready to be used. Now, when I type thm and then press TAB, thm transforms into the two lines of code that source the specific gist and set the custom theme to theme_ik(). Any ggplot that I will produce next in this R session will have my preferred theme defaults.\n\nswiss |&gt; \n        ggplot(aes(x = Agriculture, y = Fertility))+\n        geom_point()+\n        labs(\n            title = \"Fertility and rurality in Swiss cantones, 1888\"\n        )\n\n\n\n\n\n\n\nMy preferred ggplot2 theme here is optimized to produce plots that look nicely in my blog. 2 Happy coding with snippets and easily re-usable custom functions!\n2 In case you read this anywhere else, this blog post is available at https://ikashnitsky.github.io/2023/gist-snippet"
  },
  {
    "objectID": "2017/dd-journals-frequency/index.html",
    "href": "2017/dd-journals-frequency/index.html",
    "title": "30 issues of Demographic Digest - the most frequent journals",
    "section": "",
    "text": "This week, the 30-th issue of my Demographic Digest was published.\nDemographic Digest is my project that started in November 2015. Twice a month I select fresh demographic papers and write brief summaries of them in Russian to be published in Demoscope Weekly, the most popular Russian journal/website in social sciences. If you read Russian, you may want to browse the archive or visit the website of the project (which is still to be filled).\nThe project is in the transitional phase now. Since 2016 Demographic Digest welcomes contributions from from external authors. In February 2017 I launched the first iteration of a project for the students of National Research University Higher School of Economics.\nTo draw a line after the first phase of the project, I analysed what journals supplied Demographic Digest most frequently. Also, my desire was to try visualizing data with treemaps, which I mentioned in the bonus part1 of the latest digest issue.\n1 I finish each issue of Demographic Digest with a bonus, in which I cover fun papers, discuss some academia related issues, or just provide link to cool visualizations and projects.For that, I exported the bibliographic data of all the papers covered in Demographic Digest. I use Zotero as a reference manager; the paper records are exported as a single .bib file, which I then saved as a plain text (.txt) file. Then I read this data in R, cleaned it, and finally visualized.\n\n# load required packages\nlibrary(tidyverse)\nlibrary(stringr)\nlibrary(readxl)\nlibrary(extrafont)\nmyfont &lt;- \"Roboto Condensed\"\n\ndf &lt;- data.frame(\n    lines = readLines(\"https://ikashnitsky.github.io/share/1702-dd-stats/dd-bib.txt\")\n) |&gt; \n        mutate(lines = lines |&gt; as.character()) |&gt; \n        \n        # grab only the lines that contain journals' titles\n        filter(lines |&gt; str_detect(\"journaltitle\")) |&gt; \n        \n        # remove everything that is not the bare journal's title\n        transmute(\n            journals = lines |&gt; \n                       str_replace_all(\n                            pattern = \"\\tjournaltitle = |\\\\Q{\\\\E|\\\\Q}\\\\E,|\\\\Q}\\\\E\", \n                            replacement = \"\"\n                       )\n        ) |&gt; \n        # calculate frequencies\n        group_by(journals) |&gt; \n        summarise(n = n())\n\nFor one journal title, Ageing and Society, I failed to replace the “&” using regular expressions. This one is to be fixed manually. I also corrected the title of Lancet journal removing the article “The”. Finally, I corrected the frequencies for Population Studies and Population and Development Review subtracting 6, because for both journals I provided lists of most cited papers as a bonus. Following the same logic, I cleaned the data from the papers that appeared in the bonus part.\n\n# correct \"Ageing and Society\"\ndf[1,1] &lt;- \"Ageing and Society\"\n\n# correct the title of Lancet\ndf &lt;- df |&gt; \n    mutate(journals = journals |&gt; str_replace(\"The Lancet\", \"Lancet\")) |&gt;\n    # correct \"Population and Development Review\" and \"Population Studies\" for 6 each\n    # Reason - top cited papers bonus\n    mutate(\n        n = case_when(\n            journals %in% c(\n                \"Population and Development Review\", \n                \"Population Studies\"\n            ) ~ n - 6,\n            TRUE ~ n\n        )\n    )\n\nTo provide some additional metrics of the journals, I downloaded bibliometric data from the SCImago Journal & Country Rank projecthttp://www.scimagojr.com/aboutus.php. Demographic journals usually have rather low SJR, compared to medical journals; that’s why I downloaded the data only for journals in Social Sciences (the.xlsx file). Then I read the data in R and join to my data frame.\n\n# read SJR data for journals in Social Sciences\nsjr &lt;- readxl::read_excel(\n    \"https://ikashnitsky.github.io/share/1702-dd-stats/scimagojr.xlsx\", 1\n) |&gt; \n        mutate(id = Title |&gt; tolower())\n\n# join the data frames; note that I create an \"id\" variable in lower case\ndf_sjr &lt;- left_join(df |&gt; mutate(id = journals |&gt; tolower), sjr, \"id\") \n\nFinally, it’s time to visualize the data. I use the amazing treemap package2.\n2 I also tried portfolio and treemapify, but liked the output from treemap most.\n# Treemap visualization\nlibrary(treemap)\n\ntreemap(dtf = df_sjr, \n        index = \"journals\", \n        vSize = \"n\", \n        vColor = \"SJR\", \n        type = \"value\",\n        n = 5,\n        palette = \"BrBG\", \n        border.col = \"grey10\", \n        title = \"Journals' frequency in Demographic Digest\",\n        title.legend = \"SJR (only social sciences)\",\n        fontfamily.title = myfont,\n        fontfamily.labels = myfont,\n        fontfamily.legend = myfont,\n        drop.unused.levels = T)\n\nHere is how the output looks\n\nNote that the lion’s share of Population Studies is mainly explained by the first issue of Demographic Digest, in which I covered all the papers from the brilliant special issue Population — The long view."
  },
  {
    "objectID": "2018/sjrdata-package/index.html",
    "href": "2018/sjrdata-package/index.html",
    "title": "sjrdata: all SCImago Journal & Country Rank data, ready for R",
    "section": "",
    "text": "SCImago Journal & Country Rank provides valuable estimates of academic journals’ prestige. The data is freely available at the project website and is distributed for deeper analysis in forms of .csv and .xlsx files. I downloaded all the files and pooled them together, ready to be used in R.\nBasically, all the package gives you three easily accessible data frames: sjr_journals (Journal Rank), sjr_countries (Country Rank, year-by-year), and sjr_countries_1996_2017 (Country Rank, all years together).\nThe whole process of data acquisition can be found in the github repo (dev directory) or this gist.\nHow to use sjrdata\n\nInstall the package from github, load it and use the data.\nThe installation will take a while since the main dataset sjr_journals is pretty heavy (15.7MB compressed).\n\n# install\ndevtools::install_github(\"ikashnitsky/sjrdata\")\n\n# load\nlibrary(sjrdata)\n\n# use\nView(sjr_countries)\n\nA couple of examples\nLet’s compare Nature and Science.\n\nlibrary(tidyverse)\nlibrary(sjrdata)\n\nsjr_journals |&gt;\n    filter(title %in% c(\"Nature\", \"Science\")) |&gt;\n    ggplot(aes(cites_doc_2years, sjr, color = title))+\n    geom_path(size = 1, alpha = .5)+\n    geom_label(aes(label = year |&gt; str_sub(3, 4)),\n              size = 3, label.padding = unit(.15, \"line\"))\n\n\nSeveral demographic journals.\n\nsjr_journals |&gt;\n    filter(title %in% c(\n        \"Demography\",\n        \"Population and Development Review\",\n        \"European Journal of Population\",\n        \"Population Studies\",\n        \"Demographic Research\",\n        \"Genus\"\n    )) |&gt;\n    ggplot(aes(cites_doc_2years, sjr, color = title))+\n    geom_point()+\n    stat_ellipse()+\n    scale_color_brewer(palette = \"Dark2\")+\n    coord_cartesian(expand = F)"
  },
  {
    "objectID": "2017/who-is-old/index.html",
    "href": "2017/who-is-old/index.html",
    "title": "Who is old? Visualizing the concept of prospective ageing with animated population pyramids",
    "section": "",
    "text": "This post is about illustrating the concept of prospective ageing, a relatively fresh approach in demography to refine our understanding of population ageing. This visualization was created in collaboration with my colleague Michael Boissonneault: (mostly) his idea and (mostly) my implementation. The animated visualization builds upon Michael’s viz prepared for the submission to the highly anticipated event at the end June 2017 – Rostock Retreat Visualization. My visualization of the provided Swedish dataset can be found in the previous post."
  },
  {
    "objectID": "2017/who-is-old/index.html#prospective-ageing",
    "href": "2017/who-is-old/index.html#prospective-ageing",
    "title": "Who is old? Visualizing the concept of prospective ageing with animated population pyramids",
    "section": "Prospective ageing",
    "text": "Prospective ageing\nOver the past decades the alarmist views of the upcoming population ageing disaster became widely spread. True, with the growing number of countries approaching the ending of the Demographic Transition, the average/median age of their population increases rapidly, which is something unprecedented in the documented human history. But does that imply an unbearable burden of elderly population in the nearest future? Not necessarily.\nThe demographic prospects depend a lot on how we define ageing. Quite recently Waren Sanderson and Sergei Scherbov proposed 1 2 a new way to look at population ageing, they called it Prospective Ageing. The underlying idea is really simple – age is not static: a person aged 65 (the conventional border deliminating elderly population) today is in many aspects not the same as a person ages 65 half a century ago. Health and lifespan improved a lot in the last decades, meaning that today people generally have much more remaining years of life at the moment of being recognized as elderly by the conventional standards. Thus, Sanderson and Scherbov proposed to define elderly population based on the estimation of the expected remaining length of life rather than years lived. Such a refined view of population ageing disqualifies the alarmist claims of the approaching demographic collapse. The would be paradoxical title of one the latest papers of Sanderson and Scherbov 3 summarizes the phenomenon nicely: Faster Increases in Human Life Expectancy Could Lead to Slower Population Aging.\n1 Sanderson W, Scherbov S. 2005. Average remaining lifetimes can increase as human populations age. Nature 435: 811–813 DOI: 10.1038/nature035932 Sanderson W, Scherbov S. 2010. Remeasuring Aging. Science 329: 1287–1288 DOI: 10.1126/science.11936473 Sanderson WC, Scherbov S. 2015. Faster Increases in Human Life Expectancy Could Lead to Slower Population Aging. PLoS ONE 10: e0121922 DOI: 10.1371/journal.pone.01219224 See the working paper of my colleagues devoted to this questionOf course, the choice of the new ageing threshold is a rather arbitrary question 4. It became usual to define this threshold at the remaining life expectancy of 15 years."
  },
  {
    "objectID": "2017/hmd-male-mortality-rus-jpn/index.html",
    "href": "2017/hmd-male-mortality-rus-jpn/index.html",
    "title": "Male mortality in Russia and Japan",
    "section": "",
    "text": "Russia is sadly notorious for its ridiculously high adult male mortality. According to Human Mortality Database data (2010), the probability for a Russian men to survive from 20 to 60 was just 0.64 1. For women the probability is 0.87. This huge gender disproportion in mortality results in a peculiar sex ratio profile (see my old DemoTrends post and the previous blog post).\n1 To compare, the same probabilities for males in some developed countries are: France (0.89), Japan (0.92), US (0.87), UK (0.91).2 See for example the recent NIDI working paper of Balachandran et. al (2017).Now let’s compare age-specific mortality rates of Russian men to that of the Japanese. For years and years Japan performs best in reducing mortality. It became standard to compare mortality schedules of other countries to the Japanese one 2.\nFirst, I need to get HMD data for both Russian and Japanese males. Again, I am using the amazing R package HMDHFDplus of Tim Riffe to download HMD data with just a couple of lines of R code.\n\n# load required packages\nlibrary(tidyverse) # version 1.0.0\nlibrary(HMDHFDplus) # version 1.1.8\n\n# load life tables for men, RUS and JPN\nrus &lt;- readHMDweb('RUS', \"mltper_1x1\", ik_user_hmd, ik_pass_hmd)\njpn &lt;- readHMDweb('JPN', \"mltper_1x1\", ik_user_hmd, ik_pass_hmd)\n\n\n\n\n\n\n\nUse own credentials\n\n\n\nPlease note, the arguments ik_user_hmd and ik_pass_hmd are my login credentials at the website of Human Mortality Database, which are stored locally at my computer. In order to access the data, one needs to create an account at www.mortality.org and provide his own credentials to the readHMDweb() function.\n\n\nNext, I select the most recent year for comparison, 2014, and compute the rate ratio of age specific mortality rates.\n\n# compare mortality rates for 2014\nru &lt;- rus |&gt; filter(Year == 2014) |&gt; transmute(age = Age, rus = mx)\njp &lt;- jpn |&gt; filter(Year == 2014) |&gt; transmute(age = Age, jpn = mx)\ndf &lt;- left_join(jp, ru, 'age') |&gt; mutate(ru_rate = rus / jpn)\n\nFinally, I plot the resulting rate ratio of male mortality in Russia and Japan.\n\n# get nice font\nlibrary(extrafont)\nmyfont &lt;- \"Roboto Condensed\"\n\n# plot\ngg &lt;- ggplot(df, aes(age, ru_rate)) + \n        geom_hline(yintercept = 1, color = 'red') +\n        geom_line(aes(group=1)) + \n        scale_y_continuous('mortality rate ratio',\n                           breaks = 0:10, labels = 0:10, limits = c(0, 10)) +\n        annotate('text',x=c(0, 55), y = c(1.75,5), \n                 color = c('red','black'), hjust = 0, vjust = 1, size = 7,\n                 label = c('Japan','Russia'), family = myfont) +\n        ggtitle('Compare age-specific mortality of males',\n                subtitle = \"Russia and Japan, 2014, HMD\")+\n        theme_bw(base_size = 15, base_family = myfont)\n\n\nIn the middle ages, male mortality in Russian is up to 10 times higher than in Japan!\n\n\n\n\n\n\n\nThis post is based on my earlier tweet and gist"
  },
  {
    "objectID": "cv.html",
    "href": "cv.html",
    "title": "Academic CV",
    "section": "",
    "text": "Download current CV"
  },
  {
    "objectID": "2017/hello-r-world/index.html",
    "href": "2017/hello-r-world/index.html",
    "title": "Hello R world post",
    "section": "",
    "text": "Welcome to my blog!\nMy name is Ilya, I am a demographer. Here I plan to post some research related stuff. As I am crazy about R, my post will touch upon this tool/environment – I will share some tricks and cool visualizations. Eventually, I hope to contribute to the amazing R-bloggers project. First, I am going to post older bits – to gain momentum.\n\nJust to start with, let me show you a small and handy self-written R function.\nQuite often, visualizing data in R, we compose color palettes manually. It is nice to have a function that shows the actual colors of a vector with color values. Here it is.\n\nglimpse_colors &lt;- function(colors_string){\n        n &lt;- length(colors_string)\n        hist(1:n, breaks = 0:n, col = colors_string)\n}\n\nThe function takes a vector of colors as input and produces a basic uniform histogram with one bar for each color, filled accordingly. Let’s try it out. First, we create some colors, and then visualize them.\n\nlibrary(RColorBrewer)\npal &lt;- brewer.pal(n = 9, name = 'BrBG')\nglimpse_colors(pal)\n\n\n\n\n\n\n\nEnjoy!\nP.S. If something similar exists in one of the well known packages, please tell me."
  },
  {
    "objectID": "2017/gender-gap-in-swedish-mortality/index.html",
    "href": "2017/gender-gap-in-swedish-mortality/index.html",
    "title": "Gender gap in Swedish mortality",
    "section": "",
    "text": "Swedish context\nSweden, with its high quality statistical record since 1748, is the natural choice for any demographic study that aims to cover population dynamics during a long period of time.\nData\nThe data used for this visualization comes from Human Mortality Database. It can be easily accessed from an R session using HMDHFDplus package by Tim Riffe (for examples see my previous posts - one and two). For this exercise, I will use the dataset for Sweden that was provided for an application task for Rostock Retreat Visualization1.\n1 By using this data, I agree to the user agreement\nlibrary(tidyverse)\nlibrary(viridis)\nlibrary(extrafont)\n\n# download data\ndf_swe &lt;- read_csv(\"http://www.rostock-retreat.org/files/application2017/SWE.csv\")\n# copy at https://ikashnitsky.github.io/doc/misc/application-rostock-retreat/SWE.csv\n\nyears &lt;- c(1751, 1800, 1850, 1900, 1925, 1950, 1960, 1970, 1980, 1990, 2000, 2010)\n\n# select years and calculate male-to-female arte-ratio of mortality\ndf_selected &lt;- df_swe |&gt; select(Year, Sex, Age, mx) |&gt; \n        filter(Year %in% years) |&gt; \n        spread(Sex, mx) |&gt; \n        transmute(year = Year, age = Age, value = m / f)\n\nVisualization\n\nggplot(df_selected)+\n        geom_hline(yintercept = 1, color = 'grey25', size = .5)+\n        geom_point(aes(age, value), size = 2, pch=1, color = 'grey50')+\n        stat_smooth(aes(age, value, group = 1, color = factor(year)), se = F)+\n        facet_wrap(~year, ncol = 3)+\n        labs(title = \"Male-to-female age-specific mortality rate ratio, Sweden\",\n             subtitle = \"Untill quite recent times, mortality of females was not \n             much lower than that of males\",\n             caption = \"\\nData: Human Mortality Database (https://mortality.org)\n             Note: Colored lines are produced with loess smoothing\",\n             x = \"Age\", y = \"Rate ratio\")+\n        theme_minimal(base_size = 15, base_family = \"Roboto Condensed\") +\n        theme(legend.position = 'none',\n              plot.title = element_text(family = \"Roboto Mono\"))\n\n\nComment\nToday it is common knowledge that male mortality is always higher than female. There are more males being born, then eventually the sex ratio levels due to higher male mortality (see my previous post). Though, male mortality was not always much higher. Back in the days, when infant mortality was much higher and women used to have much higher fertility, there was almost no gender gap in age-specific mortality levels. Constant pregnancy and frequent childbirths had a strong negative impact on female health and survival statistics. We can see that only in the second half of the 20-th century gender gap in mortality became substantial in Sweden.\n\n\n\n\n\n\n\nThis post is based on my earlier tweet and gist"
  },
  {
    "objectID": "2018/the-lancet-paper/index.html",
    "href": "2018/the-lancet-paper/index.html",
    "title": "Regional population structures at a glance",
    "section": "",
    "text": "I am happy to announce that our paper is published today in The Lancet.\n\nKashnitsky I, Schöley J. 2018. Regional population structures at a glance. The Lancet 392: 209–210. https://doi.org/10.1016/S0140-6736(18)31194-2\n\nAt a glance\nDemographic history of a population is imprinted in its age structure. A meaningful representation of regional population age structures can tell numerous demographic stories – at a glance. To produce such a snapshot of regional populations, we use an innovative approach of ternary colour coding.\nHere is the map:\n\nWe let the data speak colours\nWith ternary colour coding, each element of a three-dimensional array of compositional data is represented with a unique colour. The resulting colours show direction and magnitude of deviations from the centrepoint, which represents the average age of the European population, and is dark grey. The hue component of a colour encodes the direction of deviation: yellow indicates an elderly population (&gt;65 years), cyan indicates people of working age (15–64 years), and magenta indicates children (0–14 years).\nThe method is very flexible, and one can easily produce these meaningful colours using our R package tricolore. Just explore the capabilities of the package in a built-in shiny app using the following lines of code:\n\ninstall.packages(\"tricolore\")\nlibrary(tricolore)\nDemoTricolore()\n\n\n\n\n\n\n\n\nReplication materials at github\n\n\n\n\n\n\n\n\n\n\n\n\nFolow us on Twitter: ikashnitsky, jschoeley\n\n\n\n\n\n\n\n\n\n\n\n\nSee also\n\n\n\n\nMy PhD project – Regional demographic convergence in Europe\nBlog post on the first version of the map presented at Rostock Retreat Visualization in June 2017\nPaper (Schöley & Willekens 2017) with the initial ideas for tricolore package\nAn example of ternary colorcoding used to visualize cause-of-death data\nMy other paper , which explores regional differences in population age structures"
  },
  {
    "objectID": "subscribe.html",
    "href": "subscribe.html",
    "title": "Ilya Kashnitsky",
    "section": "",
    "text": "Loading…\n\n\n\n\n\n\n\nNo need to log into your Google account to submit the form"
  },
  {
    "objectID": "2024/zotero7/index.html#general",
    "href": "2024/zotero7/index.html#general",
    "title": "[UPD] Zotero hacks: reliably setup unlimited storage for you personal academic library",
    "section": "General",
    "text": "General\nI only uncheck the option to create automatic web page snapshots which I find not so useful compared with all the cluttering effect of all those multiple small files needed to replicate an html page locally. 5\n5 Also note the added new option to have a dark interface. For years this was one of the main requested features, and it used to be solved via a specialized plugin Zotero Night.\nAnother important option here is File Renaming – to define the rules for renaming the attached PDF files. But we are going to deal with it at the very end, after tuning the Attanger plugin settings."
  },
  {
    "objectID": "2024/zotero7/index.html#sync",
    "href": "2024/zotero7/index.html#sync",
    "title": "[UPD] Zotero hacks: reliably setup unlimited storage for you personal academic library",
    "section": "Sync",
    "text": "Sync\nHere we need to specify the account details to sync our database. It is important to uncheck the option Sync full-text content otherwise the 300MB storage will quickly get filled. We’ll have the solution for full text a bit later."
  },
  {
    "objectID": "2024/zotero7/index.html#export",
    "href": "2024/zotero7/index.html#export",
    "title": "[UPD] Zotero hacks: reliably setup unlimited storage for you personal academic library",
    "section": "Export",
    "text": "Export\nChoose the style for quick export using Shift+Ctrl+C shortcut."
  },
  {
    "objectID": "2024/zotero7/index.html#cite",
    "href": "2024/zotero7/index.html#cite",
    "title": "[UPD] Zotero hacks: reliably setup unlimited storage for you personal academic library",
    "section": "Cite",
    "text": "Cite\nCitation styles stored locally. One nice feature here is the Get additional styles link which brings an integrated selection from the whole Zotero Styles Database. Styles can also be installed from local .csl files, for that press the + button. Don’t miss the Word Processors options at the bottom of this tab. There we can get the plugins that integrate Zotero to Microsoft Word and Libre Office."
  },
  {
    "objectID": "2024/zotero7/index.html#advanced",
    "href": "2024/zotero7/index.html#advanced",
    "title": "[UPD] Zotero hacks: reliably setup unlimited storage for you personal academic library",
    "section": "Advanced",
    "text": "Advanced\nHere we are most interested in the sub-tab Files and Folders. This is the most important step to separate the storage of metadata and files.\n\nThe first path should lead to a directory which stores the full-text PDFs, I call it zotero-library. This directory should be somewhere in the part of the local file system that is synced. In my case it’s the directory named ikx-sync, which I sync with Google Drive. The second path leads to the system files of Zotero, I call it zotero-system. This directory should be placed somewhere in the non-synced part of the local file system. It will be updated by the native Zotero sync, and it’s better if those system files are not overwritten by any external sync software."
  },
  {
    "objectID": "2024/zotero7/index.html#attanger",
    "href": "2024/zotero7/index.html#attanger",
    "title": "[UPD] Zotero hacks: reliably setup unlimited storage for you personal academic library",
    "section": "Attanger",
    "text": "Attanger\nNext we need to setup Zotero Attanger. This extension helps to rename PDFs according to pre-defined rules and store them in a hierarchical database with meaningful names of the sub-directories. There are 4 important steps here (annotated in the screenshot below).\nWe need to define two paths. The first is the default location of the files downloaded by your browser. This option tells Attanger where to look for recently downloaded PDFs to process when you import a paper from the publisher’s website (recall that earlier we installed Zotero Connector). The second path leads to the local directory created for the full-text PDFs, the one that I named zotero-library and which is synced with an external cloud solution of our choice.\n\nTo navigate easier in this database of PDFs use the option Subfolder. Here again we have a wide choice of the ways to define the rules to name the sub-directories. Click the documentation link to learn the naming options. I choose to simply have a separate folder for each first author. In my case the option is {{ authors max=\"1\" case=\"lower\" }}. The advantage of this convention is that I only reed to remember the first author – and I can easily find all the PDFs in the same folder, synced securely to my Google Drive, I can easily find any paper in seconds\n\n\n…and even from my phone on the go.\n\nThe Filename option defines the rules for renaming the attached PDF files. Clicking the button Customize Filename Format brings us to the Filename Format sub-menu of the General Settings tab. Here we set the template according to the same Zotero naming rules, click documentation link to learn the naming options.\n\nAgain, the options are nearly infinite. My choice is to have attached PDFs named according to their FIRST AUTHOR then YEAR then TITLE, everything in snake_case, which yields file names like kashnitsky_2021_unequally_ageing_regions_of_europe.pdf (again an example for my paper in Population Studies). The code to get such name is: {{ authors max=\"1\" suffix=\"_\" case=\"snake\" }}{{ year suffix=\"_\" }}{{ title truncate=\"100\" case=\"snake\"}}.\nAnd here comes the kill feature of the proper setup – paths to the full texts are stored in the metadata itself. This allows seamless transition between the machines (or even a simultaneous usage across several machines) as long as the relative file paths structure starting from the base file directory zotero-library stays unchanged.\n\nWhen I need to restore my whole database of academic papers on another machine, I just go through the steps in this tutorial. As long as the data base of system metadata (zotero-system) is synced by Zotero and a correct link to a PDFs storage (zotero-library) is specified, Zotero will recognize all the relative paths to the files, and will restore the whole library. This setup also makes it possible to have the same continuously synced library on multiple machines."
  },
  {
    "objectID": "2024/zotero7/index.html#better-bibtex",
    "href": "2024/zotero7/index.html#better-bibtex",
    "title": "[UPD] Zotero hacks: reliably setup unlimited storage for you personal academic library",
    "section": "Better BibTeX",
    "text": "Better BibTeX\nThis tab appears after we install the Better BibTeX extension. The plugin is needed to export the bibliographic library, in whole or some specific collections (aka folders), as plain .bib text files, which are later used while writing academic papers with LaTeX, rmarkdown, Quarto, or any other text based editor. `\n\nThe most important option here is to define the rules for creating citation keys. There are almost infinite number of ways one can define these keys (check the manual). My personal choice is [auth:lower][year][journal:lower:abbr], which means that a key consists of the first author’s name, publication year, and the first letters abbreviation of the journal’s title, everything in lower case. For example, the key for one of my papers published in Population Studies is kashnitsky2021ps.\nOnce Better BibTeX is installed and fine-tuned, exporting to .bib is easily achieved via the context menu – right click on a collection (or the whole library, but this would be slow and take a long whole) and choose Export Collection.\n\nIn the next small window choose the parameters of the export. There are multiple formats to choose from, yet for our purposes the default Better BibTeX is prefect. Then there are several other options to choose.\nKeep Updated would monitor the exported Zotero collection and will update/overwrite the exported .bib file whenever it sees any changes. This is very handy, but may be quite a burden for you machine, use with caution.\n\nExport Files allows to export PDFs together with the metadata. This may come super handy if you want to quickly share a collection of articles with full texts. If this option is checked, the output will be a folder with a .bib text file with the papers’ metadata and an inset folder files with the actual PDFs.\n\nThe nice feature of Better BibTeX is that it keeps the relative paths to PDFs written down in the metadata itself. This allows importing the whole collection back to Zotero (or another bibliographic manager) with full texts carried along.\n\nBetter BibTeX takes care of citation keys, the option that we tuned first in this section. Citation keys are unique identifiers of papers (or other metadata entries) in the .bib collection. We use the later in Quarto (or LaTeX, rmarkdowm, etc) to actually cite papers. For further hints on using .bib files in authoring academic texts, I suggest checking out the beautiful reference materials of Quarto."
  },
  {
    "objectID": "2022/exceptional-world-cup/index.html",
    "href": "2022/exceptional-world-cup/index.html",
    "title": "Were there too many unlikely results at the FIFA World Cup 2022 in Qatar?",
    "section": "",
    "text": "FIFA World Cup 2022 in Qatar saw many surprising results. In fact, too many – some would argue. From the unbelievable loss of Argentina to Saudi Arabia at the very beginning of the group stage, via the loss of the magnificent Brazil to Cameroon at the end of the group stage, to the groundbreaking performance of Morocco who were competitive playing against all the usual grands.\nSomewhere towards the middle of the group stage I started wondering – is it ordinary to have so many surprises at the World Cup? What if this particular World Cup is really exceptional by the number of unlikely match outcomes? 1 If so, how should I measure it?\n1 Through discussions of my earlier postings of this analysis I figured out that the usual term to describe these unlikely outcomes is “upset”. I don’t like the term since it seems to suggest inherently that one always supports the favorite, which is definitely no my case, I tend to cheer for the underdogs. The term I used initially was “sensations”, which apparently makes little sense in English and reveals the direct translation from Russian happening in my head in this case. Throughout this post I’ll use the term “unlikely outcome”.2 For now I only scraped and added several football tournaments and leagues. Please, feel free to add more results via GitHub pull requests.Just a bit of thinking yielded an answer that was really on the surface – bookmakers. They are the people who use all available knowledge to make money on the outcome expectations. Pretty soon I figured out that there is one website [oddsportal.com][oddsportal] that offers long historical data series of betting odds and match outcomes. The real challenge came at the step of scraping the website. Stack Overflow and other similar places are filled with questions about scraping this specific tricky website. Having finally figured out how to do this (after countless trials and failures) I wrapped up the working solution into an R package oddor. The idea is that the package provides both scraping tools and the cleaned extracted datasets. 2\nOkay, so the data issue is solved. Now, it’s time for the experiment, a really simple one. I simulate the scenario where I consistently bet on the least likely outcome and track how my fictional balance changes over time. Out of the three possible outcomes of the games – (1) home team wins, (2) draw in main time, and (3) away team wins 3 – I always select the one that promises the highest odds, meaning that this outcome is considered the least likely of the three by the bookmakers.\n3 Of course, “home” and “away” designation is very arbitrary at world cups, still I use the terms for consistency.Here are the results for the latest World Cup 2022 (darkest line with all the unlikely outcomes annotated) in comparison with three previous World Cups, 2018, 2014, and 2010.\n\nDecreasing step lines in the plot represent my decreasing fictional balance. Each game I bet 1 coin on the least likely outcome. Most often I lose this bet, and my balance decreases. Though, sometimes the least likely outcome happens, then my balance increases substantially by the size of the unlikely outcome odds. For example, in case of Argentina losing to Saudi Arabia the odds for this outcome was 25.\nWe can see that the 2022 World Cup was really exceptional – too often the outcomes that were considered the least likely happened. It’s also evident that surprises happen more often at the group stage, especially in the third round when many leaders apparently have already reached their group-stage goals (think of the recent game Brazil–Cameroon, where Brazilians literally played with the second team). I would say it’s really surprising – if one bets consistently against the odds at all the group-stage games, at least in the last 4 World Cups (for which we have odds and outcomes data) this dead-simple strategy turns out to be beneficial. My wild guess is that the World Cups see masses of inexperienced new betters who are placing bets on their national teams whatever, which at the global scale is disbalancing the whole system. Alternatively, maybe we are just slow and bad at recognizing how football is becoming more international, and now more underdogs are able to give a decent fight to the traditional grands.\nIn contrast, Play-offs are apparently less chaotic and more predictable. Betting on the underdogs at the Play-offs stage would guarantee to lose money in all 4 recent World Cups.\nOf course, the surprising beneficial result of the betting-on-the-underdog experiment at the group-stage games at World Cups made me curious about other competitions. So I did a similar analysis for the Champions League (from season 2004–05) and for English Premier League (from season 2003–04). Predictably, no miracle happened – betting consistently on the underdogs would almost always bring financial losses. So, either World Cups are just different, or all 4 last tournaments were exceptional with the latest being a crazy outlier. Would it be reasonable to try this no-brainer betting strategy at the group-stage of the next World Cup? I’m not sure. But let’s see in 4 years. 4\n4 I guess I have to say that this is not a financial advise =)\n\n\n\n\n\n\nReplicate this analysis using the R code from this gist"
  },
  {
    "objectID": "2019/zotero/index.html",
    "href": "2019/zotero/index.html",
    "title": "Zotero hacks: unlimited synced storage and its smooth use with rmarkdown",
    "section": "",
    "text": "About this tutorial\n\n\n\nHere is a bit refreshed translation of my 2015 blog post, initially published on Russian blog platform habr.com. The post shows how to organize a personal academic library of unlimited size for free. This is a funny case of a self written manual which I came back to multiple times myself and many many more times referred my friends to it, even non-Russian speakers who had to use Google Translator and infer the rest from screenshots. Finally, I decided to translate it adding some basic information on how to use Zotero with rmarkdown.\n\n\n\n\nA brief (and hopefully unnecessary for you) intro of bibliographic managers\nBibliographic manager is a life saver in everyday academic life. I suffer almost physical pain just thinking about colleagues who for some reason never started using one – all those excel spreadsheets with favorite citations, messy folders with PDFs, constant hours lost for the joy-killing task of manual reference list formatting. Once you start using a reference manager this all becomes a happily forgotten nightmare.\nI tend to think of bibliographic metadata as LEGO.\n\nFor each paper (book chapter / pre-print / R package) we have a number of metadata pieces – title, authors, date published, etc. These are the LEGO blocks. For different bibliographic styles we just need to re-shuffle those blocks inserting various commas, semicolons, and quotation marks.\nBibliographic manager keeps track of all the LEGO blocks and knows (learns easily) how to compose proper citation styles out of them. All we need is to download a specific journal’s citation style. There are more than six thousand bibliographic styles! [This is my #1 argument against the conspiracy ideas of some centralized power that rules our world .)]\n\n\nWhy Zotero?\nThere are dozens of bibliographic managers out there (see a comparative table). Some of them are free, the others require paid subscriptions. Probably, the most popular two are Zotero and Mendeley. Both are free to use and make money by offering cloud storage to sync PDFs of the papers. Yet, both give some limited storage for free – Zotero gives 300MB, and Mendeley gives 2GB.\nWhy do I choose and recommend Zotero then? Because it’s fairly easy to set-up Zotero so that the free 300MB are only used to sync metadata (which in practice means almost infinite storage), and the PDFs are synced separately using a cloud solution of one’s choice (I use Google Drive). It’s the main set-up hack that I’m showing in this blog post. There is no similar hack for Mendeley, and with them at some point one is bound to pay for extra storage.\nAnother consideration in favor of Zotero is that it’s an open-source program with strong community and outspoken commitment to stay free forever, while Mendeley is an Elsevier for-profit product. Academic community knows a lot about Elsevier in particular and for-profit products in general. Here the story of Academia.edu is very indicative. Have a look at this Forbes piece. As a career-long decision I’m confident in choosing Zotero. And the project keeps developing nicely – just look at the recent Zotero blog entries on the new features such as Google Docs integration, Unpaywall integration and a new web service for quick citations.\nFinally, an example of how strong Zotero community is. Once I figured out there the style repository does not have a style for Demographic Research, one of the best journals in demography. I’ve opened a request on Zotero forum and in two days the style was created.\n\n\nPrerequisites\n\nDownload and install Zotero. It’s cross-platform and works smoothly with various systems, even when the same database is sycned in parallel on machines with different operation systems. I’ve used win+linux and win+mac – no sync problems ever.\nFrom the same download page go to install Zotero Connector, a browser extension that helps to retrieve bibliographic metadata.\nCreate an account on Zotero website. It will be used later on to sync the database of bibliographic metadata.\nDownload and install the two plugins we’ll need – ZotFile (organizes the database of PDFs) and Better BibTeX (exports the library to .bib, we’ll use it later with rmarkdown). The plugins for Zotero are .xpi archives. To install the plugins open Zotero and click Tools --&gt; Add-ons. A separate window for Add-ons manager will pop-up.\n\n\nThere we need to click the options gear button and select Install Add-on From File option. Finally navigate to the .xpi file and install. Zotero will ask to restart, please do.\nWe are ready to go through the setup step-by-step.\n\n\nZotero preferences\nFirst, let’s walk though Zotero Preferences. To edit them click Edit --&gt; Preferences. A window with several tabs pops up.\nGeneral. I only uncheck the option to create automatic web page snapshots which I find not so useful compared with all the cluttering effect of all those multiple small files needed to replicate an html page locally.\n\nSync. Here we need to specify the account details to sync our database. It is important to uncheck the option of full-text sync otherwise the 300MB storage will quickly get filled. We’ll have the solution for full text a bit later.\n\nSearch. Defines the database for internal search engine. Defaults are reasonable.\nExport. Choose the style for quick export using Shift+Ctrl+C shortcut.\nCite. Citation styles stored locally. One nice feature here is the Get additional styles link which brings an integrated selection from the whole Zotero Styles Database. Styles can also be installed from local .csl files, for that press the + button. Don’t miss the Word Processors sub-tab. There we can get the plugins that integrate Zotero to Microsoft Word and Libre Office.\n\nAdvanced. Here we are most interested in the sub-tab Files and Folders. This is the most important step to separate the storage of metadata and files.\n\nThe first path should lead to a directory which stores the full-text PDFs, I call it zotero-library. This directory should be somewhere in the part of the local file system that is synced. In my case it’s the directory named ikashnitsky, which I sync with Google Drive. The second path leads to the system files of Zotero, I call it zotero-system. This directory should be placed somewhere in the non-synced part of the local file system. It will be updated by the native Zotero sync, and it’s better if those system files are not overwritten by any external sync software.\nBetter BibTeX. This tab appears after we install the Better BibTeX extension. The extension is needed to export the whole bibliographic library (or some of its parts) as a plain .bib text file. This step is needed to use Zotero in RStudio while writing academic papers with rmarkdown.\n\nThe most important option here is to define the rules for creating citation keys. There are almost infinite number of ways one can define these keys (check the manual). My personal choice is [auth:lower][year][journal:lower:abbr], which means that a key consists of the first author’s name, publication year, and the first letters abbreviation of the journal’s title, everything in lower case. Thus the key for my most recent paper published in Tijdschrift voor economische en sociale geografie is kashnitsky2019tveesg.\n\n\nZotFile Preferences\nNext we need to setup ZotFile. This extension helps to rename PDFs according to pre-defined rules and store them in a hierarchical database with meaningful names of the sub-directories. To open the setup window click Tools --&gt; ZotFile Preferences. Again, the window has several tabs.\nGeneral. Here we define two paths. The first is the default location of the files downloaded by your browser. This option tells ZotFile where to look for the PDFs to process when you import a paper from the publisher’s website (recall that earlier we installed Zotero Connector). The second path leads to the local directory created for the full-text PDFs, the one that I named zotero-library and which is synced with an external cloud solution of our choice.\n\nTo navigate easier in this database of PDFs check the option Use subfolder defined by. Here again we have a wide choice of the ways to define the rules to name the sub-directories. Click the info icon to learn the options. I choose to simply have a separate folder for each first author.\nTablet Settings. Apparently, this menu allows to setup an exchange of PDFs with a tablet. I’ve never used it, thus omit.\nRenaming Rules. Here it’s important to make sure that ZotFile is responsible for renaming. Then we define how to rename the PDFs based on the bibliographic metadata available. Again, here we have many many options. My choice is {% raw %}{%a_}{%y_}{%t}{% endraw %} which yields file names like kashnitsky_2018_russian_periphery_is_dying_in_movement.pdf (again an example for my recent paper in GeoJournal).\n\nAdvanced Settings. I only checked the option to replace all the non-standard symbols with plain ASCII.\nA very important note on ZotFile!.\nIf you parse the metadata manually from a PDF, make sure to rename the file using ZotFile. For that right-click the metadata record Manage Attachments --&gt; Rename Attachments. This action explicitly tells to use ZotFile for renaming and will move the renamed PDF to a proper sub-directory. The attachment in Zotero should not look like a PDF file…\n\n… but rather should be a link to the renamed file.\n\nIn these screenshot I also show the location of the actual PDFs in both cases (right-click the metadata record Show File). As you can see, in the first case the PDF is located in a meaninglessly named folder somewhere in the zotero-system directory. In contrast, the renamed by ZotFile PDF is located in a properly named sub-directory in zotero-library. Thus, in the latter case the PDF is synced to my Google Drive and can be accessed from anywhere.\n\nMore importantly, when I need to restore my whole database of academic papers on another machine, I just need to go through these steps. As long as the system metadata data base is synced by Zotero and I provide Zotero the link to a PDFs storage, it will recognize all the relative paths to the files, and the whole library is restored. This setup also makes it possible to have the same continuously synced library on multiple machines. The hack is in ZotFile which adds a file path line to the metadata of the papers.\n\nAs long as I keep the settings unchanged, everything will be synced fine across multiple devices. In the end, I enjoy the unlimited storage of my PDFs with the very nice and reliable native sync of metadata form Zotero.\nFinal remark on Zotero. Feel free to clean from time to time all the clutter from zotero-system/storage.\n\n\nUse Zotero library in RStudio with rmarkdown\nZotero has a very nice built-in integration with Microsoft Word and Libre Office. A bit of magic is needed if one wants to use it with LaTeX or (like me) with rmarkdown. The magic part is the Better BibTeX plugin, which we’ve installed and set up earlier.\nBetter BibTeX offers an easy way to export bibliographic records from Zotero as plain .bib text and keep the file updated once the records are changed. Just right-click on the collection in Zotero and choose Export Collection.\n\nThen in the next window choose to export as Better BibTeX and check the option to Keep updated.\n\nThe output .bib file should be placed in the directory from which we are going to knit the .rmd file. The name of the .bib is specified in YAML header of the .rmd. Here is an example from my running project with jmaburto.\n\nNote that the exact YAML functions may vary depending on the rmarkdown template package. In this case I’m using bookdown, which also allows to specify the desired bibliographic style, .csl file should also be copied to the knit directory.\nThen, everything is ready to use the citation keys to generate citations throughout the text. For details on rmarkdown citation syntax, it’s better to refer to RStudio’s manual (see below) or the relevant chapter of xieyihui’s book on bookdown.\n\n\nThe final hint here is to use citr package, which brings an easy and interactive way to select citations from the .bib file. Once the package in installed, an RStudio addin Insert citation appears which executes the citr:::insert_citation() command (you can assign a short-key to the addin). This function brings a shiny app to select a citation interactively. More details in the github repo.\n\n\n\n\n\n\nHappy paper writing with Zotero and RStudio!"
  },
  {
    "objectID": "2019/barcelona-summer-school-of-demography/index.html",
    "href": "2019/barcelona-summer-school-of-demography/index.html",
    "title": "See you in Barcelona this summer",
    "section": "",
    "text": "Have you been feeling lately that you are missing out the coolest skill-set in academia?\n\n\nHere is you chance to cut in and dive into R.\nIn July BaRacelona Summer School of Demography welcomes dedicated scholars, aspiring or established, to help them migrate to the world of new oppoRtunities.\nThe school consists of 4 modules. You can take them all or choose specific ones. The first, instructed by Tim Riffe, introduces the basics of R. The second, instructed by myself, focuses on visualizing data, very general with a slight tilt towards population data. The third, instructed by Marie-Pier Bergeron Boucher, presents the foundations of demographic analysis with R. Even if you are not (yet) a demographer these methods are very general and are usable in a wide range of social science disciplines. Finally, the fourth course, instructed by Juan Galeano, teaches the powerful ways to unleash the spatial dimension of data analysis.\nThere are still several places available, the call closes on March 31st.\nI’ll be happy to see you in sunny Catalonia!"
  },
  {
    "objectID": "2018/deep-catalan-roots/index.html#here-is-the-table-for-guys",
    "href": "2018/deep-catalan-roots/index.html#here-is-the-table-for-guys",
    "title": "Deep Catalan roots: playing with stringdist",
    "section": "Here is the table for guys:",
    "text": "Here is the table for guys:\n\nMy personal favorite here is Hanbo Wo becoming Antonio Duc."
  },
  {
    "objectID": "2018/deep-catalan-roots/index.html#and-a-similar-table-for-girls",
    "href": "2018/deep-catalan-roots/index.html#and-a-similar-table-for-girls",
    "title": "Deep Catalan roots: playing with stringdist",
    "section": "And a similar table for girls:",
    "text": "And a similar table for girls:\n\nHere I like Elena Bastianelli turning to Elena Albanell."
  },
  {
    "objectID": "2025/my-path-reflection/llm-jumpstarters.html",
    "href": "2025/my-path-reflection/llm-jumpstarters.html",
    "title": "",
    "section": "",
    "text": "Code"
  },
  {
    "objectID": "2025/my-path-reflection/llm-jumpstarters.html#my-already-not-just-academic-path-reflection",
    "href": "2025/my-path-reflection/llm-jumpstarters.html#my-already-not-just-academic-path-reflection",
    "title": "",
    "section": "My (Already Not Just) Academic Path Reflection",
    "text": "My (Already Not Just) Academic Path Reflection\nAs I look back on my academic journey, I am struck by how far I’ve come and how much I’ve learned along the way. What began as a straightforward academic path has evolved into something far more diverse and multifaceted. This blog post is a reflection on my experiences, the challenges I’ve faced, and the lessons I’ve learned. It’s also a declaration of readiness to take on a new role: that of a scientific mentor.\n\nA Very Successful Academic Path\nBy many measures, my academic career has been a success. I’ve earned a PhD in Demography, published in reputable journals, and presented at international conferences. My work has taken me across Europe and beyond, allowing me to collaborate with leading researchers in the field. I’ve secured funding, won awards, and built a strong professional network. On paper, it looks like a classic academic success story. Yet, as I reflect on this path, I realize that success in academia is not just about ticking boxes or climbing the ladder. It’s about finding meaning in the work you do and making a tangible impact—something I’ve been striving to achieve in my own way.\n\n\nThe Lack of a Clear Research Topic\nOne of the challenges I’ve faced is the absence of a single, well-defined research topic that I can call my own. While I’ve worked on a variety of projects—ranging from mortality and aging to migration and spatial demography—I’ve often felt like a jack-of-all-trades rather than a specialist. This lack of focus has sometimes made it difficult to position myself within the academic landscape. However, it has also allowed me to explore diverse areas of demography, gaining a broad perspective that I wouldn’t have had otherwise. I’ve come to see this not as a weakness, but as a strength that enables me to connect ideas across disciplines and contribute to interdisciplinary research.\n\n\nDiverse Experiences: Research, Teaching, Outreach, and More\nMy career has been anything but linear. I’ve conducted research in multiple fields of demography, worked in several countries, and collaborated with diverse research teams. Beyond research, I’ve taught in various contexts, from university courses to workshops, and developed an R package to make demographic methods more accessible. I’ve also engaged in science communication, writing blog posts, interacting with journalists, and maintaining an active presence on social media. These experiences have taught me the value of versatility and adaptability. They’ve also shown me that academia is not just about publishing papers—it’s about sharing knowledge, inspiring others, and contributing to the broader scientific community.\n\n\nMentoring Early Career Researchers\nOne of the most rewarding aspects of my career has been mentoring early career researchers. Over the years, I’ve guided countless students and young professionals, helping them navigate the complexities of academia, develop their skills, and build their confidence. This role has given me a sense of purpose and fulfillment that goes beyond my own research achievements. It’s also made me realize how much I enjoy supporting others in their academic journeys. While I’ve always been happy to offer advice and encouragement, I’ve recently come to see mentoring as a more formal and integral part of my professional identity.\n\n\nThe Lack of Formal Scientific Mentoring\nDespite my success in mentoring others, I’ve often felt the absence of formal scientific mentoring in my own career. While I’ve had supportive colleagues and collaborators, I’ve never had a dedicated mentor who could provide consistent guidance and feedback. This has sometimes left me feeling isolated and uncertain about my direction. However, it has also pushed me to take ownership of my career and seek out opportunities for growth on my own. In a way, this lack of formal mentoring has made me more self-reliant and resourceful, qualities that I now bring to my own mentoring relationships.\n\n\nA Recent Mentoring Experience: Giving Feedback to Sebastian\nA recent experience solidified my belief in the importance of mentoring. I had the opportunity to give feedback to Sebastian, a young researcher who was struggling with a project. As we discussed his work, I realized how much I’ve learned over the years—not just about demography, but about the process of doing research, communicating ideas, and overcoming challenges. Sharing this knowledge with Sebastian was incredibly rewarding, and it reminded me of the impact that a good mentor can have. It also made me realize that I’m ready to take on a more formal mentoring role, not just as an occasional advisor, but as a dedicated guide for early career researchers.\n\n\nI’m Ready to Be a Scientific Mentor Now!\nAfter years of diverse experiences, both within and beyond academia, I feel ready to embrace the role of a scientific mentor. I want to help others navigate the challenges I’ve faced, share the lessons I’ve learned, and support them in achieving their goals. Mentoring is not just about imparting knowledge—it’s about building relationships, fostering growth, and creating a sense of community. I’m excited to take on this role and contribute to the development of the next generation of researchers. If my journey has taught me anything, it’s that success in academia is not just about individual achievements, but about the connections we make and the impact we have on others.\nAs I move forward, I look forward to continuing my research, teaching, and outreach efforts, but also to dedicating more time and energy to mentoring. My academic path may have started as a straightforward pursuit of knowledge, but it has evolved into something much richer and more meaningful. I’m grateful for the opportunities I’ve had and excited for what’s to come. Here’s to the next chapter—and to helping others write their own stories."
  },
  {
    "objectID": "2025/my-path-reflection/llm-jumpstarters.html#my-already-not-just-academic-path-reflection-1",
    "href": "2025/my-path-reflection/llm-jumpstarters.html#my-already-not-just-academic-path-reflection-1",
    "title": "",
    "section": "My (Already Not Just) Academic Path Reflection",
    "text": "My (Already Not Just) Academic Path Reflection\nThe Traditional Success Story\nMy academic journey has followed what many would consider a textbook path to success. I’ve published in respected journals, secured competitive funding, and established myself within my field. The metrics that academia values—citations, impact factors, and institutional affiliations—have all aligned favorably with my career trajectory. From the outside looking in, the conventional checkboxes of academic achievement have been ticked off one by one.\nSearching for My North Star\nDespite these external markers of success, I’ve often found myself wrestling with a fundamental question: what is my research topic? While colleagues built careers around specific niches, methodologies, or theoretical frameworks, my path has been less singularly focused. This absence of a clear research identity has been both a source of insecurity and, paradoxically, a catalyst for the diverse path I’ve forged.\nThe Power of Academic Versatility\nWhat my career may lack in singular focus, it makes up for in breadth and adaptability. My research has spanned multiple subfields of demography, taking me across continents and introducing me to diverse research teams with varying approaches and priorities. I’ve taught everything from large introductory lectures to specialized graduate seminars, each requiring different pedagogical approaches. Beyond traditional academic outputs, I’ve developed R packages that serve the wider community, maintained an active blog and social media presence, and cultivated relationships with journalists to communicate research to broader audiences.\nThe Accidental Mentor\nPerhaps the most unexpected development in my career has been the gravitational pull toward mentoring early career researchers. Without consciously seeking this role, I’ve found myself repeatedly providing guidance on everything from statistical methods to work-life balance, from publication strategies to career decisions. These conversations have become not just a peripheral activity but a central and deeply rewarding aspect of my professional identity.\nThe Missing Piece\nDespite my comfort in advising others, I’ve navigated much of my own path without formal scientific mentorship. While administrative mentors have helped me understand promotion processes and institutional politics, I’ve largely lacked someone to help shape my research agenda, challenge my methodological choices, or push me toward more ambitious questions. This absence has fostered independence but also left me wondering what different paths might have opened with more guided scientific development.\nLearning to Guide by Being Guided\nA recent experience providing feedback to Sebastian revealed something significant: the act of critically engaging with another’s research—identifying strengths, questioning assumptions, suggesting alternative approaches—crystallized my own scientific perspective in ways I hadn’t previously recognized. The process of articulating what makes for rigorous, innovative, and meaningful research helped me realize that I’ve developed a coherent scientific viewpoint despite my diverse path.\nThe Next Chapter: Embracing the Scientific Mentor Role\nThis moment of clarity has brought me to an important realization: I’m ready to step fully into the role of scientific mentor. The breadth of my experience, combined with the perspective gained from working across different research contexts, has equipped me to guide others not just in navigating academic careers but in shaping research agendas and advancing scientific understanding. What once felt like a liability—my non-linear research path—now reveals itself as a unique asset in helping others find their own way through the complex terrain of academic science."
  },
  {
    "objectID": "2025/my-path-reflection/llm-jumpstarters.html#my-already-not-just-academic-path-reflection-2",
    "href": "2025/my-path-reflection/llm-jumpstarters.html#my-already-not-just-academic-path-reflection-2",
    "title": "",
    "section": "My (Already Not Just) Academic Path Reflection",
    "text": "My (Already Not Just) Academic Path Reflection\nLooking back on my journey, my academic path has been filled with achievements, opportunities, and invaluable experiences that extend far beyond traditional research. What began as a structured pursuit of demography has evolved into a dynamic career spanning research, teaching, outreach, and mentoring. This reflection explores my trajectory—its successes, challenges, and the point I’ve now reached: I am ready to be a scientific mentor."
  },
  {
    "objectID": "2025/my-path-reflection/llm-jumpstarters.html#a-successful-academic-path",
    "href": "2025/my-path-reflection/llm-jumpstarters.html#a-successful-academic-path",
    "title": "",
    "section": "A Successful Academic Path",
    "text": "A Successful Academic Path\nMy academic career has been marked by significant milestones. After completing a BSc in Geography at Moscow State University and an MSc in Demography at the Higher School of Economics in Moscow, I earned a PhD in Demography from the University of Groningen and the Netherlands Interdisciplinary Demographic Institute in 2020. My dissertation, Changing Regional Inequalities in Ageing Across Europe, examined demographic disparities across the continent.\nFollowing my PhD, I navigated through various research positions, from Research Assistant to Assistant Professor at the Interdisciplinary Centre on Population Dynamics, University of Southern Denmark. Now, as a Senior Researcher (Specialkonsulent) at Statistics Denmark, I continue to contribute to the field, specializing in demographic research and data analysis. Additionally, I am an Associate Member of the Leverhulme Centre for Demographic Science at the University of Oxford and a board member of the Danish Demographic Association.\nBeyond research, my work has received recognition, including the EAPS Outreach Award in 2018 and selection as a Leading Population Expert by Population Europe in 2022."
  },
  {
    "objectID": "2025/my-path-reflection/llm-jumpstarters.html#the-challenge-no-singular-research-topic",
    "href": "2025/my-path-reflection/llm-jumpstarters.html#the-challenge-no-singular-research-topic",
    "title": "",
    "section": "The Challenge: No Singular Research Topic",
    "text": "The Challenge: No Singular Research Topic\nDespite my accomplishments, one aspect of my academic career has always left me unsettled: I have never had a single, well-defined research focus. My interests span various areas within demography, including spatial analysis, population aging, internal migration, and COVID-19’s demographic impact.\nWhile many researchers develop a signature topic early in their careers, I have instead followed a broad and interdisciplinary path. I have contributed to high-impact publications in Nature Human Behaviour, BMJ Open, and Population Studies, collaborated on projects in diverse domains, and even developed R packages like sjrdata and tricolore. While this diversity has enriched my experience, it has also made me feel like an academic generalist rather than a specialist."
  },
  {
    "objectID": "2025/my-path-reflection/llm-jumpstarters.html#a-multifaceted-career-research-teaching-and-outreach",
    "href": "2025/my-path-reflection/llm-jumpstarters.html#a-multifaceted-career-research-teaching-and-outreach",
    "title": "",
    "section": "A Multifaceted Career: Research, Teaching, and Outreach",
    "text": "A Multifaceted Career: Research, Teaching, and Outreach\nAlthough my research spans different topics, my career has been anything but narrow. I have worked in multiple countries (Russia, the Netherlands, Denmark, Germany, and the UK) and collaborated with international research teams. Teaching has been another major pillar of my work, with courses and workshops on data visualization, R programming, and demographic methods at institutions such as the University of Oxford, the Max Planck Institute for Demographic Research, and Universitat Autònoma de Barcelona.\nOutreach has also been a passion of mine. I have actively communicated demographic insights to wider audiences, engaging with journalists and contributing to discussions on platforms like Our World in Data, The Guardian, and The Economist. My blog and social media presence have allowed me to share research, tutorials, and reflections on academia."
  },
  {
    "objectID": "2025/my-path-reflection/llm-jumpstarters.html#mentoring-early-career-researchers-1",
    "href": "2025/my-path-reflection/llm-jumpstarters.html#mentoring-early-career-researchers-1",
    "title": "",
    "section": "Mentoring Early-Career Researchers",
    "text": "Mentoring Early-Career Researchers\nThroughout my career, I have been deeply involved in mentoring early-career researchers. I have provided career advice, guidance on academic writing, and insights into navigating academia’s challenges. Having gone through the ups and downs of research, funding, and publishing, I find it incredibly fulfilling to help others avoid pitfalls and develop their careers strategically."
  },
  {
    "objectID": "2025/my-path-reflection/llm-jumpstarters.html#the-absence-of-formal-mentorship",
    "href": "2025/my-path-reflection/llm-jumpstarters.html#the-absence-of-formal-mentorship",
    "title": "",
    "section": "The Absence of Formal Mentorship",
    "text": "The Absence of Formal Mentorship\nInterestingly, while I have mentored others, I have never had a formal scientific mentor myself. My academic journey has been largely self-directed, shaped by interactions with peers and collaborators rather than structured guidance. This has given me independence but also reinforced the importance of mentorship—something I lacked and now want to provide to others."
  },
  {
    "objectID": "2025/my-path-reflection/llm-jumpstarters.html#a-recent-mentorship-experience",
    "href": "2025/my-path-reflection/llm-jumpstarters.html#a-recent-mentorship-experience",
    "title": "",
    "section": "A Recent Mentorship Experience",
    "text": "A Recent Mentorship Experience\nA recent experience that reinforced this realization was providing feedback to Sebastian Hanika, a PhD candidate at Lund University. Reviewing his work and discussing his research made me recognize how much I enjoy guiding and shaping the ideas of emerging scholars. It was a moment of clarity: I am ready to take on a formal mentorship role."
  },
  {
    "objectID": "2025/my-path-reflection/llm-jumpstarters.html#i-am-ready-to-be-a-scientific-mentor",
    "href": "2025/my-path-reflection/llm-jumpstarters.html#i-am-ready-to-be-a-scientific-mentor",
    "title": "",
    "section": "I Am Ready to Be a Scientific Mentor",
    "text": "I Am Ready to Be a Scientific Mentor\nReflecting on my journey, I see that my broad expertise, international experience, and commitment to outreach and mentoring have prepared me for this next step. I am eager to support young researchers, help them navigate academia, and create an environment where they can thrive.\nAcademia can be daunting, but good mentorship makes a difference. Now, I want to be the mentor I never had."
  },
  {
    "objectID": "2025/my-path-reflection/llm-jumpstarters.html#my-already-not-just-academic-path-reflection-3",
    "href": "2025/my-path-reflection/llm-jumpstarters.html#my-already-not-just-academic-path-reflection-3",
    "title": "",
    "section": "My (Already Not Just) Academic Path Reflection",
    "text": "My (Already Not Just) Academic Path Reflection\nAs I look back on my academic journey, I am struck by how far I’ve come and how much I’ve learned along the way. What began as a straightforward academic path has evolved into something far more diverse and multifaceted. This blog post is a reflection on my experiences, the challenges I’ve faced, and the lessons I’ve learned. It’s also a declaration of readiness to take on a new role: that of a scientific mentor.\n\nA Very Successful Academic Path\nBy many measures, my academic career has been a success. I’ve earned a PhD in Demography from the University of Groningen, where I defended my dissertation on Changing regional inequalities in ageing across Europe in 2020. My work has been published in reputable journals such as Nature Human Behaviour, Population Studies, and The Lancet, and I’ve presented at international conferences across Europe and beyond. I’ve held positions at prestigious institutions, including my current role as a Senior Researcher at Statistics Denmark and my Associate Membership at the Leverhulme Centre for Demographic Science, University of Oxford. I’ve also been recognized with awards such as the EAPS Outreach Award and the Best Poster Award from the Netherlands Demographic Society. On paper, it looks like a classic academic success story. Yet, as I reflect on this path, I realize that success in academia is not just about ticking boxes or climbing the ladder. It’s about finding meaning in the work you do and making a tangible impact—something I’ve been striving to achieve in my own way.\n\n\nThe Lack of a Clear Research Topic\nOne of the challenges I’ve faced is the absence of a single, well-defined research topic that I can call my own. While I’ve worked on a variety of projects—ranging from mortality and aging to migration and spatial demography—I’ve often felt like a jack-of-all-trades rather than a specialist. For example, my research has explored topics as diverse as non-survival to pension age in Denmark and Sweden, COVID-19’s impact on aging European regions, and internal youth migration in Russia. This lack of focus has sometimes made it difficult to position myself within the academic landscape. However, it has also allowed me to explore diverse areas of demography, gaining a broad perspective that I wouldn’t have had otherwise. I’ve come to see this not as a weakness, but as a strength that enables me to connect ideas across disciplines and contribute to interdisciplinary research.\n\n\nDiverse Experiences: Research, Teaching, Outreach, and More\nMy career has been anything but linear. I’ve conducted research in multiple fields of demography, worked in several countries, and collaborated with diverse research teams. Beyond research, I’ve taught in various contexts, from university courses to workshops, and developed an R package (DemoTools) to make demographic methods more accessible. I’ve also engaged in science communication, writing blog posts, interacting with journalists, and maintaining an active presence on social media. My teaching experience includes courses like Data Visualization – the Art/Skill Cocktail at the University of Oxford and workshops on Mapping in R at the University of Groningen. These experiences have taught me the value of versatility and adaptability. They’ve also shown me that academia is not just about publishing papers—it’s about sharing knowledge, inspiring others, and contributing to the broader scientific community.\n\n\nMentoring Early Career Researchers\nOne of the most rewarding aspects of my career has been mentoring early career researchers. Over the years, I’ve guided countless students and young professionals, helping them navigate the complexities of academia, develop their skills, and build their confidence. For example, I’ve served as an external evaluator for PhD candidates like Sebastian Hanika at Lund University, and I’ve reviewed master’s theses at the National Research University Higher School of Economics. This role has given me a sense of purpose and fulfillment that goes beyond my own research achievements. It’s also made me realize how much I enjoy supporting others in their academic journeys. While I’ve always been happy to offer advice and encouragement, I’ve recently come to see mentoring as a more formal and integral part of my professional identity.\n\n\nThe Lack of Formal Scientific Mentoring\nDespite my success in mentoring others, I’ve often felt the absence of formal scientific mentoring in my own career. While I’ve had supportive colleagues and collaborators, I’ve never had a dedicated mentor who could provide consistent guidance and feedback. This has sometimes left me feeling isolated and uncertain about my direction. However, it has also pushed me to take ownership of my career and seek out opportunities for growth on my own. In a way, this lack of formal mentoring has made me more self-reliant and resourceful, qualities that I now bring to my own mentoring relationships.\n\n\nA Recent Mentoring Experience: Giving Feedback to Sebastian\nA recent experience solidified my belief in the importance of mentoring. I had the opportunity to give feedback to Sebastian, a young researcher who was struggling with a project. As we discussed his work, I realized how much I’ve learned over the years—not just about demography, but about the process of doing research, communicating ideas, and overcoming challenges. Sharing this knowledge with Sebastian was incredibly rewarding, and it reminded me of the impact that a good mentor can have. It also made me realize that I’m ready to take on a more formal mentoring role, not just as an occasional advisor, but as a dedicated guide for early career researchers.\n\n\nI’m Ready to Be a Scientific Mentor Now!\nAfter years of diverse experiences, both within and beyond academia, I feel ready to embrace the role of a scientific mentor. I want to help others navigate the challenges I’ve faced, share the lessons I’ve learned, and support them in achieving their goals. Mentoring is not just about imparting knowledge—it’s about building relationships, fostering growth, and creating a sense of community. I’m excited to take on this role and contribute to the development of the next generation of researchers. If my journey has taught me anything, it’s that success in academia is not just about individual achievements, but about the connections we make and the impact we have on others.\nAs I move forward, I look forward to continuing my research, teaching, and outreach efforts, but also to dedicating more time and energy to mentoring. My academic path may have started as a straightforward pursuit of knowledge, but it has evolved into something much richer and more meaningful. I’m grateful for the opportunities I’ve had and excited for what’s to come. Here’s to the next chapter—and to helping others write their own stories.\nThis version of the blog post integrates specific details from your CV, such as your publications, teaching experiences, and mentoring activities, to provide a more comprehensive and personalized reflection on your academic journey."
  },
  {
    "objectID": "2025/my-path-reflection/llm-jumpstarters.html#the-traditional-success-story",
    "href": "2025/my-path-reflection/llm-jumpstarters.html#the-traditional-success-story",
    "title": "",
    "section": "The Traditional Success Story",
    "text": "The Traditional Success Story\nMy academic journey has followed what many would consider a textbook path to success. From my early beginnings at Moscow State University to earning my PhD at the University of Groningen, I’ve published in respected journals like The Lancet, PNAS, and Population Studies. My research on regional inequalities in aging across Europe has secured recognition, including the EAPS Outreach Award and an AURORA-II PhD Scholarship. The metrics that academia values—citations, impact factors, and institutional affiliations—have aligned favorably with my career trajectory, leading to my current position as Senior Researcher at Statistics Denmark and Associate Member at the Leverhulme Centre for Demographic Science at Oxford."
  },
  {
    "objectID": "2025/my-path-reflection/llm-jumpstarters.html#searching-for-my-north-star",
    "href": "2025/my-path-reflection/llm-jumpstarters.html#searching-for-my-north-star",
    "title": "",
    "section": "Searching for My North Star",
    "text": "Searching for My North Star\nDespite these external markers of success, I’ve often found myself wrestling with a fundamental question: what is my research topic? While colleagues built careers around specific niches, my publications span diverse areas—from COVID-19 impacts on unequally aging regions to sex gaps in life expectancy, from internal youth migration in Russia to regional convergence in population aging across Europe. This absence of a clear research identity has been both a source of insecurity and, paradoxically, a catalyst for the diverse path I’ve forged."
  },
  {
    "objectID": "2025/my-path-reflection/llm-jumpstarters.html#the-power-of-academic-versatility",
    "href": "2025/my-path-reflection/llm-jumpstarters.html#the-power-of-academic-versatility",
    "title": "",
    "section": "The Power of Academic Versatility",
    "text": "The Power of Academic Versatility\nWhat my career may lack in singular focus, it makes up for in breadth and adaptability. My research has spanned multiple subfields of demography, taking me from Moscow to Groningen, from Odense to Copenhagen, and introducing me to diverse research teams with varying approaches. I’ve taught everything from specialized workshops on data visualization at the Barcelona Summer School of Demography to guest lectures on migration and R programming at various institutions. Beyond traditional academic outputs, I’ve developed R packages like DemoTools and tricolore that serve the wider community, maintained an active blog and social media presence (as evidenced by my BSKY and GitHub profiles), and cultivated relationships with outlets like The Atlantic, The Economist, and The Guardian to communicate research to broader audiences."
  },
  {
    "objectID": "2025/my-path-reflection/llm-jumpstarters.html#the-accidental-mentor",
    "href": "2025/my-path-reflection/llm-jumpstarters.html#the-accidental-mentor",
    "title": "",
    "section": "The Accidental Mentor",
    "text": "The Accidental Mentor\nPerhaps the most unexpected development in my career has been the gravitational pull toward mentoring early career researchers. Without consciously seeking this role, I’ve found myself repeatedly providing guidance through my teaching activities across institutions like the Max Planck Institute, Oxford, and Barcelona. These teaching experiences in data visualization, mapping in R, and demographic methods have become not just peripheral activities but central and deeply rewarding aspects of my professional identity, reflected in the community service section of my CV that shows my commitment to nurturing the field."
  },
  {
    "objectID": "2025/my-path-reflection/llm-jumpstarters.html#the-missing-piece",
    "href": "2025/my-path-reflection/llm-jumpstarters.html#the-missing-piece",
    "title": "",
    "section": "The Missing Piece",
    "text": "The Missing Piece\nDespite my comfort in advising others, I’ve navigated much of my own path without formal scientific mentorship. While I’ve had excellent PhD advisors in Leo van Wissen and Joop de Beer, and have collaborated with established scholars like James Vaupel (whom I helped commemorate through the memorial website), I’ve largely lacked someone to consistently help shape my research agenda across the various institutions where I’ve worked. This absence has fostered independence but also left me wondering what different paths might have opened with more guided scientific development throughout my career transitions from Russia to the Netherlands to Denmark."
  },
  {
    "objectID": "2025/my-path-reflection/llm-jumpstarters.html#learning-to-guide-by-being-guided",
    "href": "2025/my-path-reflection/llm-jumpstarters.html#learning-to-guide-by-being-guided",
    "title": "",
    "section": "Learning to Guide by Being Guided",
    "text": "Learning to Guide by Being Guided\nA recent experience providing feedback to Sebastian Hanika as an external PhD evaluator at Lund University revealed something significant: the act of critically engaging with another’s research—identifying strengths, questioning assumptions, suggesting alternative approaches—crystallized my own scientific perspective in ways I hadn’t previously recognized. This formal evaluation role, combined with my extensive peer review experience for journals like Population Studies, Demography, and The BMJ, has helped me develop a coherent scientific viewpoint despite my diverse path."
  },
  {
    "objectID": "2025/my-path-reflection/llm-jumpstarters.html#the-next-chapter-embracing-the-scientific-mentor-role",
    "href": "2025/my-path-reflection/llm-jumpstarters.html#the-next-chapter-embracing-the-scientific-mentor-role",
    "title": "",
    "section": "The Next Chapter: Embracing the Scientific Mentor Role",
    "text": "The Next Chapter: Embracing the Scientific Mentor Role\nThis moment of clarity has brought me to an important realization: I’m ready to step fully into the role of scientific mentor. The breadth of my experience across multiple countries and research contexts, combined with my board membership in the Danish Demographic Association and my involvement with organizations like IUSSP and EAPS, has equipped me to guide others not just in navigating academic careers but in shaping research agendas and advancing scientific understanding. What once felt like a liability—my non-linear research path spanning urbanization, mortality, and migration—now reveals itself as a unique asset in helping others find their own way through the complex terrain of academic science."
  },
  {
    "objectID": "2017/global-male-life-expectancy-convergence/index.html",
    "href": "2017/global-male-life-expectancy-convergence/index.html",
    "title": "Global convergence in male life expectancy at birth",
    "section": "",
    "text": "In the modern history, the world has seen unprecedented decrease in human mortality – the result of the Demographic Transition. Initially, the improvements occurred only in the most developed societies, and by the mid XX century the world population was roughly divided in two parts according to mortality patterns (see the bi-modal distribution). After the 2nd World War, the developing countries started to catch up, and there was a clear convergence in life expectancy at birth, the most common summary measure of mortality.\n\n\n\n\n\n\n\n\nTo reproduce the plot from the scratch please see the gist"
  },
  {
    "objectID": "2018/perfect-rstudio-layout/index.html",
    "href": "2018/perfect-rstudio-layout/index.html",
    "title": "A perfect RStudio layout",
    "section": "",
    "text": "Tiny things can separate life into “before” and “after”. Here is one. For almost a year I’ve been daily sending mental “thank you” to Ugo who showed me how to re-organize panes in RStudio. Since then I’ve been spreading this tiny improvement so many times that I thought the tiny advise deserved a separate tiny post. Please note, below is an opinionated view of a comfortable UI improvement; feel free to ignore it if you don’t like. This advise is highly subjective, though, I really believe it is useful.\nI find the default 4-pane layout of RStudio is not perfect. One needs more space for the “Source” pane. Especially when RStudio is used as the main text editor, i.e. the program to write code, papers, blog posts, prepare presentations… Thus, the perfect solution is to move “Console” to the top-right position, leave least useful “History” in the bottom-left corner and collapse it, and move everything else to the bottom-right corner (see the screenshot).\n\nJust go to “Tools” –&gt; “Global options” –&gt; “Pane layout” and fix it.\n\nThat’s it!\nJust enjoy your improved RStudio, the program.\nP.S. It is also very handy to memorize and use the hot keys for panes. CTRL + # moves focus to the pane, CTRL + SHIFT + # maximizes the pane."
  },
  {
    "objectID": "2019/dotplot/index.html",
    "href": "2019/dotplot/index.html",
    "title": "Dotplot – the single most useful yet largely neglected dataviz type",
    "section": "",
    "text": "Important\n\n\n\nI have to confess that the core message of this post is not really a fresh saying. But if I was given a chance to deliver one dataviz advise to every (ha-ha-ha) listening mind, I’d choose this: forget multi-category bar plots and use dotplots instead.\n\n\nI was converted several years ago after reading this brilliant post. Around the same time, as I figured out later, demographer Griffith Feeney wrote a similar post. From fresher examples, there are chapters discussing dotplot visualizations in Claus Wilke’s book and Kieran Healy’s book. So, what are dotplots and what’s so special about them?\nBasically, dotplots are the same regular barplots rotated 90 degrees and optimized on “ink usage”, i.e. dots represent values along the continuous horizontal axis. This leaves the vertical axis for the categorical variable allowing to write out long labels for the categories as normal horizontally aligned text (nobody likes to read that awful 45 degrees labels). The use of dots instead of bars allows to represent several comparable values for each category.\nHere is a sample of some ugly plots that I found in the recent papers in my personal library of papers.\n\nIt was not difficult to compose such a selection as these plots are, unfortunately, super popular. I think, the guy to blame here is (surprise-surprise) Excel. I’m failing to understand how such a basic plot type can be missed out.\nJust to be clear, I’m guilty too. Before the happy rstats-ggplot switch, I’ve been producing the same Excel plots. Here is an example from my 2014 working paper on internal youth migration in the central regions of Russia.\n\nThese two figures became one in the final paper recently published in GeoJournal. I think it’s a nice example how information dense, pretty looking (very subjective here), and easy to read a dotplot can be.\n\nThe figure shows discrepancy in migration estimates based on statistical record and census indirect estimates.\nThe code to replicate this figure is in this gist. A couple of coding solutions might be interesting here. First, the trick to offset vertically the dots for the two cohorts. Yet I realise that the way I implemented it is a bit clumsy; if you know a more elegant solution please let me know. Also, I believe in some cases composing the legend manually pays off, especially when we are dealing with two-dimensional legend. If the plot has some empty area it’s always a good idea to move the legend in the plotting area thus cutting away the margins."
  },
  {
    "objectID": "2021/life-expectancy-101/index.html",
    "href": "2021/life-expectancy-101/index.html",
    "title": "What is life expectancy? And, even more important, what it isn’t",
    "section": "",
    "text": "It really is a remarkable achievement and maybe a lot of luck that the world mundanely operates with such a complex indicator as life expectancy. Unlike many statistics and quantities of general use that are being monitored and reported regularly, life expectancy is not observed directly. It’s an output of a mathematical model called life table. And as any model it comes with a certain load of assumptions and limitations, which are easily forgotten and omitted in the everyday interpretations and misinterpretations of the indicator.\nSo, why do we need any mathematical modelling in the first place? Consider a seemingly simple task: you want to know how long people live. What can be easier? Let’s just see how many years lived those who died recently. Why not? Such a metric would be massively driven by population age structure. For the most of the recent history human populations were rapidly growing, which means that each next generation was bigger than the previous one. Relative differences in the size of generations affect the age composition of those dying.\nOkay. Then why don’t we simply take a group of people born in the same year and see how long on average they live? We could (demographers call such groups cohorts). But it takes remarkably long to wait until the last person in the cohort dies. And we want to know what’s happening now.\nHow can we learn what’s happening now? Well, for that we need a mathematical model. We cannot observe the unfulfilled lifespans directly but we can construct/imagine an artificial population to help us understand the current mortality. The idea is simple: let’s take those dying now and divide them by the size of their age groups. This yields age-specific death rates – the key input for the life table needed to calculate life expectancy. Now, let’s take an imaginary cohort and see how long would they live on average if they experience these observed age-specific death rates. The imaginary population is know as a synthetic cohort. And here comes the main assumption of the life table: The model assumes that the observed age-specific death rates stay unchanged throughout the hypothetical lives of the hypothetical people in the synthetic cohort.\nThis big assumption of unchanging age profile of death rates almost never holds in real life. Mortality in human populations keeps improving beyond the most optimistic expectations. For decades the best demographers were systematically underestimating the progress in mortality reduction (Oeppen and Vaupel, 2002).\n\nThe horizontal lines are the limits of human population-level life expectancy as anticipated by renowned scholars; points are the actual data in the world leading countries. Source: (Oeppen and Vaupel, 2002)\nLife expectancy is a snapshot of the current mortality and is not a projection/forecast of the actual experience of the newborn cohorts. The current nature of period life expectancy is nicely illustrated by Dr. Robert Chung: “I have a car that can display”driving range” given its estimate of fuel level and how I’m driving. When climbing a steep hill, the range can decrease a lot; when descending, the range can increase. That’s what period e(x) is like.”\nBut if life expectancy talk about now and not the future, why is it called “expectancy” in the first place? This rather unfortunate and confusing naming comes from statistics, where “expected value” is a standard term for the mean of a distribution. The connotation crossing is rather unfortunate and as it strongly nudges the common future oriented misinterpretation of life expectancy.\nThe most popular error in public perception of period life expectancy forgets about the heavy assumption of the synthetic cohort (constant age-specific death rates throughout their hypothetic lives anchored in current year) and talks about the future of kids being born now. In normal years this large interpretation error is somewhat masked by the gradual and often close to linear improvements in mortality. A rule of thumb is to simply add ~6 years to period life expectancy to obtain a reasonable cohort estimate (Goldstein and Wachter, 2006). Mortality shocks like 2020 are a different story though. Here the “forward looking” (mis)interpretation of period life expectancy projects the shock levels of mortality into the future. Of course this doesn’t happen. Shocks are called shocks because they are temporal.\nAnother important detail that often misses public attention is that life expectancy is not a single value – it can be estimated for every age. Most often and by default life expectancy is reported “at birth”. But we can estimate remaining life expectancy for various ages. And here comes another popular misunderstanding of life expectancy. Too often we come across the references to human age and longevity in the past that sound something like: “She was 40, a very elderly lady by the standards of that time as people lived on average about 30 years back the”. True, there were times when life expectancy at birth was about 30 years even in the most developed now countries. This doesn’t mean though that those who outlived this threshold age were getting old at young (by our current standards) ages. Let me illustrate.\nLet’s take Italian male population in 1872, the first available year in Human Mortality Database. Have a look at the survival of this synthetic cohort – the proportion of the initial cohort that is still alive by certain age. Half of the synthetic cohort died by age 15!\n\nAnd here is how remaining period life expectancy looked by age. Infant and child mortality was sooo high that those escaping early deaths had higher remaining life expectancy.\n\nAt age 34 remaining life expectancy was the same as at birth. Only, it applied to the 41% survivors. And I guess the perception of age was not radically different among those survivors. It was all about selection and luck getting there. The high early life mortality is responsible for another popular demographic myth, which postulates that everybody used to have many kids in the past. No, people used to have many births, and only a fraction of those kids survived to adult life.\n\n\n\n\n\n\n\nThis post is based on my previous Twitter thread\n\n\n\n\n\n\n\n\n\n\n\n\nReferences\n\nGoldstein JR, Wachter KW. 2006. Relationships between period and cohort life expectancy: Gaps and lags. Population Studies 60: 257–269 DOI: 10.1080/00324720600895876\n\n\nOeppen J, Vaupel JW. 2002. Broken limits to life expectancy. Science 296: 1029–1031 DOI: 10.1126/science.1069675"
  },
  {
    "objectID": "dd.html",
    "href": "dd.html",
    "title": ".",
    "section": "",
    "text": "Demographic Digest is a regular column at Demoscope Weekly which publishes (in Russian) brief summaries of fresh demographic papers from the best academic journals.\n\nДемографический Дайджест – регулярная рубрика журнала Демоскоп Weekly, в которой приводятся краткие обзоры демографических статей, публикуемых в ведущих зарубежных журналах.\n\n\n[PROJECT WEBSITE]\n\n\n\n[ARCHIVE]"
  },
  {
    "objectID": "2017/neet-in-europe/index.html",
    "href": "2017/neet-in-europe/index.html",
    "title": "Young people neither in employment nor in education and training in Europe, 2000-2016",
    "section": "",
    "text": "As an example of Eurostat data usage I chose to show the dynamics of NEET (Young people neither in employment nor in education and training) in European countries. The example is using the brilliant geofact package.\n\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(forcats)\nlibrary(eurostat)\nlibrary(geofacet)\nlibrary(viridis)\nlibrary(ggthemes)\nlibrary(extrafont)\n\n# Find the needed dataset code \n# http://ec.europa.eu/eurostat/web/regions/data/database\n\n# download fertility rates for countries\nneet &lt;- get_eurostat(\"edat_lfse_22\")\n\n# if the automated download does not work, the data can be grabbed manually at\n# http://ec.europa.eu/eurostat/estat-navtree-portlet-prod/BulkDownloadListing\n\nneet |&gt; \n        filter(geo |&gt; paste |&gt; nchar == 2,\n               sex == \"T\", age == \"Y18-24\") |&gt;\n        group_by(geo) |&gt; \n        mutate(avg = values |&gt; mean()) |&gt; \n        ungroup() |&gt; \n        ggplot(aes(x = time |&gt; year(),\n                   y = values))+\n        geom_path(aes(group = 1))+\n        geom_point(aes(fill = values), pch = 21)+\n        scale_x_continuous(breaks = seq(2000, 2015, 5),\n                           labels = c(\"2000\", \"'05\", \"'10\", \"'15\"))+\n        scale_y_continuous(expand = c(0, 0), limits = c(0, 40))+\n        scale_fill_viridis(\"NEET, %\", option = \"B\")+\n        facet_geo(~ geo, grid = \"eu_grid1\")+\n        labs(x = \"Year\",\n             y = \"NEET, %\",\n             title = \"Young people neither in employment nor in education and training in Europe\",\n             subtitle = \"Data: Eurostat Regional Database, 2000-2016\",\n             caption = \"ikashnitsky.github.io\")+\n        theme_few(base_family =  \"Roboto Condensed\", base_size = 15)+\n        theme(axis.text = element_text(size = 10),\n              panel.spacing.x = unit(1, \"lines\"),\n              legend.position = c(0, 0),\n              legend.justification = c(0, 0))\n\n\n\n\n\n\n\n\n\nThe whole code may be downloaded from the gist"
  },
  {
    "objectID": "2017/align-six-maps/index.html",
    "href": "2017/align-six-maps/index.html",
    "title": "Arranging subplots with ggplot2",
    "section": "",
    "text": "For my recently published paper, I produced not-so-standard figures that show the two step decomposition used in the analysis. Have a look:\nFigure 3 from my paper (PDF)\nActually, ggplot2 is a very powerful and flexible tool that allows to draw figures with quite a complex layout. Today I want to show the code that aligns six square plots (actually, maps) just as in the figure above. And it’s all about the handy function ggplot2::annotation_custom(). Since I used the layout more than once, I wrapped the code that produced it into a function that takes a list of 6 square plots as an input and yields the arranged figure with arrows as an output. Here is the commented code of the function.\n\nalign_six_plots &lt;- function(list.plots, \n                                    family = \"\",\n                                    labels=LETTERS[1:6], \n                                    labels.size=8){\n\n        require(tidyverse)\n        require(gridExtra)\n\n        gg &lt;- ggplot()+\n                coord_equal(xlim = c(0, 21), ylim = c(0, 30), expand = c(0,0))+\n\n                annotation_custom(ggplotGrob(list.plots[[1]]),\n                                  xmin = 0.5, xmax = 8.5, ymin = 21, ymax = 29)+\n\n                annotation_custom(ggplotGrob(list.plots[[2]]),\n                                  xmin = 12.5, xmax = 20.5, ymin = 19.5, ymax = 27.5)+\n                annotation_custom(ggplotGrob(list.plots[[3]]),\n                                  xmin = 12.5,xmax = 20.5,ymin = 10.5,ymax = 18.5)+\n\n                annotation_custom(ggplotGrob(list.plots[[4]]),\n                                  xmin = 0.5, xmax = 8.5, ymin = 9,ymax = 17)+\n                annotation_custom(ggplotGrob(list.plots[[5]]),\n                                  xmin = 0.5, xmax = 8.5, ymin = 0, ymax = 8)+\n                annotation_custom(ggplotGrob(list.plots[[6]]),\n                                  xmin = 12.5,xmax = 20.5, ymin = 0, ymax = 8)+\n\n                labs(x = NULL, y = NULL)+\n                theme_void()\n\n        # DF with the coordinates of the 5 arrows\n        df.arrows &lt;- data.frame(id=1:5,\n                                x=c(8.5,8.5,12.5,12.5,12.5),\n                                y=c(21,21,10.5,10.5,10.5),\n                                xend=c(12.5,12.5,8.5,8.5,12.5),\n                                yend=c(20.5,17.5,10,7,7))\n\n        # add arrows\n        gg &lt;- gg +\n                geom_curve(data = df.arrows |&gt; filter(id==1),\n                           aes(x=x,y=y,xend=xend,yend=yend),\n                           curvature = 0.1,\n                           arrow = arrow(type=\"closed\",length = unit(0.25,\"cm\"))) +\n                geom_curve(data = df.arrows |&gt; filter(id==2),\n                           aes(x=x,y=y,xend=xend,yend=yend),\n                           curvature = -0.1,\n                           arrow = arrow(type=\"closed\",length = unit(0.25,\"cm\"))) +\n                geom_curve(data = df.arrows |&gt; filter(id==3),\n                           aes(x=x,y=y,xend=xend,yend=yend),\n                           curvature = -0.15,\n                           arrow = arrow(type=\"closed\",length = unit(0.25,\"cm\"))) +\n                geom_curve(data = df.arrows |&gt; filter(id==4),\n                           aes(x=x,y=y,xend=xend,yend=yend),\n                           curvature = 0,\n                           arrow = arrow(type=\"closed\",length = unit(0.25,\"cm\"))) +\n                geom_curve(data = df.arrows |&gt; filter(id==5),\n                           aes(x=x,y=y,xend=xend,yend=yend),\n                           curvature = 0.3,\n                           arrow = arrow(type=\"closed\",length = unit(0.25,\"cm\")))\n\n        # add labes\n        gg &lt;- gg + annotate('text',label = labels,\n                            x=c(.5,12.5,12.5,.5,.5,12.5)+.5,\n                            y=c(29,27.5,18.5,17,8,8)+.1,\n                            size=labels.size,hjust=0, vjust=0, family = family)\n\n        return(gg)\n}\n\nLet’s check, if the function works. For that I create just a blank plot, clone it six times, store the six plots in a list, and finally feed it to the function.\n\nlibrary(tidyverse)\nlibrary(ggthemes)\n\n# create a simple blank square plot\np &lt;- ggplot()+\n  expand_limits(x = c(0,1), y = c(0,1))+\n  theme_map()+\n  theme(panel.border = element_rect(color = \"black\", size = 0.5, fill = NA),\n        aspect.ratio = 1)\n\n# clone this plot six times and store as a list of six\nplots &lt;- mget(rep(\"p\", 6))\n\n# use the function on the list\nsix &lt;- align_six_plots(plots)\n\n# save the output\nggsave(\"six_square_plots_aligned.png\", six, width=12, height=18)\n\n\nJust what we wanted to get.\n\n\n\n\n\n\n\nTo reproduce all the actual results and figures from my paper, have a look at this github repo"
  },
  {
    "objectID": "2017/ggplot2-microbenchmark/index.html",
    "href": "2017/ggplot2-microbenchmark/index.html",
    "title": "Accelerating ggplot2: use a canvas to speed up plots creation",
    "section": "",
    "text": "Too wrong; don’t read\n\n\n\nBasically, this post turned out to be just a wrong, premature, and unnecessary attempt of code optimization. If you still want to have look, make sure that later you read this post by Thomas Lin Pedersen. You are warned\n\n\n\n\n\n\n\n\n\nThis post is updated on 2017-07-15\n\n\n\nThe earlier version had a terminology mistake pointed out by Hadley Wickham. I wrongly called creation time of the plots as rendering time.\n\n\nOne of the nice features of the ggapproach to plotting is that one can save plots as R objects at any step and use later to render and/or modify. I used that feature extensively while creating maps with ggplot2 (see my previous posts: one, two, three, four, five). It is just convenient to first create a canvas with all the theme parameters appropriate for a map, and then overlay the map layer. At some point I decided to check if that workflow was computationally efficient or not. To my surprise, the usage of canvas reduces the creation time of a ggplot quite a lot. To my further surprise, this finding holds for simple plots as well as maps.\nLet’s start with a simple check.\n\n# load required packages\nlibrary(tidyverse)      # data manipulation and viz\nlibrary(ggthemes)       # themes for ggplot2\nlibrary(viridis)        # the best color palette\nlibrary(rgdal)          # deal with shapefiles\nlibrary(microbenchmark) # measure the speed of executing\nlibrary(extrafont)      # nice font\nmyfont &lt;- \"Roboto Condensed\"\nlibrary(RColorBrewer)\n\n# create a canvas \ncanv_mt &lt;- ggplot(mtcars, aes(hp, mpg, color = cyl))+\n        coord_cartesian()\n\n# test speed with mocrobenchmark\ntest &lt;- microbenchmark(\n        without_canvas = ggplot(mtcars, aes(hp, mpg, color = cyl))+\n                coord_cartesian()+\n                geom_point()\n        \n        ,\n        \n        with_canvas = canv_mt+\n                geom_point()\n       \n        ,\n        \n        times = 100\n)\n\ntest\n\nautoplot(test)+\n        aes(fill = expr)+\n        scale_fill_viridis(discrete = T)+\n        theme_bw(base_size = 15, base_family = myfont)+\n        theme(legend.position = \"none\",\n              axis.text = element_text(size = 15))+\n        labs(title = \"The speed of creating a simple ggplot\")\n\nFigure 1. Microbenchmark output for a simple plot\nThe median time of execution is 3.24 milliseconds for the plot without canvas and 2.29 milliseconds for the plot with canvas.\nNext, let’s do the same check for a map. For that, I will use the data prepared for one of my earlier posts and recreate the simple map that shows the division of European Union 27 into three subregions.\nFigure 2. The map we use to test the plot creation speed\n\n# load the already prepared data\nload(url(\"https://ikashnitsky.github.io/doc/misc/map-subplots/df-27-261-urb-rur.RData\"))\nload(url(\"https://ikashnitsky.github.io/doc/misc/map-subplots/spatial-27-261.RData\"))\n\n# fortify spatial objects\nneib &lt;- fortify(Sneighbors)\nbord &lt;- fortify(Sborders)\nfort &lt;- fortify(Sn2, region = \"id\")\n\n# join spatial and statistical data\nfort_map &lt;- left_join(df, fort, \"id\")\n\n# pal for the subregions\nbrbg3 &lt;- brewer.pal(11,\"BrBG\")[c(8,2,11)]\n\n# create a blank map\nbasemap &lt;- ggplot()+\n        geom_polygon(data = neib,\n                     aes(x = long, y = lat, group = group),\n                     fill = \"grey90\",color = \"grey90\")+\n        coord_equal(ylim = c(1350000,5450000), \n                    xlim = c(2500000, 6600000), \n                    expand = c(0,0))+\n        theme_map(base_family = myfont)+\n        theme(panel.border = element_rect(color = \"black\",size = .5,fill = NA),\n              legend.position = c(1, 1),\n              legend.justification = c(1, 1),\n              legend.background = element_rect(colour = NA, fill = NA),\n              legend.title = element_text(size = 15),\n              legend.text = element_text(size = 15))+\n        labs(x = NULL, y = NULL)\n\n\n# test speed with mocrobenchmark\ntest_map &lt;- microbenchmark(\n        without_canvas = \n                ggplot()+\n                geom_polygon(data = neib,\n                             aes(x = long, y = lat, group = group),\n                             fill = \"grey90\",color = \"grey90\")+\n                coord_equal(ylim = c(1350000,5450000), \n                            xlim = c(2500000, 6600000), \n                            expand = c(0,0))+\n                theme_map(base_family = myfont)+\n                theme(panel.border = element_rect(color = \"black\",\n                                                  size = .5,fill = NA),\n                      legend.position = c(1, 1),\n                      legend.justification = c(1, 1),\n                      legend.background = element_rect(colour = NA, fill = NA),\n                      legend.title = element_text(size = 15),\n                      legend.text = element_text(size = 15))+\n                labs(x = NULL, y = NULL) +\n                geom_polygon(data = fort_map, \n                             aes(x = long, y = lat, group = group, \n                                 fill = subregion), color = NA)+\n                scale_fill_manual(values = rev(brbg3)) +\n                theme(legend.position = \"none\")\n        \n        ,\n        \n        with_canvas = \n                basemap +\n                geom_polygon(data = fort_map, \n                             aes(x = long, y = lat, group = group, \n                                 fill = subregion), color = NA)+\n                scale_fill_manual(values = rev(brbg3)) +\n                theme(legend.position = \"none\")\n        \n        ,\n        \n        times = 100\n)\n      \n\nautoplot(test_map)+\n        aes(fill = expr)+\n        scale_fill_viridis(discrete = T)+\n        theme_bw(base_size = 15, base_family = myfont)+\n        theme(legend.position = \"none\",\n              axis.text = element_text(size = 15))+\n        labs(title = \"The speed of creating a map with ggplot2\")\n\nFigure 3. Microbenchmark output for a map\nThe median time of execution is 18.8 milliseconds for the map without canvas and 6.3 milliseconds for the map with canvas.\nConclusion: Use canvas with ggplot2\n\n\n\n\n\n\n\nFor the full script to reproduce the results check out this gist"
  },
  {
    "objectID": "2020/background-data/index.html",
    "href": "2020/background-data/index.html",
    "title": "Show all data in the background of your faceted ggplot",
    "section": "",
    "text": "One of the game-changing features of ggplot2 was the ease with which one can explore the dimensions of the data using small multiples.1 There is a small trick that I was to share today – put all the data in background of every panel. This can considerably improve comparability of the data across the dimension which splits the dataset into the subsets for the small multiples. Better to show right away what I mean and then explain in details.\n1 At some point lattice became pretty popular for the task, but then ggplot2 entered the scene.2 Images are clickableThere is a weekly dataviz challenge organized by Cole Knaflic. One particular challenge stroke me as an ultimate case to showcase this background data trick. Here are the two plots:2 the challenge one and my version.\n\n\nImage to improve\n\n\n\nMy version\n\nIt is impossible to meaningfully compare and distinguish multiple spaghetti lines at once. Thus, my choice here was to use small multiples and look at the lines one by one. But we still want to compare the lines. For this, I added the pale background lines that show the spread of all data. Note that I also sorted the small multiples in the decreasing order and added the average line in yellow.\nBut the main trick here is adding all the data in the background. And with ggplot2 it’s super easy to do. All we need is to add a layer to the plot in which we modify the data by removing the variable that was used for faceting.\nHere we use the nice feature of ggplot2 – the layers inherit whatever you specify in the main ggplot() call. In this case our background layer is inheriting the data parameter and all we need is just to remove the variable that is later used for faceting. Consider the following pseudo-code:\n\ndf |&gt; \n    ggplot(PLOT_PARAMETERS)+\n    geom_WHATEVER(data = df |&gt; select(-FACETING_VARIABLE))+\n    facet_wrap(~ FACETING_VARIABLE)\n\nOnce we’ve done this, gpplot2 no longer knows how to assign subsets of data to the corresponding small multiples. Note that this only happens in the layer where we perform the trick and explicitly throw out the faceting variable. As the result, in each small multiple we end up with all the data in this layer. Put this layer in the background, make it appropriately pale/transparent – and that’s it. I find this dataviz trick amazingly straightforward, simple, and powerful.\n\n\n\n\n\n\n\nYou can replicate the full figure above using the R code from this gist\n\n\n\n\n\n\n\n\n\n\n\n\nThis post is one in the faceting series. Other posts:\n\n\n\n\nSave space in faceted plots"
  },
  {
    "objectID": "me.html",
    "href": "me.html",
    "title": ".",
    "section": "",
    "text": "CV"
  },
  {
    "objectID": "me.html#bio",
    "href": "me.html#bio",
    "title": ".",
    "section": "BIO",
    "text": "BIO\nI was born in Israel, grew up in Moscow, during my PhD years lived in The Hague, and now live in Odense. Married and have two daughters. I got a bachelors in geography from Moscow State University in 2012, masters in demography from the National Research University Higher School of Economics (Moscow) in 2014, and PhD from University of Groningen in 2020.\n\n\n\n\n\n\n\n\n\nFeel free to contact me via emaililya.kashnitsky at gmail dot com\nIlya\n\n  Subscribe for email updates"
  },
  {
    "objectID": "2017/map-hacking/index.html",
    "href": "2017/map-hacking/index.html",
    "title": "Hacking maps with ggplot2",
    "section": "",
    "text": "This is a very short post on mapping with ggplot2.\nQuite often, mapping some data, we do not need to follow scrupulously the formal requirements to geographical maps – the idea is just to show the spatial dimension of the data. For instance, the network of rivers is not the most important information when we map the elections outcome. Thus, the simplified mapping allows quite some freedom in transforming the geodata. The classical example of such geodata transformation is the replacement and scaling of Alaska and Hawaii to be mapped alongside the mainland of the US. As one may see in this example, usually such geodata transformations utilize quite complex GIS tools in order to reposition an object in the coordinate system.\nThe interesting feature of mapping with ggplot2 is that, before the actual plotting, geodata has to be fotrified (ggplot2::fortify) – transformed to a simple dataframe object. Since fortified geodata is basically a dataframe, some simple transformations could be made really easily.\nIn my last paper, I needed to show a two-dimensional grouping of the European NUTS-2 regions in 4 quadrants according to GDP per capita and the share of working-age population (see Figure 8 in the preprint). In line with the study setting, I did the grouping separately for Western, Southern, and Eastern Europe. I decided that the most straightforward way to show that on map would be to visually separate the 3 subregions of Europe. The task is easily doable through triggering the fortified geodata object – see the code below.\nFirst, the code to prepare the R session and load the (already prepared) data.\n\nlibrary(tidyverse) # version 1.1.1\nlibrary(extrafont) # version 0.17\nlibrary(ggthemes) # version 3.4.0\nfont &lt;- \"Roboto Condensed\"\nlibrary(hrbrthemes) # version 0.1.0\n# The code is tested on a PC-win7-x64\n# R version 3.3.3\n\n\n# load the prepared geodata and stat data\nload(url(\"https://ikashnitsky.github.io/share/1704-map-hacking/map-hacking.Rdata\"))\n\n# fortify the spatial objects\nbord &lt;- fortify(Sborders)\nfort &lt;- fortify(Sn2, region = 'id')\n\nNext, I hack the geodata (long and lat variables) moving groups of NUTS-2 regions (Western, Southern, and Eastern Europe) apart. The appropriate values to move the groups of regions were found empirically.\n\n# hack geodata to separate macro-regions\nfort_hack &lt;- fort |&gt; \n        left_join(df |&gt; select(id, subregion), 'id') |&gt; \n        mutate(long = ifelse(subregion=='E', long + 5e5, long),\n               long = ifelse(subregion=='S', long + 2e5, long),\n               lat = ifelse(subregion=='S', lat - 5e5, lat),\n               long = ifelse(subregion=='W', long - 2e5, long))\n\nFinally, we are ready to create the schematic map.\n\n# create color pallete\nbrbg &lt;- RColorBrewer::brewer.pal(11,\"BrBG\")\nbrbg4 &lt;- brbg[c(4,9,2,11)]\n\n# create the two-dim legend\nggleg &lt;- ggplot()+\n        coord_equal(xlim = c(0,1), ylim = c(0,1), expand = c(0,0))+\n        annotate('rect', xmin = .45, xmax = .6, ymin = .1, ymax = .25, \n                 fill = brbg4[1], color = NA)+\n        annotate('rect', xmin = .45, xmax = .6, ymin = .4, ymax = .55, \n                 fill = brbg4[2], color = NA)+\n        annotate('rect', xmin = .75, xmax = .9, ymin = .1, ymax = .25, \n                 fill = brbg4[3], color = NA)+\n        annotate('rect', xmin = .75, xmax = .9, ymin = .4, ymax = .55, \n                 fill = brbg4[4], color = NA)+\n        annotate('rect', xmin = .05, xmax = .95, ymin = .05, ymax = .95, \n                 fill = NA, color = \"grey20\")+\n        \n        annotate('text', x = .35, y = c(.175, .475), vjust = .5, hjust = 1,\n                 size = 6, fontface = 2, label = c('POOR', 'RICH'), family = font) + \n        annotate('text', x = c(.525, .825), y = .65, vjust = 0, hjust = .5,\n                 size = 6, fontface = 2, label = c('LOW', 'HIGH'), family = font)+\n        annotate('text', x = .1, y = .9, vjust = 1, hjust = 0,\n                 size = 7, fontface = 2, label = \"LEGEND\", family = font)+\n        theme_map()\n\n# create the blank map\nbasemap &lt;- ggplot()+\n        coord_equal(\n            ylim=c(900000,5400000), xlim=c(2500000, 7000000), expand = c(0,0)\n        )+\n        theme_map()+\n        theme(panel.border=element_rect(color = 'black',size=.5,fill = NA),\n              legend.position = 'none')\n\n# the main map\nmap_temp &lt;- basemap + \n        geom_map(map = fort_hack, data = df, aes(map_id=id, fill=group))+\n        scale_fill_manual(values = brbg4[c(3, 1, 4, 2)])\n\n# now combine the map and the legend\nmap &lt;- ggplot() + \n        coord_equal(xlim = c(0,1), ylim = c(0,1), expand = c(0,0))+\n        annotation_custom(\n            ggplotGrob(map_temp), xmin = 0, xmax = 1, ymin = 0, ymax = 1\n        )+\n        annotation_custom(\n            ggplotGrob(ggleg), xmin = 0.72, xmax = 0.99, ymin = 0.72, ymax = 0.99\n        )+\n        labs(\n            title = \"Labour force and income in EU-27 NUTS-2 regions\",\n            subtitle = \"Within each of the three macro-regions of Europe - Westren, \n             Southern, and Eastern -\\nNUTS-2 regions are classified in 4 groups \n             according to the level of GDP per capita\\nand the share of working \n             age population in 2008\",\n            caption = \"Data: Eurostat\\nAuthor: Ilya Kashnitsky (ikashnitsky.github.io)\"\n        )+\n        theme_ipsum_rc(plot_title_size = 30, subtitle_size = 20, caption_size = 15)\n\nAnd here is the result."
  },
  {
    "objectID": "2018/ddd-poster/index.html",
    "href": "2018/ddd-poster/index.html",
    "title": "Compare population age structures of Europe NUTS-3 regions and the US counties using ternary color-coding",
    "section": "",
    "text": "On 28 November 2018 I presented a poster at Dutch Demography Day in Utrecht. Here it is:\n\n\n\n\n\n\n\n\nThe poster compares population age structures, represented as ternary compositions in three broad age groups, of European NUTS-3 regions and the United States counties. I used ternary color-coding, a dataviz approach that Jonas Schöley and me recently brought to R in tricolore package.\nIn these maps, each region’s population age composition is uniquely color-coded. Colors show direction and magnitude of deviation from the center point, which represents the average age composition. Hue component of a color encodes the direction of deviation: towards yellow – more elderly population (65+); cyan – more people at working ages (15–64); magenta–more kids (&lt;15).\nOf course, NUTS-3 regions and the US counties are not perfect to compare; on average, NUTS-3 regions are roughly ten times bigger. That’s why the colors for European regions look quite muted, they are closer to the grey average composition.\nThe poster won NVD Poster Award via online voting of the conference participants.\n\n\n\n\n\n\n\n\nReplication\n\n\n\nThis time I layouted the poster in Inkscape rather than arranging everything with hundreds of R code lines. But all the elements of the posted are reproducible with code from this github repo.\n\n\n\n\n\n\n\n\nSee also\n\n\n\n\nKashnitsky, I., & Schöley, J. (2018). Regional population structures at a glance. The Lancet, 392(10143), 209–210.\nMy PhD project – Regional demographic convergence in Europe\nPaper (Schöley & Willekens 2017) with the initial ideas for tricolore package\nAn example of ternary colorcoding used to visualize cause-of-death data"
  },
  {
    "objectID": "2017/data-acquisition-three/index.html",
    "href": "2017/data-acquisition-three/index.html",
    "title": "Data acquisition in R (3/4)",
    "section": "",
    "text": "The series consists of four posts:\n\n\n\n\nLoading prepared datasets\nAccessing popular statistical databases\nDemographic data sources\nGetting spatial data\n\n\n\nFor each of the data acquisition options I provide a small visualization use case.\nHuman Mortality Database\nWhen it comes to testing the big questions of human population dynamics, there is no more reliable data source than Human Mortality Database. This database is run by demographers who use state-of-the-art methodology to overcome issues in the data. As the result, the estimates are as precise as possible. Their methods protocol is a masterpiece of demographic data processing. On the down side, the data of decent enough quality is available for only a bunch of countries. To explore the data I highly recommend [Human Mortality Database Explorer][exp] by Jonas Schöley.\nThanks to Tim Riffe’s HMDHFDplus package, one can now download HMD data with just a couple of lines of R code. Please note that an account at mortality.org is needed in order to download data. As one may guess from the package name, it also helps to grab data from equally brilliant Human Fertility Database.\nThe following example is taken from my earlier post (and updated a bit). I think it illustrates nicely the power of automated data acquisition in R. Here I am going to download one year population population structures for both males and females for each single country of HMD. If you want to reproduce the result, beware that the script will download a couple of dozens megabits of data. Then I will calculate and visualize as small multiples sex ratios in all countries along age dimension. Sex ratios reflect the two basic regularities of human demographics: 1) there are always more boys being born; 2) males experience higher mortality throughout their life-course. Besides some artificial and well known exceptions, sex ratio at birth does not vary dramatically and is more or less constant at the level of 105-106 boys per 100 girls. Hence, differences in the sex ratio profiles of countries mainly reflect gender gap in mortality.\n\n# load required packages\nlibrary(HMDHFDplus)\nlibrary(tidyverse)\nlibrary(purrr)\n\n\n# help function to list the available countries\ncountry &lt;- getHMDcountries()\n\n# remove optional populations\nopt_pop &lt;- c(\"FRACNP\", \"DEUTE\", \"DEUTW\", \"GBRCENW\", \"GBR_NP\")\ncountry &lt;- country[!country %in% opt_pop]\n\n# temporary function to download HMD data for a simgle county (dot = input)\ntempf_get_hmd &lt;- . |&gt; readHMDweb(\"Exposures_1x1\", ik_user_hmd, ik_pass_hmd) \n\n\n# download the data iteratively for all countries using purrr::map()\nexposures &lt;- country |&gt; map(tempf_get_hmd)\n\n# data transformation to apply to each county dataframe\ntempf_trans_data &lt;- . |&gt; \n        select(Year, Age, Female, Male) |&gt; \n        filter(Year %in% 2012) |&gt;\n        select(-Year) |&gt;\n        transmute(age = Age, ratio = Male / Female * 100)\n\n# perform transformation\ndf_hmd &lt;- exposures |&gt; \n        map(tempf_trans_data) |&gt; \n        bind_rows(.id = \"country\")\n\n# summarize all ages older than 90 (too jerky)\ndf_hmd_90 &lt;- df_hmd |&gt; \n        filter(age %in% 90:110) |&gt; \n        group_by(country) |&gt; \n        summarise(ratio = ratio |&gt; mean(na.rm = T)) |&gt;\n        ungroup() |&gt; \n        transmute(country, age = 90, ratio)\n\n# insert summarized 90+\ndf_hmd_fin &lt;- bind_rows(df_hmd |&gt; filter(!age %in% 90:110), df_hmd_90)\n\n# finaly - plot\ndf_hmd_fin |&gt;  \n        ggplot(aes(age, ratio, color = country, group = country))+\n        geom_hline(yintercept = 100, color = \"grey50\", size = 1)+\n        geom_line(size = 1)+\n        scale_y_continuous(limits = c(0, 120), \n                           expand = c(0, 0), \n                           breaks = seq(0, 120, 20))+\n        scale_x_continuous(limits = c(0, 90), \n                           expand = c(0, 0), \n                           breaks = seq(0, 80, 20))+\n        facet_wrap(~country, ncol = 6)+\n        theme_minimal(base_family = \"mono\", base_size = 15)+\n        theme(legend.position = \"none\",\n              panel.border = element_rect(size = .5, fill = NA, \n                                          color = \"grey50\"))+\n        labs(x = \"Age\", \n             y = \"Sex ratio, males per 100 females\", \n             title = \"Sex ratio in all countries from Human Mortality Database\",\n             subtitle = \"HMD 2012, via HMDHFDplus by @timriffe1\",\n             caption = \"ikashnitsky.github.io\")\n\n\nUnited Nations World Population Prospects\nPopulation Department of the United Nations provides high quality population estimates for all countries of the world. They update estimates every 2-3 years and publish openly as an interactive report World Population Prospects. One may find in these reports key highlights and, of course, rich data. The data is later wrapped in R packages called wpp20xx. Currently, the available packages are for the estimate updates 2008, 2010, 2012, 2015, and 2017. I will give here an example of wpp2015 use adapted from my earlier post.\nUsing ridgeplot, the amazing type of dataviz promoted ggridges package by Claus Wilke, I am going to show the impressive reduction of global inequality in male mortality that took place since 1950.\n\nlibrary(wpp2015)\nlibrary(tidyverse)\nlibrary(ggridges)\nlibrary(viridis)\n\n# get the UN country names\ndata(UNlocations)\n\ncountries &lt;- UNlocations |&gt; pull(name) |&gt; paste\n\n# data on male life expectancy at birth\ndata(e0M) \n\ne0M |&gt; \n        filter(country %in% countries) |&gt; \n        select(-last.observed) |&gt; \n        gather(period, value, 3:15) |&gt; \n        ggplot(aes(x = value, y = period |&gt; fct_rev()))+\n        geom_density_ridges(aes(fill = period))+\n        scale_fill_viridis(discrete = T, option = \"B\", direction = -1, \n                           begin = .1, end = .9)+\n        labs(x = \"Male life expectancy at birth\",\n             y = \"Period\",\n             title = \"Global convergence in male life expectancy at birth since 1950\",\n             subtitle = \"UNPD World Population Prospects 2015 Revision, via wpp2015\",\n             caption = \"ikashnitsky.github.io\")+\n        theme_minimal(base_family =  \"mono\")+\n        theme(legend.position = \"none\")\n\n\nEuropean Social Survey (ESS)\nEuropean Social Survey provides uniquely rich nationally representative cross-county comparable information on the values of Europeans. Every two years a cross-sectional sample is taken in each participating country. All the data is easily available upon registration. Datasets are distributed as SAS, SPSS, or STATA files. Thanks to Jorge Cimentada, these datasets are now easily available via ess package. I am going to visualize how respondents assessed their level of trust in police in all available countries at the latest round of the survey.\n\nlibrary(ess)\nlibrary(tidyverse) \n\n# help gunction to see the available countries\nshow_countries()\n\n# check the available rounds for a selected country\nshow_country_rounds(\"Netherlands\")\n\n# get the full dataset of the last (8) round\ndf_ess &lt;- ess_rounds(8, your_email =  ik_email) \n\n# select a variable and calculate mean value\ndf_ess_select &lt;- df_ess |&gt; \n        bind_rows() |&gt; \n        select(idno, cntry, trstplc) |&gt; \n        group_by(cntry) |&gt; \n        mutate(avg = trstplc |&gt; mean(na.rm = T)) |&gt; \n        ungroup() |&gt; \n        mutate(cntry = cntry |&gt; as_factor() |&gt; fct_reorder(avg))\n\ndf_ess_select |&gt; \n        ggplot(aes(trstplc, fill = avg))+\n        geom_histogram()+\n        scale_x_continuous(limits = c(0, 11), breaks = seq(2, 10, 2))+\n        scale_fill_gradient(\"Average\\ntrust\\nscore\", \n                            low = \"black\", high = \"aquamarine\")+\n        facet_wrap(~cntry, ncol = 6)+\n        theme_minimal(base_family = \"mono\")+\n        labs(x = \"Trust score [0 -- 10]\",\n             y = \"# of respondents\",\n             title = \"Trust in police\",\n             subtitle = \"ESS wave 8 2017, via ess by @cimentadaj\",\n             caption = \"ikashnitsky.github.io\")\n\n\nAmerican Community Survey and Census\nThere are several packages that provide access to the US Census and ACS data. Perhaps the most convenient one is the recent tidycensus package by Kyle Walker. One extremely useful feature of this approach is the ability to download geodata along with stats in the form of simple features. Simple features, a revolutionary approach to deal with spatial data in R implemented in sf package by Edzer Pebesma, allow to manage and visualize geodata tidy and efficiently. Note that in order to reproduce the following example one would have to install the development version of ggplot2.\nBelow I map median ages of census tracts population in Chicago based on the ACS estimates in 2015. To use tidycensus, an API key is required. API is instantly provided upon registration.\n\nlibrary(tidycensus)\nlibrary(tidyverse)\nlibrary(viridis)\nlibrary(janitor)\nlibrary(sf)\n# to use geom_sf we need the latest development version of ggplot2\ndevtools::install_github(\"tidyverse/ggplot2\", \"develop\")\nlibrary(ggplot2)\n\n\n# you need a personal API key, available free at\n# https://api.census.gov/data/key_signup.html\n# normally, this key is to be stored in .Renviron\n\n# see state and county codes and names\nfips_codes |&gt; View\n\n# the available variables\nload_variables(year = 2015, dataset = \"acs5\") |&gt; View\n\n# data on median age of population in Chicago\ndf_acs &lt;- get_acs(\n        geography = \"tract\",\n        county = \"Cook County\",\n        state = \"IL\",\n        variables = \"B01002_001E\",\n        year = 2015,\n        key = ik_api_acs,\n        geometry = TRUE\n) |&gt; clean_names()\n\n\n# map the data\ndf_acs |&gt; \n        ggplot()+\n        geom_sf(aes(fill = estimate |&gt; \n                            cut(breaks = seq(20, 60, 10))), \n                color = NA)+\n        scale_fill_viridis_d(\"Median age\", begin = .4)+\n        coord_sf(datum = NA)+\n        theme_void(base_family =  \"mono\")+\n        theme(legend.position = c(.15, .15))+\n        labs(title = \"Median age of population in Chicago\\nby census tracts\\n\",\n             subtitle = \"ACS 2015, via tidycensus by @kyle_e_walker\",\n             caption = \"ikashnitsky.github.io\", \n             x = NULL, y = NULL)\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nAll the code chunks together can be found in this gist"
  },
  {
    "objectID": "2017/hmd-all-sex-ratio/index.html",
    "href": "2017/hmd-all-sex-ratio/index.html",
    "title": "Sex ratios in all countries from Human Mortality Database",
    "section": "",
    "text": "Sex ratios reflect the two basic regularities of human demographics: 1) there are always more boys being born; 2) males experience higher mortality throughout their life-course. The sex ratio at birth does not vary dramatically1 and is more or less constant at the level of 105-106 boys per 100 girls. Hence, differences in the sex ratio profiles of countries mainly reflect gender gap in mortality. In this post I will compare sex ratios age profiles in all countries included in Human Mortality Database.\n1 There are cases of big deviations from this natural constant. The most well known one is the skewed sex ratio in China, where decades of One Child Policy together with strong traditional son preference resulted in selective abortions. Read more: Frejka et al. (2010); Feng (2011); Basten and Verropoulou (2013).R gives amazing opportunities to grab data fast and easy. Thanks to Tim Riffe’s HMDHFDplus package, one can now download HMD data with just a couple of lines of R code.\nThere is a handy function in HMDHFDplus package – getHMDcountries(). It lists the codes for all countries in HMD. So it becomes really easy to loop through the database and download data for all countries.\n\n# load required packages\nlibrary(tidyverse) # version 1.0.0\nlibrary(HMDHFDplus) # version 1.1.8\n\ncountry &lt;- getHMDcountries()\n\nexposures &lt;- list()\nfor (i in 1: length(country)) {\n        cnt &lt;- country[i]\n        exposures[[cnt]] &lt;- readHMDweb(cnt, \"Exposures_1x1\",\n                                       ik_user_hmd, ik_pass_hmd)\n        \n        # let's print the progress\n        paste(i,'out of',length(country)) \n}\n\n\n\n\n\n\n\nUse own credentials\n\n\n\nPlease note, the arguments ik_user_hmd and ik_pass_hmd are my login credentials at the website of Human Mortality Database, which are stored locally at my computer. In order to access the data, one needs to create an account at www.mortality.org and provide his own credentials to the readHMDweb() function.\n\n\nNext, I select 2012 for comparison – it is quite recent, and for most of the HMD countries there are data for 2012. The loop goes through each of the countries’ dataframe in exposures list, selects data for 2012 and calculates sex ratio at each age. I also remove data for several populations (like East and West Germany separately).\n\nsr_age &lt;- list()\n\nfor (i in 1:length(exposures)) {\n        di &lt;- exposures[[i]]\n        sr_agei &lt;- di |&gt; select(Year,Age,Female,Male) |&gt; \n                filter(Year %in% 2012) |&gt;\n                select(-Year) |&gt;\n                transmute(country = names(exposures)[i],\n                          age = Age, sr_age = Male / Female * 100)\n        sr_age[[i]] &lt;- sr_agei\n}\nsr_age &lt;- bind_rows(sr_age)\n\n# remove optional populations\nsr_age &lt;- sr_age |&gt; \n    filter(!country %in% c(\"FRACNP\",\"DEUTE\",\"DEUTW\",\"GBRCENW\",\"GBR_NP\"))\n\nAfter age 90, sex ratios become quite jerky due to the relatively small numbers of survivors. I decided to aggregate data after the age 90.\n\n# summarize all ages older than 90 (too jerky)\nsr_age_90 &lt;- sr_age |&gt; filter(age %in% 90:110) |&gt; \n        group_by(country) |&gt; summarise(sr_age = mean(sr_age, na.rm = T)) |&gt;\n        ungroup() |&gt; transmute(country, age=90, sr_age)\n\ndf_plot &lt;- bind_rows(sr_age |&gt; filter(!age %in% 90:110), sr_age_90)\n\nFinally, I plot the resulting sex ratios.\n\n# get nice font\nlibrary(extrafont)\nmyfont &lt;- \"Roboto Condensed\"\n\n# finaly - plot\ngg &lt;- ggplot(df_plot, aes(age, sr_age, color = country, group = country))+\n        geom_hline(yintercept = 100, color = 'grey50', size = 1)+\n        geom_line(size = 1)+\n        scale_y_continuous(\n            limits = c(0, 120), expand = c(0, 0), breaks = seq(0, 120, 20)\n        )+\n        scale_x_continuous(\n            limits = c(0, 90), expand = c(0, 0), breaks = seq(0, 80, 20)\n        )+\n        xlab('Age')+\n        ylab('Sex ratio, males per 100 females')+\n        facet_wrap(~country, ncol=6)+\n        theme_minimal(base_family = myfont, base_size = 15)+\n        theme(legend.position='none',\n              panel.border = element_rect(size = .5, fill = NA))\n\ngg\n\n\nThere is quite a variety in the sex ratio profiles. If the initial prevalence of males equalizes in Japan, Sweden, or Norway at around 60, in Russia, Belarus, and Ukraine this happens at around 30 due to very high male mortality. In many countries there are pronounced bumps in the sex ratio at ages 20-30, that are likely to be caused by international migration. For example, Scotland, Northern Ireland, Portugal, and New Zealand are experiencing substantial outflow of young men.\nWhat happened in Taiwan?\n\n\n\n\n\n\n\nThis post is based on my earlier tweet and gist\n\n\n\n\n\n\n\n\n\n\n\nReferences\n\nBasten S, Verropoulou G. 2013. “Maternity migration” and the increased sex ratio at birth in hong kong SAR. Population Studies 67: 323–334 DOI: 10.1080/00324728.2013.826372\n\n\nFeng W. 2011. The future of a demographic overachiever: Long-term implications of the demographic transition in china. Population and Development Review 37: 173–190 DOI: 10.1111/j.1728-4457.2011.00383.x\n\n\nFrejka T, Jones GW, Sardon J-P. 2010. East asian childbearing patterns and policy developments. Population and Development Review 36: 579–606 DOI: 10.1111/j.1728-4457.2010.00347.x"
  },
  {
    "objectID": "2024/rotate-damn-plot/index.html",
    "href": "2024/rotate-damn-plot/index.html",
    "title": "Rotate the damn plot",
    "section": "",
    "text": "Several days ago I saw a post on LinkedIn by Jornt Mandemakers with some very curious results from Gender and Generations Programme surveys. While representing interesting data, it was a perfect example of a way too common academic dataviz fallacy, and I decided to finally write this blog post.\nHere is the original plot by Jornt.\n\nWe are going to replicate it and then redo – all in hope to illustrate how much it can be improved with very very simple alterations. In fact, this post revives the argument I made in an earlier post called Dotplot – the single most useful yet largely neglected dataviz type. Despite the nice title and the same main message, I’m afraid the older post used a less indicative example. So I hope this time I’ll manage to illustrate the key message a bit clearer. But the message itself stays the same – forget multi-category bar/column plots and use dotplots instead. Or, put it the other way around: whenever you are visualizing a continuous measurement split into multiple categories, place the continuous variable on the horizontal (x) axis and the categorical variable on the vertical (y) axis. That’s it. Just a simple trick improves the readability of your plot a lot.\nUnfortunately, the use of crumpled multi-category bar plots is very widespread in academic papers. Have a look at a Google Lens search on the focal plot in this post. Google Lens browses the internet for similarly looking images, and they are plenty, most of them coming straight from academic papers.\n\nRotate the damn plot\nOver years of teaching dataviz to researchers I came to understanding that this particular mistreatment of data may be the most common and easily avoidable dataviz fallacy. Another dated attempt to spread this simple knowledge yielded an educational datatviz aimed at the general audience. Here it is.\n\nAs you can see, much better readable text is what we immediately get via such a simple alteration of the dataviz, just Rotate the damn plot. There are several more equally simple dataviz rules/principles that I keep teaching and want to write-up better here in future. If you are interested, follow the dedicated BlueSky account @damnplot.ikashnitsky.phd, I have big plans for it.\nOkay. Enough with the lengthy intro. Let’s replicate the plot from the very beginning of the post and then improve it.\nDigitizing the data\nBut first we need the data from the plot. Of course, I could have taken it from the original source, Gender and Generations Programme. Or I could have asked the authors to share only the data used for this plot. But for simple plots that use a handful of data points there is a better alternative – digitizing the values from the plot itself. There are numerous software solutions that streamline the task, including an R package {digitize}. This time I used the brilliant WebPlotDigitizer. The idea of digitizing is very straightforward – we read the actual values from a plot and thus convert the image into data underlying it. To do that we only need to provide the information about the coordinates of the plot grid.\n\nThen the remaining task is to click all the data points in already known coordinate space.\n\nJust 35 clicks, and we are ready to export a clean CSV file with the data of the plot. For replicability of this post I uploaded the data and code to a gist.\n\nFunny enough, I tried to set various LLMs to do the job for me, and some of them even happily reported the extracted data. I was impressed. Really impressed. Until I figured out that, once again, bullshitters are gonna bullshit. All the so called digitized data happened to be approximate guesses of varied wildness. I’m pretty sure that this not so complex task can be reasonably automated. But so far the mainstream LLMs consistently failed to do this. Here is a sneak-peek of my discussion with Claude. Everything is really impressive until you check closely.\n\nReplicating original plot\nHaving digitized the data manually, we are now good to go and finally replicate the original plot.\nThe following lines of code retrieve and reshape the data\n\nlibrary(tidyverse)\n\n# Points digitized manually using\n# https://web.eecs.utk.edu/~dcostine/personal/PowerDeviceLib/DigiTest/index.html\n# Data uploaded to a gist\ndigi &lt;- read_csv(\n    \"https://gist.githubusercontent.com/ikashnitsky/8cc26eab8165b0b79f67da761aa66a1e/raw/8d44c9e6ddea51de3cbb0fdb742fb75a62a307fd/data.csv\",\n    col_names = c(\"x\", \"happiness\")\n)\n\n# get all combinations of the categories and attach the digitized data\ndf &lt;- crossing(\n    country = c(\"United Kingdom\", \"Estonia\", \"Denmark\", \"Netherlands\", \"Czech Republic\", \"Finland\", \"Austria\") |&gt; as_factor(),\n    group = c(\"18-29\", \"30-39\", \"40-49\", \"50-59\", \"Total\")\n) |&gt;\n    mutate(happiness = digi |&gt; pull(happiness))\n\nNow we have the dataset ready to be plotted. First, let’s try to recreate the original plot as closely as we can. Note that I use exactly the same colors, which I picked from the original plot using a color picker tool. I also tried to outsource this simple task to LLMs. Yes, you guessed it right – they failed.\n\n# recreate the plot\ndf |&gt;\n    ggplot(aes(x = country, y = happiness, fill = group))+\n    geom_col(position = position_dodge()) +\n    labs(\n        x = NULL,\n        y = NULL,\n        fill = NULL\n    ) +\n    coord_cartesian(ylim = c(6, NA), expand = 0)+\n    scale_fill_manual(values = c(\"#2f74cc\", \"#ff7502\", \"#a3d384\", \"#ffbc00\", \"#908d90\"))+\n    theme_minimal()+\n    theme(\n        legend.position = \"bottom\",\n        panel.grid.minor = element_blank()\n    )\n\n\n\n\n\n\n\nHere I’ll briefly mention that there are more common issues with this particular plot. As a general rule think of any of your plots to be self-sufficient if just posted on social media without any additional context. Thus, it’s always a good idea to add a brief meaningful title, label the axes properly, explain the legend, and maybe add a few annotations.\nImproving the plot\nThe following lines of code take care of {ggplot2} theme, setting it to the one with my preferred selection of options. Earlier I wrote a dedicated blog post about this portable theming solution.\n\n# custom theming\ndevtools::source_gist(\"653e1040a07364ae82b1bb312501a184\")\nsysfonts::font_add_google(\"Atkinson Hyperlegible\", family = \"ah\")\ntheme_set(theme_ik(base_family = \"ah\", base_size = 11))\n\nAnd finally the improved version of the plot.\n\n# Create the plot\ndf |&gt;\n    ggplot(aes(y = country, x = happiness, color = group))+\n    geom_point(size = 3) +\n    labs(\n        title = \"How happy are you with your home?\",\n        subtitle = \"Happiness scores from Generations and Gender Programme surveys\\non a scale of 0 to 10\",\n        x = NULL,\n        y = NULL,\n        color = \"Age group\"\n    ) +\n    scale_color_manual(values = c(\"#2f74cc\", \"#ff7502\", \"#a3d384\", \"#ffbc00\", \"#908d90\"))+\n    theme(\n        legend.position = \"bottom\",\n        panel.grid.minor.x = element_blank(),\n        axis.text.y = element_text(size = 12)\n    )+\n    scale_x_continuous(position = \"top\")\n\n\n\n\n\n\n\nNote that the essential difference here is that I switched the axes effectively rotating the damn plot. I also used dots instead of filled bars. There is a great advantage in doing this, which is nicely illustrated by this plot. Using dots allows to trim the continuous axis to zoom closer to the data. When you do the same to a bar plot, the visual representation ofthe data gets distorted via the disproportional changes to the areas of the filled bars. Claus Wilke points this out and explains more in his brilliant dataviz book.\n\nOne important limitation of bars is that they need to start at zero, so that the bar length is proportional to the amount shown.\n\nAnd of course I added a title, subtitle, and legend caption. Note also a small handy trick that I use all the time – I moved x-axis labels to the top of the plot which allows me to use the subtitle as the axis label and not only save some space but also make the process of reading the plot more straightforward.\nLet me conclude with just reiterating that it is hard to overestimate how important is text in dataviz. It should be meaningful and sufficiently detailed, easily readable (i.e. necessarily horizontal and large enough), preferable using some nice font. Pay attention to the text elements of you dataviz!\n\n\n\n\n\n\n\nThis post is the first in the Damn Plot series. Other posts will follow, for now you may follow @damnplot.ikashnitsky.phd BlueSky account."
  },
  {
    "objectID": "2023/shrink-space/index.html",
    "href": "2023/shrink-space/index.html",
    "title": "Save space in faceted plots",
    "section": "",
    "text": "Faceting1 is probably the most distinctive feature that defined the early success and wide adoption of ggplot2. Small-multiples are often a great dataviz choice.2 But one common problem is when your panels for the subsets of data requite vastly different amount of space. By default the panels in faceted ggplots are all of the same size. If the data subsets are very different is size – a common case yould be time series of varying length – this results in a lot of plot space wasted in the panels with little data to show. In this post I’m showing how to deal with this common issue.\n1 This is the term used for small-multiples in ggplot22 See the post in which I improve an overloaded line chart using small-multiplesLet me first show you the solution. The cornerstone source of data in demography is Human Mortality Database. It provides demographic data of highest possible quality for a selection of available countries. The availability of data varies vastly across countries – from 270+ years in Sweden to a handful of decades in many other countries with less exceptional population data statistics. Here are two plots from my recent papers that use HMD.\n\n\nFigure A3 from Zarulli et al. (2021)\n\n\n\nFigure 3 from Aburto et al. (2022)\n\nThe trick in these plots is that countries are arranged by the length of time series. The width of each panel is net to the longest time series observed omong the countries in the column of small multiples. This is easily achieved via the parameter space = \"free\" in the facet_grid() call.\nOne slightly annoying nuance is that the space = \"free\" parameter is only available facet_grid(). That’s why we need to specify both column and row variables for the layout of small multiples. Yet, usually we have just one meaningful faceting variable in such a setup, country in the examples above. That’s why the steps of data preparation for these plots included the creation of variables row and column that explicitly located the position of each small multiple. You can find code that replicates both figures shown above here and here. To illustrate the approach in this post we’ll use a minimal example with generated data.\nConsider a case when we have 6 countries, 3 of which have relatively long time series and 3 have relatively short period of observed data. To stay a bit closer to the examples above, let’s say that we have data for Sweden (271 years worth of data), Denmark (186 years), Netherlands (169), Portugal (81), Japan (74), and Estonia (60). Let’s generate some random data of the specified time series’ length.\n\nset.seed(911)\n\nraw &lt;- tibble(\n    country = c(\"Sweden\", \"Denmark\", \"Netherlands\", \"Portugal\", \"Japan\", \"Estonia\"),\n    n_years = c(271, 186, 169, 81, 74, 60)\n) |&gt; \n    # mutate(country = country |&gt; as_factor()) |&gt; \n    group_by(country) |&gt; \n    group_modify(~ runif(n = .x$n_years) |&gt; tibble(random = _)) |&gt; \n    mutate(year = 2022 - seq_along(country)) |&gt; \n    ungroup()\n\nLet’s say I want a plot with 3 rows and 2 columns. First, here’s how a simple faceted plot would look like.\n\nraw |&gt; \n    ggplot(aes(year, random, color = country))+\n    geom_path()+\n    facet_wrap(~country, ncol = 2)\n\n\n\n\n\n\n\nSee how much space is just wasted in the plot.\nNext, we’ll do the trick outlined above: arrange the countries by the length of available data and shrink the unused space in the two columns.\n\narr &lt;- raw |&gt; \n    mutate(\n        country = country |&gt; as_factor() |&gt; fct_infreq()\n    ) |&gt;\n    arrange(country, year) |&gt; \n    # create facet positioning variables on a 3x2 canvas\n    mutate(\n        row = country |&gt;\n            lvls_revalue(\n                new_levels = 1:3 |&gt; rep(2) |&gt; paste() \n            ),\n        col = country |&gt;\n            lvls_revalue(\n                new_levels = 1:2 |&gt; rep(each = 3) |&gt; paste() \n            )\n    ) |&gt;\n    ungroup()\n\narr |&gt; \n    ggplot(aes(year, random, color = country))+\n    geom_path()+ \n    facet_grid(row~col, scales = \"free_x\", space = \"free\")\n\n\n\n\n\n\n\nThat’s it, the rest is just visual polishing of the plot. At the very least, we need to get rid of the facet strips which are now meaningless counts of rows and columns and add country names as text annotations.\n\narr |&gt; \n    ggplot(aes(year, random, color = country))+\n    geom_path()+ \n    facet_grid(row~col, scales = \"free_x\", space = \"free\")+\n    geom_text(\n        data = function(x) x|&gt; distinct(country, row, col),\n        aes(label = country), x = 2020, y = 1.05,\n        hjust = 1, vjust = 0, size = 5, \n        family = \"ah\", fontface = 2\n    )+\n    scale_y_continuous(limits = c(0, 1.15), breaks = seq(0, 1, .25))+\n    scale_x_continuous(breaks = seq(1750, 2000, 50))+\n    theme(\n        legend.position = \"none\",\n        strip.text = element_blank()\n    )\n\n\n\n\n\n\n\nEnjoy more dataviz freedom with faceting tricks =)\n\n\n\n\n\n\n\nThis post is one in the faceting series. Other posts:\n\n\n\n\nShow all data in the background of your faceted ggplot\n\n\n\n\n\n\n\n\nReferences\n\nAburto JM, Schöley J, Kashnitsky I, Zhang L, Rahal C, Missov TI, Mills MC, Dowd JB, Kashyap R. 2022. Quantifying impacts of the COVID-19 pandemic through life-expectancy losses: A population-level study of 29 countries. International Journal of Epidemiology 51: 63–74 DOI: 10.1093/ije/dyab207\n\n\nZarulli V, Kashnitsky I, Vaupel JW. 2021. Death rates at specific life stages mold the sex gap in life expectancy. Proceedings of the National Academy of Sciences 118 DOI: 10.1073/pnas.2010588118"
  },
  {
    "objectID": "2017/data-acquisition-one/index.html",
    "href": "2017/data-acquisition-one/index.html",
    "title": "Data acquisition in R (1/4)",
    "section": "",
    "text": "The series consists of four posts:\n\n\n\n\n\nLoading prepared datasets\n\n\nAccessing popular statistical databases\n\n\nDemographic data sources\n\nGetting spatial data\n\n\n\n\nFor each of the data acquisition options I provide a small visualization use case.\nBuilt-in datasets\nFor illustration purposes, many R packages include data samples. Base R comes with a datasets package that offers a wide range of simple, sometimes very famous, datasets. Quite a detailed list of built-in datasets from various packages is maintained by Vincent Arel-Bundock.\nThe nice feature of the datasets form datasets package is that they are “always there”. The unique names of the datasets may be referred as the objects from Global Environment. Let’s have a look at a beautiful small dataset calls swiss - Swiss Fertility and Socioeconomic Indicators (1888) Data. I am going to check visually the difference in fertility based of rurality and domination of Catholic population.\n\nlibrary(tidyverse)\n\nswiss |&gt; \n        ggplot(aes(x = Agriculture, y = Fertility, \n                   color = Catholic &gt; 50))+\n        geom_point()+\n        stat_ellipse()+\n        theme_minimal(base_family = \"mono\")\n\n\nGapminder\nSome packages are created specifically to disseminate datasets in a ready to use format. One of the nice examples is a package gapminder that contains a neat dataset widely used by Hans Rosling in his Gapminder project.\n\nlibrary(tidyverse)\nlibrary(gapminder)\n\ngapminder |&gt; \n        ggplot(aes(x = year, y = lifeExp, \n                   color = continent))+\n        geom_jitter(size = 1, alpha = .2, width = .75)+\n        stat_summary(geom = \"path\", fun.y = mean, size = 1)+\n        theme_minimal(base_family = \"mono\")\n\n\nGrab a dataset by URL\nIf a dataset is hosted online and has a direct link to the file, it can be easily imported into the R session just specifying the URL. For illustration, I will access Galton dataset from HistData package using a direct link from Vincent Arel-Bundock’s list.\n\nlibrary(tidyverse)\n\ngalton &lt;- read_csv(\n    \"https://raw.githubusercontent.com/vincentarelbundock/Rdatasets/master/csv/HistData/Galton.csv\"\n)\n\ngalton |&gt; \n        ggplot(aes(x = father, y = height))+\n        geom_point(alpha = .2)+\n        stat_smooth(method = \"lm\")+\n        theme_minimal(base_family = \"mono\")\n\n\nDownload and unzip an archive\nQuite often datasets are stored in archived from. With R it is very simple to download and unzip the desired data archives. As an example, I will download Historical New York City Crime Data provided by the Government of the Sate of New York and hosted at data.gov portal. The logic of the process is: first, we create a directory for the unzipped data; second, we download the archive; finally, unzip the archive and read the data.\n\nlibrary(tidyverse)\nlibrary(readxl)\n\n# create a directory for the unzipped data\nifelse(!dir.exists(\"unzipped\"), dir.create(\"unzipped\"), \"Directory already exists\")\n\n# specify the URL of the archive\nurl_zip &lt;- \"http://www.nyc.gov/html/nypd/downloads/zip/analysis_and_planning/citywide_historical_crime_data_archive.zip\"\n\n# storing the archive in a temporary file\nf &lt;- tempfile()\ndownload.file(url_zip, destfile = f)\nunzip(f, exdir = \"unzipped/.\")\n\nIf the zipped file is rather big and we don’t want to download it again the next time we run the code, it might be useful to keep the archived data.\n\n# if we want to keep the .zip file\npath_unzip &lt;- \"unzipped/data_archive.zip\"\nifelse(!file.exists(path_unzip), \n       download.file(url_zip, path_unzip, mode=\"wb\"), \n       'file alredy exists')\nunzip(path_unzip, exdir = \"unzipped/.\")\n\nFinally, let’s read and plot some of the downloaded data.\n\nmurder &lt;- read_xls(\n    \"unzipped/Web Data 2010-2011/Seven Major Felony Offenses 2000 - 2011.xls\",\n    sheet = 1, range = \"A5:M13\"\n) |&gt; \n        filter(OFFENSE |&gt; substr(1, 6) == \"MURDER\") |&gt; \n        gather(\"year\", \"value\", 2:13) |&gt; \n        mutate(year = year |&gt; as.numeric())\n\nmurder |&gt; \n        ggplot(aes(year, value))+\n        geom_point()+\n        stat_smooth(method = \"lm\")+\n        theme_minimal(base_family = \"mono\")+\n        labs(title = \"Murders in New York\")\n\n\nFigshare\nIn Academia it is becoming more and more popular to store the datasets accompanying papers in the specialized repositories. Figshare is one of the most popular free repositories. There is an R package rfigshare to access the datasets from this portal. As an example I will grab the dataset on ice-hockey playes height that I assembled manually for my blog post. Please note that at the first run the package will ask to enter your Figshare login details to access API - a web page will be opened in browser.\nThere is a search function fs_search, though my experience shows that it is easier to search for a dataset in a browser and then use the id of a file to download it. The function fs_download turns an id number into a direct URL to download the file.\n\nlibrary(tidyverse)\nlibrary(rfigshare)\n\nurl &lt;- fs_download(article_id = \"3394735\")\n\nhockey &lt;- read_csv(url)\n\nhockey |&gt; \n        ggplot(aes(x = year, y = height))+\n        geom_jitter(size = 2, color = \"#35978f\", alpha = .1, width = .25)+\n        stat_smooth(method = \"lm\", size = 1)+\n        ylab(\"height, cm\")+\n        xlab(\"year of competition\")+\n        scale_x_continuous(\n            breaks = seq(2005, 2015, 5), labels = seq(2005, 2015, 5)\n        )+\n        theme_minimal(base_family = \"mono\")\n\n\n\n\n\n\n\n\n\nAll the code chunks can be found in this gist"
  },
  {
    "objectID": "2017/data-acquisition-two/index.html",
    "href": "2017/data-acquisition-two/index.html",
    "title": "Data acquisition in R (2/4)",
    "section": "",
    "text": "The series consists of four posts:\n\n\n\n\n\nLoading prepared datasets\n\n\nAccessing popular statistical databases\n\n\nDemographic data sources\n\nGetting spatial data\n\n\n\n\nFor each of the data acquisition options I provide a small visualization use case.\nEurostat\nThe package eurostat has a function search_eurostat to search for the relevant datasets. Though, sadly enough, this function does not provide the codes of all the datasets that has the expression of interest in the title. For example, the search on the expression life expectancy produces an output with just 2 results, which does not make any sense. Thus, the best strategy is to go to Eurostat website, find the needed dataset code, and fetch the desired dataset by its code. Note that there is a separate database for subregional level indicators.\nI am going to download life expectancy estimates for European countries; the dataset code is demo_mlexpec.\n\nlibrary(tidyverse) \nlibrary(lubridate)\nlibrary(eurostat) \n\n# download the selected dataset\ne0 &lt;- get_eurostat(\"demo_mlexpec\")\n\nIt can take a while because the dataset is quite big (0.4m obs). If the automated procedure does not work, one can download the data manually via the Bulk Download Service of Eurostat.\nLet’s have a look at the remaining life expectancy at age 65, the most common conventional age at retirement, in some European countries, separately for males, females, and total population. Some data preparation steps are needed. First, we only need the life expectancy estimates for those aged 65. Next, we don’t need total population, only males and females separately. Finally, let’s select just a bunch of countries: Germany, France, Italy, Russia, Spain, the UK.\n\ne0 |&gt; \n        filter(! sex == \"T\",\n               age == \"Y65\", \n               geo %in% c(\"DE\", \"FR\", \"IT\", \"RU\", \"ES\", \"UK\")) |&gt; \n        ggplot(aes(x = time |&gt; year(), y = values, color = sex))+\n        geom_path()+\n        facet_wrap(~ geo, ncol = 3)+\n        labs(y = \"Life expectancy at age 65\", x = NULL)+\n        theme_minimal(base_family = \"mono\")\n\n\nWorld Bank\nThere are several packages that provide an API to World Bank data. Probably, the most elaborated one is a fairly recent wbstats. Its wbsearch function really does great job searching through the database for the relevant datasets. For example, wbsearch(\"fertility\") produces a dataframe of 339 entries with the codes and names of the relevant indicators.\n\nlibrary(tidyverse) \nlibrary(wbstats)\n\n# search for a dataset of interest\nwbsearch(\"fertility\") |&gt; head\n\n\n\n\n\n\n\n\n\nindicatorID\nindicator\n\n\n\n2479\nSP.DYN.WFRT.Q5\nTotal wanted fertility rate (births per woman): Q5 (highest)\n\n\n2480\nSP.DYN.WFRT.Q4\nTotal wanted fertility rate (births per woman): Q4\n\n\n2481\nSP.DYN.WFRT.Q3\nTotal wanted fertility rate (births per woman): Q3\n\n\n2482\nSP.DYN.WFRT.Q2\nTotal wanted fertility rate (births per woman): Q2\n\n\n2483\nSP.DYN.WFRT.Q1\nTotal wanted fertility rate (births per woman): Q1 (lowest)\n\n\n2484\nSP.DYN.WFRT\nWanted fertility rate (births per woman)\n\n\n\nLet’s have a look at the indicator Lifetime risk of maternal death (%) (code SH.MMR.RISK.ZS). World Bank provides a variety of country groupings. One of the curious groupings divides countries based on the advance over the Demographic Transition path. Below I plot our selected indicator for (1) the countries that have passed the Demographic Transition, (2) the countries that haven’t yet experienced demographic dividend, and (3) the whole World.\n\n# fetch the selected dataset\ndf_wb &lt;- wb(indicator = \"SH.MMR.RISK.ZS\", startdate = 2000, enddate = 2015)\n\n# have look at the data for one year\ndf_wb |&gt; filter(date == 2015) |&gt; View\n\ndf_wb |&gt; \n        filter(iso2c %in% c(\"V4\", \"V1\", \"1W\")) |&gt; \n        ggplot(aes(x = date |&gt; as.numeric(), y = value, color = country))+\n        geom_path(size = 1)+\n        scale_color_brewer(NULL, palette = \"Dark2\")+\n        labs(x = NULL, y = NULL, title = \"Lifetime risk of maternal death (%)\")+\n        theme_minimal(base_family = \"mono\")+\n        theme(panel.grid.minor = element_blank(),\n              legend.position = c(.8, .9))\n\n\nOECD\nOrganization for Economic Cooperation and Development provides detailed economic and demographic data on the member countries. There is an R package OECD that streamlines the use of their data in R. The function search_dataset works nicely to browse the available datasets by keywords. Then get_dataset would fetch the chosen dataset. In the example below I grab the data on the duration of unemployment and then plot the data for the male population of EU16, EU28 and the US as heatmaps.\n\nlibrary(tidyverse) \nlibrary(viridis)\nlibrary(OECD)\n\n# search by keyword\nsearch_dataset(\"unemployment\") |&gt; View\n\n# download the selected dataset\ndf_oecd &lt;- get_dataset(\"AVD_DUR\")\n\n# turn variable names to lowercase\nnames(df_oecd) &lt;- names(df_oecd) |&gt; tolower()\n\ndf_oecd |&gt; \n        filter(\n            country %in% c(\"EU16\", \"EU28\", \"USA\"), sex == \"MEN\", ! age == \"1524\"\n        ) |&gt; \n        ggplot(aes(obstime, age, fill = obsvalue))+\n        geom_tile()+\n        scale_fill_viridis(\"Months\", option = \"B\")+\n        scale_x_discrete(breaks = seq(1970, 2015, 5) |&gt; paste)+\n        facet_wrap(~ country, ncol = 1)+\n        labs(x = NULL, y = \"Age groups\", \n             title = \"Average duration of unemployment in months, males\")+\n        theme_minimal(base_family = \"mono\")\n\n\nWID\nWorld Wealth and Income Database is a harmonized dataset on income and wealth inequality. The developers of the database provide an R package to get their data, which is only available from github so far.\n\nlibrary(tidyverse) \n\n#install.packages(\"devtools\")\ndevtools::install_github(\"WIDworld/wid-r-tool\")\nlibrary(wid)\n\nThe function to acquire data is download_wid(). To specify the arguments, one would have to consult help pages of the package and select desired datasets.\n\n?wid_series_type\n?wid_concepts\n\nThe following nice example is adapted from the package vignette. It shows the share of wealth that was owned by the richest 1% and 10% of population in France and Great Britain.\n\ndf_wid &lt;- download_wid(\n        indicators = \"shweal\", # Shares of personal wealth\n        areas = c(\"FR\", \"GB\"), # In France an Italy\n        perc = c(\"p90p100\", \"p99p100\") # Top 1% and top 10%\n)\n\n\ndf_wid |&gt; \n        ggplot(aes(x = year, y = value, color = country)) +\n        geom_path()+\n        labs(\n            title = \"Top 1% and top 10% personal wealth shares in \n            France and Great Britain\",\n             y = \"top share\"\n        )+\n        facet_wrap(~ percentile)+\n        theme_minimal(base_family = \"mono\")\n\n\n\n\n\n\n\n\n\nAll the code chunks together can be found in this gist"
  },
  {
    "objectID": "2023/30-dmc/index.html",
    "href": "2023/30-dmc/index.html",
    "title": "#30DayMapChallenge my 25/30 contributions",
    "section": "",
    "text": "For four years I’ve been following #30DayMapCallenge with admiration but not daring to commit to it. Producing maps was always an accompanying step in my main activities, and towards the end of a year it never felt possible to focus on producing them daily. This year I decided to cheat and re-publish many of my maps that accumulated over the years, and only produce new ones for a handful of days/topics. I mostly succeded with posting 25 out of 30. And out of these 25 submissions 6 are freshly coded, 2 are old educational materials turned into blog posts, and 5 more are new quick stuff done without coding. All the new code is available in the gihub repo; and the blog posts (geocoding, map-projections) contain all the necessary #rstats code inline.\nBelow I list all my contributions as Mastodon post embeds. I use Mastodon because it’s fully open, but the actual #30DayMapCallenge communication was mostly happening on Bluesky, which is still invite-only platform, closed to unregistered viewers. The bsky thread with my contributions is here (note that I’m linking the last post, scroll upwards). Enjoy exploring!\n\n\n\n\n01 points\n16 Oceania\n\n\n02 lines\n17 flow\n\n\n03 polygons\n18 atmosphere\n\n\n04 a bad map\n19 5-minutes map\n\n\n05 analog map\n20 outdoors\n\n\n06 Asia\n21 raster\n\n\n07 navigation\n22 North is not always up\n\n\n08 Africa\n23 3D\n\n\n09 hexagons\n24 black & white\n\n\n10 North America\n25 Antarctica\n\n\n11 retro\n26 minimal\n\n\n12 South America\n27 dot\n\n\n13 choropleth\n28 Is it a chart or a map?\n\n\n14 Europe\n29 population\n\n\n15 openstreetmap\n30 my favourite\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2023-11-01 | points\nbsky | fosstodon | blog\n\n\n2023-11-02 | lines\nbsky | fosstodon | blog\n\n\n2023-11-03 | polygons\nbsky | fosstodon | code\n\n\n\n\n  \n\n\n  \n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2023-11-04 | points\nbsky | fosstodon | blog\n\n\n2023-11-05 | lines\nbsky | fosstodon\n\n\n2023-11-06 | polygons\nbsky | fosstodon | code\n\n\n\n\n  \n\n\n  \n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2023-11-07 | navigation\nbsky | fosstodon\n\n\n2023-11-08 | Africa\nbsky | fosstodon | code\n\n\n2023-11-09 | hexagons\nbsky | fosstodon\n\n\n\n\n  \n\n\n  \n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2023-11-10 | North America\nbsky | fosstodon | code | paper\n\n\n2023-11-11 | retro\nbsky | fosstodon | code\n\n\n2023-11-12 | South America\nbsky | fosstodon | code\n\n\n\n\n  \n\n\n  \n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2023-11-13 | choropleth\nbsky | fosstodon | code | paper\n\n\n2023-11-14 | Europe\nbsky | fosstodon | code | reddit\n\n\n2023-11-15 | OpenStreetMap\nbsky | fosstodon | code\n\n\n\n\n  \n\n\n  \n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2023-11-16 | Oceania\nbsky | fosstodon\n\n\n2023-11-17 | flow\nbsky | fosstodon | code\n\n\n2023-11-19 | 5-minute map\nbsky | fosstodon\n\n\n\n\n  \n\n\n  \n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2023-11-20 | outdoors\nbsky | fosstodon | code\n\n\n2023-11-22 | North is always up\nbsky | fosstodon | reddit\n\n\n2023-11-23 | 3D\nbsky | fosstodon\n\n\n\n\n  \n\n\n  \n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2023-11-25 | Antarctica\nbsky | fosstodon | code\n\n\n2023-11-28 | Is this a chart or a map?\nbsky | fosstodon | reddit\n\n\n2023-11-29 | Population\nbsky | fosstodon | code | paper | reddit\n\n\n\n\n  \n\n\n  \n\n\n  \n\n\n\n\n\n\n\n2023-11-30 | my favourite\nbsky | fosstodon | code | paper | reddit"
  },
  {
    "objectID": "2017/denmark-nuts-reconstruction/index.html",
    "href": "2017/denmark-nuts-reconstruction/index.html",
    "title": "R, GIS, and fuzzyjoin to reconstruct demographic data for NUTS regions of Denmark",
    "section": "",
    "text": "NUTS stands for the Nomenclature of Territorial Units For Statistics. The history of NUTS dates back to the beginning of 1970s, when European countries developed unified standards for systems of administrative geography. It was not until the beginning of this century when such a system finally became widely used. There are three main hierarchical levels of NUTS, and the most commonly used for regional analysis is NUTS-2.\n\nFigure 1. Illustration of the principle of NUTS hierarchical system\nOne of the objectives of NUTS was to provide more or less comparable administrative divisions for all countries of Europe. Nevertheless, in 2013, population figures for single NUTS-2 regions ranged from 28.5 thousands in Aland island (Finland) to almost 12 million in Ile-de-France (Paris and surroundings, France).\nThe broken time series\nQuite arbitrary in its essence, territorial division tends to evolve. Changes in administrative boundaries can cause problems for regional analysis as they break the time series and therefore make it harder to analyze trends. Despite this inconvenience, the boundaries of regions actually change quite often based on the needs and interests of local or national governmenta. Eurostat tracks all modifications providing detailed explanations of all the changes that happen between versions of NUTS (figure 2).\nFigure 2. Changes in NUTS between versions 2006 and 2010\nDespite this, Eurostat does not recalculate historic demographic data to match the most recent NUTS version. This means that, for the most recent version of NUTS, there is missing data for all years before the latest administrative change. So researchers have to reconstruct historical data manually to obtain a long time series. Of course, crude assumptions often have to be accepted in order to approximate the population figures for the current regions that did not exist in the past.\nTo make thing even more complex, Eurostat provides the data only for the latest version of NUTS (at least, I did not work out how to download previous versions). In my PhD project I carry out regional analysis for the NUTS-2 regions of European Union. To have the longest possible time series, when I did the data preparation in 2015, I chose the 2010 version of NUTS, on which the regional demographic projection EUROPOP2013 is based. For reproducibility, I uploaded the precise versions of the Eurostat data at NUTS-2 level on population age structures and deaths, as downloaded in 2015, to figshare.\nDenmark\nSome countries had to perform major changes in their systems of territorial division to fit the NUTS standards. The most significant reform happened in Denmark in 2007, where the former 271 municipalities were transformed into the new 98 municipalities. At the same time, NUTS was introduced, so that 98 municipalities were allocated to 11 NUTS-3 regions, which aggregate to 5 NUTS-2 regions. Typically, for a small country, there is only one NUTS-1 region in Denmark, which is the whole country.\nAs far as I know, there was no official attempt of Eurostat to reconstruct the time series for Denmark before 2007. The typical map of Eurostat for the pre-2007 period shows Denmark as “no data available” country (figure 3).\nFigure 3. Life expectancy at birth in European NUTS-2 regions, 2006; a screenshot from the Eurostat’s interactive data exploratory tool\nSuch a data loss is somewhat surprising for a country such as Denmark. It might be quite difficult to match the old and new municipal systems; but it should be relatively easy to re-aggregate the old municipalities into the new (higher level) NUTS regions. That is precisely what I did during my data preparation1 and what I now want to share in this post.\n1 I have spent quite some time searching if someone else did the job before me and failed to find.The task is basically to identify which of the old 271 municipalities are located within the modern 11 NUTS-3 regions and to aggregate the municipal data. Then, NUTS-3 data is easily aggregated for the NUTS-2 level. Such a task could have meant working late into the night, but luckily we live in the GIS era. I used GIS to match the old municipalities with the NUTS-3 regions. Here I want to show (with code) how the task can be performed using the amazing and opensource R. Below I show the process of matching old municipalities to the NUTS regions and the process that I used to aggregate population data.\nData\nThe data on the population age structures for the old 271 municipalities of Denmark was downloaded from the official website of Statistics Denmark. The system only allows you to grab up to 10K cells for unregistered users and up to 100K for registered users. So the process of downloading the data involves some tedious manual manipulations. For the purpose of my phd project, I downloaded the data for the period 2001-2006; but, if needed, the data is available since 1979. The data, downloaded in 2015 and ‘tidied up’ can be found here.\nI have spent a lot of time trying to find geodata with the boundaries of the old municipalities. Now, coming back to the topic more than 1.5 year later, I failed to identify the original source of the shapefile, though I am pretty sure that it came from here 2. The copy of the shapefile that I used can be found here.\n2 There is a note on the website saying that, due to a planned change in the structure of the website, there might be some problems with data accuisition. I failed to download the geodata on 2017-02-23.Finally, we need a shapefile of NUTS-3 regions. It can be easily downloaded from Eurostat geodata repository. The shapefile that I used is “NUTS_2010_20M_SH.zip”. The selection of the 11 Danish regions can be found here.\nThe projection used for both shapefiles is ESPG-3044, the one often used to map Denmark.\nNow, the code to prepare the R session and load the data.\n\n# set locale and encoding parameters to read Danish\nif(Sys.info()['sysname']==\"Linux\"){\n        Sys.setlocale(\"LC_CTYPE\", \"da_DK.utf8\")\n        danish_ecnoding &lt;- \"WINDOWS-1252\"\n}else if(Sys.info()['sysname']==\"Windows\"){\n        Sys.setlocale(\"LC_CTYPE\", \"danish\")\n        danish_ecnoding &lt;- \"Danish_Denmark.1252\"\n}\n\n# load required packages (install first if needed)\nlibrary(tidyverse) # version: 1.0.0\nlibrary(ggthemes) # version: 3.3.0\nlibrary(rgdal) # version: 1.2-4\nlibrary(rgeos) # version: 0.3-21\nlibrary(RColorBrewer) # version: 1.1-2\nmypal &lt;- brewer.pal(11, \"BrBG\")\nlibrary(fuzzyjoin) # version: 0.1.2\nlibrary(viridis) # version: 0.3.4\n\n# load Denmark pop structures for the old municipalities\ndf &lt;- read_csv(\"https://ikashnitsky.github.io/share/1703-nuts2-denmark/BEF1A.csv.gz\")\n\n# create a directory for geodata\nifelse(!dir.exists(\"geodata\"), dir.create(\"geodata\"), \"Directory already exists\")\n\n# download, unzip and read Danish NUTS-3 geodata (31KB)\nurl_nuts &lt;- \n    \"https://ikashnitsky.github.io/share/1703-nuts2-denmark/denmark-nuts3-espg3044.tgz\"\npath_nuts &lt;- \"geodata/denmark-nuts3-espg3044.tgz\"\nifelse(\n    !file.exists(path_nuts), \n    download.file(url_nuts, path_nuts, mode=\"wb\"), \n    'file alredy exists'\n)\n# If there are problems downloading the data automatically, please download it from\n# https://ikashnitsky.github.io/share/1703-nuts2-denmark/denmark-nuts3-espg3044.tgz\nuntar(tarfile = path_nuts, exdir = \"geodata\")\n\nsp_nuts3 &lt;- readOGR(dsn = \"geodata/.\", layer = \"denmark-nuts3-espg3044\")\ngd_nuts3 &lt;- fortify(sp_nuts3, region = \"NUTS_ID\") # to the ggplot format\n\n\n# download, unzip and read Danish old municipal geodata (6.0MB)\nurl_mun &lt;- \n    \"https://ikashnitsky.github.io/share/1703-nuts2-denmark/kommune2006win1252.tgz\"\npath_mun &lt;- \"geodata/kommune2006win1252.tgz\"\nifelse(\n    !file.exists(path_mun), \n    download.file(url_mun, path_mun, mode=\"wb\"), \n    'file alredy exists'\n)\n# If there are problems downloading the data automatically, please download it from\n# https://ikashnitsky.github.io/share/1703-nuts2-denmark/kommune2006utf8.tgz\nuntar(tarfile = path_mun, exdir = \"geodata\")\n\nsp_mun &lt;- readOGR(\n    dsn = \"geodata/.\", layer = \"kommune2006win1252\", encoding = danish_ecnoding\n) \ngd_mun &lt;- fortify(sp_mun)\n\n# coordinates of the municipalities\nmun_coord &lt;- bind_cols(as.data.frame(coordinates(sp_mun)), sp_mun@data[,1:3]) |&gt; \n        transmute(long = V1, lat = V2, enhedid, objectid, name = navn)\n\nSpatial matching\nLet’s first have a look at the map.\n\nggplot()+\n        geom_polygon(data = gd_nuts3, aes(long, lat, group = group), \n                     color = brbg[3], fill = \"grey90\", size = 1)+\n        geom_point(data = mun_coord, aes(long, lat), \n                   color = brbg[10], size = 1)+\n        theme_map()\n\nFigure 4. Reference map of the old municipalities and NUTS-3 regions of Denmark\nWe can easily see that the boundaries of the municipalities (light blue) are much more precise than that of the NUTS-3 regions (orange/brown). This is not a problem as long as all the centroids of the municipalities fall within the boundaries of the NUTS-3 regions, which seems to be true for all municipalities except for the easternmost one. A quick check reveals that this is Christiansø, a tiny fortified island, whose history goes back to the Middle Ages. It has a special status and is not included into the NUTS system. For further manipulations, Christiansø can safely merge it with the close-by Bornholm.\nTo identify which municipalities fall into which NUTS regions, I use the spatial overlap function (over) from sp package. Here I should thank Roger Bivand, a person who made it possible to do any spatial analysis in R.\n\n# municipality coordinates to Spatial\nmun_centr &lt;- SpatialPoints(\n    coordinates(sp_mun), proj4string = CRS(proj4string(sp_nuts3))\n)\n\n# spatial intersection with sp::over\ninter &lt;- bind_cols(mun_coord, over(mun_centr, sp_nuts3[,\"NUTS_ID\"])) |&gt; \n        transmute(long, lat, objectid,\n                  nuts3 = as.character(NUTS_ID),\n                  nuts2 = substr(nuts3, 1, 4))\n\nLet’s again check visually if the spatial matching worked okay.\n\nggplot()+\n        geom_polygon(data = gd_mun, aes(long, lat, group = group), \n                     color = brbg[9], fill = \"grey90\", size = .1)+\n        geom_polygon(data = gd_nuts3, aes(long, lat, group = group), \n                     color = brbg[3], fill = NA, size = 1)+\n        geom_point(data = inter, aes(long, lat, color = nuts3), size = 1)+\n        geom_point(data = inter[is.na(inter$nuts3),], \n                   aes(long, lat), color = \"red\", size = 7, pch = 1)+\n        theme_map(base_size = 15)+\n        theme(legend.position = c(1, 1),\n              legend.justification = c(1, 1))\n\nFigure 5. Checking the spatial intersection between the old municipalities and NUTS-3 regions of Denmark\nNot bad. But there is an “NA” category that represents all the cases where the spatial match failed. How many such cases do we have?\n\n# how many failed cases do we have\nsum(is.na(inter$nuts3))\n\n\n## [1] 3\n\n\n# where the intersection failed\ninter[is.na(inter$nuts3),]\n\n\n##         long     lat objectid nuts3 nuts2\n## 23  892474.0 6147918    46399  &lt;NA&gt;  &lt;NA&gt;\n## 65  504188.4 6269329   105319  &lt;NA&gt;  &lt;NA&gt;\n## 195 533446.8 6312770    47071  &lt;NA&gt;  &lt;NA&gt;\n\nAs there are only 3 cases, I decided to fix them manually.\n\n# fix the three cases manually\nfixed &lt;- inter\nfixed[fixed$objectid==\"46399\", 4:5] &lt;- c(\"DK014\", \"DK01\")\nfixed[fixed$objectid==\"105319\", 4:5] &lt;- c(\"DK041\", \"DK04\")\nfixed[fixed$objectid==\"47071\", 4:5] &lt;- c(\"DK050\", \"DK05\")\n\nThe final visual check.\n\nggplot()+\n        geom_polygon(data = gd_mun, aes(long, lat, group = group), \n                     color = brbg[9], fill = \"grey90\", size = .1)+\n        geom_polygon(data = gd_nuts3, aes(long, lat, group = group), \n                     color = brbg[3], fill = NA, size = 1)+\n        geom_point(data = fixed, aes(long, lat, color = nuts3), size = 1)+\n        theme_map(base_size = 15)+\n        theme(legend.position = c(1, 1),\n              legend.justification = c(1, 1))\n\nFigure 6. Re-checking the spatial intersection between the old municipalities and NUTS-3 regions of Denmark\nNow everything seems okay.\nJoining spatial and statistical data (fuzzy join)\nThe next task is to join the spatial data and statistical data together. The spatial layer for municipalities does not contain the codes that are used by Statistics Denmark, so I have to match municipalities in the two datasets by their names. This is quite a difficult task. Names can be written slightly differently, there are some special characters in Danish alphabet, and some municipalities may have experienced a change of name. To solve the task most efficiently, I used the ‘Fuzzy String Matching’ approach which is implemented in the fuzzyjoin package by David Robinson.\nFirst, I simplify the names in both datasets turning them into lowercase, replacing the character “å” with “aa”, and removing the “Kommune” word in the spatial dataset names. Please note that I downloaded (separately) a small selection from Statistics Denmark to have a lightweight dataframe with municipal codes and names.\n\n# simplify municipalities names\nmun_geo &lt;- mun_coord |&gt;         \n        transmute(name = sub(x = name, \" Kommune\", replacement = \"\"), objectid) |&gt; \n        mutate(name = gsub(x = tolower(name), \"å\", \"aa\"))\n\nmun_stat &lt;- read.csv2(\n    \"https://ikashnitsky.github.io/share/1703-nuts2-denmark/stat-codes-names.csv\", \n    fileEncoding = danish_ecnoding\n) |&gt; \n        select(name) |&gt; \n        separate(name, into = c(\"code\", \"name\"), sep = \" \", extra = \"merge\") |&gt; \n        mutate(name = gsub(\"\\\\s*\\\\([^\\\\)]+\\\\)\", \"\", x = name)) |&gt; \n        mutate(name = gsub(x = tolower(name), \"å\", \"aa\"))\n\nLet’s try fuzzy join.\n\n# first attempt\nfuz_joined_1 &lt;- regex_left_join(mun_geo, mun_stat, by = \"name\")\n\nThe resulting dataframe has 278 rows instead of 271. That means that for some municipalities in the spatial dataset there was more than one match. Let’s identify them.\n\n# identify more that 1 match (7 cases) and select which to drop\nfuz_joined_1 |&gt; group_by(objectid) |&gt; mutate(n = n()) |&gt; filter(n &gt; 1)\n\n\n## Source: local data frame [14 x 5]\n## Groups: objectid [7]\n## \n##         name.x objectid  code      name.y     n\n##          &lt;chr&gt;    &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt; &lt;int&gt;\n## 1       haslev   105112   313      haslev     2\n## 2       haslev   105112   403       hasle     2\n## 3  brønderslev    47003   739       rønde     2\n## 4  brønderslev    47003   805 brønderslev     2\n## 5    hirtshals    47037   817        hals     2\n## 6    hirtshals    47037   819   hirtshals     2\n## 7      rønnede    46378   385     rønnede     2\n## 8      rønnede    46378   407       rønne     2\n## 9     hvidebæk    46268   317    hvidebæk     2\n## 10    hvidebæk    46268   681     videbæk     2\n## 11    ryslinge    46463   477    ryslinge     2\n## 12    ryslinge    46463   737          ry     2\n## 13     aarslev    46494   497     aarslev     2\n## 14     aarslev    46494   861        aars     2\n\nSo, for 7 municipalities, two matches were found. I will drop the imperfect match variants in the next iteration of fuzzy join.\nThe other issue is the municipalities for which no match was found in that statistical data.\n\n# show the non-matched cases\nfuz_joined_1 |&gt; filter(is.na(code))\n\n\n##              name.x objectid code name.y\n## 1              faxe   105120 &lt;NA&gt;   &lt;NA&gt;\n## 2 nykøbing falsters    46349 &lt;NA&gt;   &lt;NA&gt;\n## 3       herstederne    46101 &lt;NA&gt;   &lt;NA&gt;\n\nAs there are only three such cases, I corrected them manually in the spatial data to match the statistical data. There are two cases of a difference in the way the name of municipality are written and one case of name change.\n\n# correct the 3 non-matching geo names\nmun_geo_cor &lt;- mun_geo\n\nmun_geo_cor[mun_geo_cor$name==\"faxe\", \"name\"] &lt;- \"fakse\"\nmun_geo_cor[mun_geo_cor$name==\"nykøbing falsters\", \"name\"] &lt;- \"nykøbing f.\"\nmun_geo_cor[mun_geo_cor$name==\"herstederne\", \"name\"] &lt;- \"albertslund\"\n\nNow the second attempt to match the datasets (spatial dataset is corrected).\n\n# second attempt\nfuz_joined_2 &lt;- regex_left_join(mun_geo_cor, mun_stat, by = \"name\")\n\n# drop non-perfect match\nfuz_joined_2 &lt;- fuz_joined_2 |&gt; \n        group_by(objectid) |&gt; \n        mutate(n = n()) |&gt; \n        ungroup() |&gt; \n        filter(n &lt; 2 | name.x==name.y)\n\nfuz_joined_2 &lt;- fuz_joined_2 |&gt; transmute(name = name.x, objectid, code)\n\nThe output looks perfect. Now, the last step – using the matched “objectid” field, I will finally attach the NUTS data to statistical codes.\n\n# finally, attach the NUTS info to matched table\nkey &lt;- left_join(fuz_joined_2, fixed, \"objectid\")\n\nAggregate old municipal data to NUTS levels\nThe previous manipulations yielded a dataframe that links statistical codes of the old municipalities with the corresponding NUTS regions. The last thing that has to be done is aggregation. I will attach the “key” dataset to a statistical dataset and aggregate the data at NUTS-3 and NUTS-2 levels.\n\n# finally, we only need to aggregate the old stat data\ndf_agr &lt;- left_join(key, df, \"code\") |&gt; \n        filter(!is.na(name)) |&gt; \n        gather(\"year\", \"value\", y2001:y2006)\n\ndf_nuts3 &lt;- df_agr |&gt; \n        group_by(year, sex, age, nuts3) |&gt; \n        summarise(value = sum(value)) |&gt; \n        ungroup()\n\ndf_nuts2 &lt;- df_agr |&gt; \n        group_by(year, sex, age, nuts2) |&gt; \n        summarise(value = sum(value)) |&gt; \n        ungroup()\n\nLet’s now calculate the shares of working age population in Danish NUTS-3 regions in 2001 and map the information.\n\n# total population in 2001 by NUTS-3 regions\ntot_01 &lt;- df_nuts3 |&gt; \n        filter(year==\"y2001\") |&gt; \n        group_by(nuts3) |&gt; \n        summarise(tot = sum(value, na.rm = TRUE)) |&gt; \n        ungroup()\n\n# working-age population in 2001 by NUTS-3 regions\nworking_01 &lt;- df_nuts3 |&gt; \n        filter(year==\"y2001\", age %in% paste0(\"a0\", 15:64)) |&gt; \n        group_by(nuts3) |&gt; \n        summarise(work = sum(value, na.rm = TRUE)) |&gt; \n        ungroup()\n\n# calculate the shares of working age population\nsw_01 &lt;- left_join(working_01, tot_01, \"nuts3\") |&gt; \n        mutate(sw = work / tot * 100)\n\n\n# map the shares of working age population in 2001 by NUTS-3 regions\nggplot()+\n        geom_polygon(data = gd_nuts3 |&gt; left_join(sw_01, c(\"id\" = \"nuts3\")),\n                     aes(long, lat, group = group, fill = sw), \n                     color = \"grey50\", size = 1) +\n        scale_fill_viridis()+\n        theme_map(base_size = 15)+\n        theme(legend.position = c(1, 1),\n              legend.justification = c(1, 1))\n\nFigure 7. The share of working age (15-64) population by NUTS-3 regions of Denmark in 2001\nThe result (thankfully!) looks realistic, with higher shares of the working-age population in the capital region, and in other regions that have relatively big cities.\n\n\n\n\n\n\n\nThis post is written for Demotrends"
  },
  {
    "objectID": "2017/colorcoded-map/index.html",
    "href": "2017/colorcoded-map/index.html",
    "title": "Colorcoded map: regional population structures at a glance",
    "section": "",
    "text": "Data visualization is quite often a struggle to represent multiple relevant dimensions preserving the readability of the plot. In this post I will show my recent multidimensional dataviz prepared for Rostock Retreat Visualization, an event that gathered demographers for an amazing “three days long coffebreak”.\nEuropean population is rapidly ageing. But the process is not happening uniformly in all parts of Europe (see my recent paper for more info). Regions differ quite a lot: Eastern Europe still undergoes demographic dividend; Southern European regions form a cluster of lowest-low fertility; Western Europe experiences the greying of the baby boomers; urban regions attract young professionals and force out young parents; peripheral rural regions lose their youths forever… How can we grasp all the differences at a glance?\nHere I want to present a colorcoded map. For each NUTS-3 region the unique color is produced by mixing red, green, and blue color spectrums in the proportions that reflect,correspondingly, relative shares of elderly populating (aged 65+), population at working ages (15-64), and kids (0-14).\n\nEach of the three variables mapped here is scaled between 0 and 1: otherwise, the map would be just green with slightly variations in tones because the share of working age population is ranged between 65-75% for modern European regions. Thus, it is important to note that this map is not meant to be able to inform the reader of the exact population structure in a specific region. Rather, it provides a snapshot of all the regional population structures, facilitating comparisons between them. So, by design, the colors are only meaningful in comparison only for the given set of regions in a given year, in this case 2015. If we want cross-year comparisons, the variables are to be scaled across the whole timeseries, meaning that each separate map would, most likely, become less contrast.\nIn the map we can easily spot the major differences between subregions of Europe. Turkey is still having relatively high fertility, especially in the south-eastern Kurdish part, thus it has higher share of kids and it’s colored in blueish tones. The high-fertility Ireland is also evidently blue in the map. East-European regions are green due to the still lasting demographic dividend. Southern Europe is ageing fastest, thus the colors are reddish.\nWe can also see most of the major capital regions that are bright-green as opposed to the depleted periphery. In some countries there are huge regional differences: Northern and Southern Italy, Eastern and Western Germany.\nIt is striking how clearly can we see the borders between European countries: Poland and Germany, Czech Republic and Slovakia, Portugal and Spain, France and all the neighbors. The slowly evolving population structures bare imprints of unique populations’ histories, that largely correspond with state borders.\nThe obvious drawback of the map is that it is not colorblind friendly, and there is no way to make it so because color is the main player in this dataviz.\n\n\n\n\n\n\n\nTo reproduce the map from the scratch please see the gist"
  }
]